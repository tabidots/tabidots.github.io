<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Judo Salt Genius</title><link href="http://tabidots.github.io/" rel="alternate"></link><link href="http://tabidots.github.io/feeds/all.atom.xml" rel="self"></link><id>http://tabidots.github.io/</id><updated>2019-01-20T10:30:47+07:00</updated><entry><title>Model Thinker notes, Ch. 2 andÂ 3</title><link href="http://tabidots.github.io/2019/01/model-thinker-notes-chapters-2-3" rel="alternate"></link><published>2019-01-20T10:30:47+07:00</published><updated>2019-01-20T10:30:47+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-20:/2019/01/model-thinker-notes-chapters-2-3</id><summary type="html">&lt;p&gt;Getting reacquainted with statistics for the first time since my ill-fated stint in introductory statistics in&amp;nbsp;college.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Along with &lt;a href="http://www.pimbook.org/"&gt;&lt;em&gt;A Programmerâ€™s Introduction to Mathematics&lt;/em&gt;&lt;/a&gt;, I started reading &lt;a href="https://www.amazon.com/Model-Thinker-What-Need-Know/dp/0465094627/ref=zg_bs_13884_41?_encoding=UTF8&amp;amp;psc=1&amp;amp;refRID=ZK3QGA2250Q3CJSWR8Q2"&gt;&lt;em&gt;Model Thinker: What You Need to Know to Make Data Work for You&lt;/em&gt;&lt;/a&gt; as a sort of analogous non-statisticianâ€™s introduction toÂ statistics.&lt;/p&gt;
&lt;p&gt;Or perhaps I should say &lt;em&gt;re&lt;/em&gt;-introduction, since I had to take Intro to Stats in college. But I passed that class by the skin of my teeth and really developed an aversion to statistical analysis from that course. But itâ€™s a necessary part of data science,Â soâ€¦&lt;/p&gt;
&lt;h1 id="chapter-2"&gt;ChapterÂ 2&lt;/h1&gt;
&lt;p&gt;Chapter 2 is about what models can do and their strength inÂ numbers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Models can take one of threeÂ approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;embodiment&lt;/em&gt; approach (simplified but still realistic, like a geologicalÂ model)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;analogy&lt;/em&gt; approach (abstracted from reality: &lt;em&gt;Think of the situation as aâ€¦&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;alternative reality&lt;/em&gt;Â approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Models must be &lt;em&gt;communicable&lt;/em&gt; and &lt;em&gt;tractable&lt;/em&gt; (analyzable). This means that it should be possible to translate them precisely into math, code, or some other formal language. (Most models are mathematical in nature, but this is not aÂ must.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All models are wrong in their own way, so no single model can accurately represent complex phenomena, but many, together, areÂ useful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Models are a superhero &lt;em&gt;&lt;span class="caps"&gt;REDCAPE&lt;/span&gt;&lt;/em&gt;: Reason, explain, design, communicate, action, predict,Â explore.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Many models can be applied to the same problem (many-to-one). And with &lt;em&gt;creativity&lt;/em&gt;, one model can also be put to many uses. For example, how many applications are there for a random walk? (What a relief to know that there is a place at the table for creativity and a non-&lt;span class="caps"&gt;STEM&lt;/span&gt;Â background!)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="chapter-3"&gt;ChapterÂ 3&lt;/h1&gt;
&lt;p&gt;Chapter 3 is about how there is a Goldilocks-esque balance to be struck with regard to the diversity of models and the accuracy of any givenÂ model.&lt;/p&gt;
&lt;p&gt;More is better, until itâ€™s not; more accurate is better, until itâ€™sÂ not.&lt;/p&gt;
&lt;p&gt;The chapter uses some fancy-looking equations dressed up in terms that are specific to this context, but the equations are actually rooted in statistics and often have simpler, more intuitiveÂ explanations.&lt;/p&gt;
&lt;h2 id="diversity"&gt;Diversity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Condorcet jury theorem&lt;/strong&gt;: If all jury members in a case have an above-average probability of choosing the correct verdict, then each successive jury member after the first increases the probability of the whole jury choosing the correct verdict. The probability approaches 1 as you keep addingÂ members.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wisdom of the crowd&lt;/strong&gt;: The authorâ€™s &lt;em&gt;model diversity theorem&lt;/em&gt; is an adaptation of &lt;a href="https://www.archania.org/theorems/diversity/"&gt;the wisdom of the crowd theorem&lt;/a&gt;, and it uses the &lt;em&gt;error&lt;/em&gt; as we calculated in linear regression. (Actually, all of the equations in this chapter are based on a mean squared error of some sort.) The book presents it as follows:
  &lt;br/&gt;
&lt;div class="math"&gt;$$ \underbrace{(\bar{M} - V)^2}_{\textrm{Many-Model Error}} =
  \underbrace{\sum_{i=1}^N \frac{(M_i - V)^2}{N}}_{\textrm{Average-Model Error}} -
  \underbrace{\textcolor{teal}{\sum_{i=1}^N} \frac{\textcolor{teal}{(M_i - \bar{M})^2}}{N}}_{\textrm{Diversity of Model Predictions}} $$&lt;/div&gt;
&lt;br/&gt;
  where &lt;span class="math"&gt;\(M_i\)&lt;/span&gt; is the prediction of model &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(\bar{M}\)&lt;/span&gt; is the average value of all models, and &lt;span class="math"&gt;\(V\)&lt;/span&gt; is the true value. In otherÂ words,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;if you have only one model (&lt;span class="math"&gt;\(M_i = \bar{M}\)&lt;/span&gt;), you can never decrease the discrepancy between its prediction andÂ truth.&lt;/li&gt;
&lt;li&gt;Even if you have many models that make identical predictions (still &lt;span class="math"&gt;\(M_i = \bar{M}\)&lt;/span&gt;), they will collectively misestimate truth by as much as each one misestimatesÂ truth.&lt;/li&gt;
&lt;li&gt;However, as the modelsâ€™ predictions diverge from each other (that is, as &lt;span class="math"&gt;\(\color{teal}\sum_{i=1}^N (M_i - \bar{M})^2\)&lt;/span&gt; increases), their &lt;em&gt;collective misestimation&lt;/em&gt; goes on decreasing. This is similar to the Condorcet jury theorem, except in continuous rather than binaryÂ terms.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However, there is a diminishing rate of returns as you &lt;em&gt;include more models&lt;/em&gt;: Accuracy seems to converge on a limit of some sort, not unlike gradient descent. Conceived in that way, you should stop adding models once the error between &lt;span class="math"&gt;\(n\)&lt;/span&gt; models and &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; models is lower than some &lt;a href="/2019/01/from-zero-to-ero"&gt;&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; threshold value&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why is this the case? Diversity is a factor of the dataâ€™s dimensionality. Models using the same (or similar) subsets of salient features are liable to predictÂ similarly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is a limit to how independent (&lt;em&gt;accurate&lt;/em&gt; and &lt;em&gt;diverse&lt;/em&gt;) a given group of models can be. Accuracy may suffer as a result of artificially trying to increase diversity (categorizing a list of locations by alphabetical order, forÂ example).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="accuracy"&gt;Accuracy&lt;/h2&gt;
&lt;p&gt;Increasing the fit (and thus predictive accuracy) of a model by increasing its granularity (adding categories, features, etc.) can backfire after a certainÂ point.&lt;/p&gt;
&lt;p&gt;The rest of the chapter is a long and slightly overly complicated explanation of the what statisticians call the &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The relationship between bias and variance is similar to the relationship to &lt;em&gt;precision&lt;/em&gt; and &lt;em&gt;recall&lt;/em&gt; for binary classification. Ideal B-V are both as low as possible; ideal P-R are both as high as possible. However, they have a diametrically opposing relationship, so this is not actuallyÂ feasible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="bias"&gt;Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;High &lt;strong&gt;bias&lt;/strong&gt; âŸ¶ underfitting (model is too coarse to capture the general trend of the data). &lt;img alt="underfitting" src="../../images/underfitting.png"/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Source for this illustration and the following ones: &lt;a href="https://medium.freecodecamp.org/using-machine-learning-to-predict-the-quality-of-wines-9e2e13d7480d"&gt;FreeCodeCamp&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the context of the chapterâ€™s house price example, high bias is also called &lt;strong&gt;categorization error&lt;/strong&gt;. This is the discrepancy between the samples in each category and the mean of thatÂ category.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bias (categorization error) â†‘ if your categories are bad and donâ€™t accurately reflect any shared characteristics among the samples. This makes sense because in terms of a scatter plot, the data points in each category will be spread out randomly and have no trend. In terms of house prices, this could be like lumping real estate markets of states that begin with the same letterÂ together.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bias (categorization error) â†“ as category granularity â†‘ (gets moreÂ precise)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bias (categorization error) also â†“ as sample size â†‘, because the law of large numbers dictates that a larger number of samples in a category will tend toward the mean of that category. This makes sense because more data points should more clearly indicate a trend for the model toÂ follow.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="variance"&gt;Variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;High &lt;strong&gt;variance&lt;/strong&gt; âŸ¶ overfitting (model is so fine that it starts to capture noise in the data, or in other words, every little &lt;em&gt;variation&lt;/em&gt;).
&lt;img alt="overfitting" src="../../images/underfitting.png"/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the context of the chapterâ€™s house price example, high bias is also called &lt;strong&gt;valuation error&lt;/strong&gt;. This is the total discrepancy between the estimated category means and the actual categoryÂ means.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variance (valuation error) â†“ as categories get larger, again because of the law of largeÂ numbers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="we-all-want-what-we-cant-have"&gt;We all want what we canâ€™tÂ have&lt;/h2&gt;
&lt;p&gt;We want low variance and low bias:
&lt;img alt="good fit" src="../../images/goodfit.png"/&gt;&lt;/p&gt;
&lt;p&gt;But decreasing variance increases bias, and decreasing bias increases variance. So good luck withÂ that.&lt;/p&gt;
&lt;h2 id="a-more-intuitive-explanation-of-r2"&gt;A more intuitive explanation of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;There are many ways to measure a modelâ€™s accuracy, and there is a sidebar mentioning &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;, which quantifies the predictive accuracy of a regression model. However, it doesnâ€™t explain it anywhere near as intuitively as illustration onÂ Wikipedia:&lt;/p&gt;
&lt;p&gt;&lt;img alt="RÂ² image" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Coefficient_of_Determination.svg/640px-Coefficient_of_Determination.svg.png" title="By Orzetto - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=11398293"/&gt;&lt;/p&gt;
&lt;p&gt;The chapterâ€™s definition of &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; is a pretty common one: &lt;em&gt;The proportion of variance explained by the model&lt;/em&gt;. Thatâ€™s prettyÂ abstract.&lt;/p&gt;
&lt;p&gt;A more intuitive explanation, based on the above illustration, is: &lt;em&gt;How much better is a prediction made with your model than just taking the average of theÂ data?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You want the total area of the blue squares (including overlaps) to be as small as possible compared to the total area of the red squares (includingÂ overlaps).&lt;/p&gt;
&lt;h2 id="model-error-decomposition-theorem"&gt;Model error decompositionÂ theorem&lt;/h2&gt;
&lt;p&gt;The problem with a score like &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; is that it doesnâ€™t tell you how much of the error is due to bias and how much is due to variance. I suppose that might be helpful if you know exactly how to fine-tune one or theÂ other.&lt;/p&gt;
&lt;p&gt;The author presents the &lt;strong&gt;model error decomposition theorem&lt;/strong&gt; to solve thisÂ problem.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered}
\textrm{Model Error} = \textrm{Categorization Error} + \textrm{Valuation Error} \\
\underbrace{\sum_{x \in \textbf X} \Big(M(x) - V(x) \Big)^2}_\textrm{Model Error} =
\underbrace{\sum_{i=1}^n \sum_{x \in S_i} \Big(V(x) - V_i \Big)^2}_\textrm{Categorization Error} +
\underbrace{\sum_{i=1}^n \Big(M_i - V_i \Big)^2}_\textrm{Valuation Error}
\end{gathered} $$&lt;/div&gt;
&lt;p&gt;Lotta variablesÂ here.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Variable&lt;/th&gt;
&lt;th align="left"&gt;Meaning&lt;/th&gt;
&lt;th align="left"&gt;Variable&lt;/th&gt;
&lt;th align="left"&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;sample&lt;/td&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(S_i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;category &lt;span class="math"&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(\textbf X\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;dataset&lt;/td&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(M_i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;mean of modelâ€™s predictions for category &lt;span class="math"&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(M(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;modelâ€™s predicition for sample &lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(V_i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;mean of true values for category &lt;span class="math"&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;span class="math"&gt;\(V(x)\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;true value for sample &lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\textrm{Model Error}\)&lt;/span&gt;: sum discrepancy between model and truth for allÂ samples&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\textrm{Categorization Error}\)&lt;/span&gt;: sum discrepancy between true values in a category and true mean of that category for allÂ categories&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\textrm{Valuation Error}\)&lt;/span&gt;: sum discrepancy between predicted mean and true mean for allÂ categories&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seems like you could do &lt;span class="math"&gt;\(\sqrt{\textrm{Model Error}}\)&lt;/span&gt; to find a number the average amount that a prediction made with the model will be offÂ by.&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.freecodecamp.org/using-- machine-learning-to-predict-the-quality-of-wines-9e2e13d7480d"&gt;Using Machine Learning to Predict the Quality of Wines&lt;/a&gt;,Â FreeCodeCamp&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"&gt;Bias-variance tradeoff&lt;/a&gt;,Â Wikipedia&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"&gt;Coefficient of determination&lt;/a&gt;,Â Wikipedia&lt;/li&gt;
&lt;/ul&gt;</content><category term="bias"></category><category term="variance"></category><category term="model error decomposition"></category><category term="predictive accuracy"></category><category term="RÂ²"></category><category term="wisdom of the crowd"></category></entry><entry><title>From zero to â€œÎµ-roâ€: Infinitesimals, floating-point, convergence, and randomÂ error</title><link href="http://tabidots.github.io/2019/01/from-zero-to-ero" rel="alternate"></link><published>2019-01-19T17:32:22+07:00</published><updated>2019-01-19T17:32:22+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-19:/2019/01/from-zero-to-ero</id><summary type="html">&lt;p&gt;Who knew so much could be said about a value so small? I didnâ€™t even cover&amp;nbsp;everything!&lt;/p&gt;</summary><content type="html">
&lt;p&gt;One of the questions I left open from my blogâ€™s inaugural post on linear regression was &lt;em&gt;How do you know when youâ€™ve reachedÂ convergence?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;That led me to learning about &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (epsilon), or I suppose re-learning, if you count high school calculus, in which I think it made a single brief appearance in the formal definition ofÂ limit.&lt;/p&gt;
&lt;p&gt;Iâ€™m going to take a rather circuitous route to explaining the role of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; in linear regression, as I kept discovering interesting new things along theÂ way.&lt;/p&gt;
&lt;p&gt;Hopefully your attention span is larger than the value of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (harÂ har)!&lt;/p&gt;
&lt;h1 id="all-the-small-things"&gt;All the smallÂ things&lt;/h1&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; can be found in a variety of contexts, but it always represents an &lt;strong&gt;infinitesimal&lt;/strong&gt;: a quantity that it is infinitely small and basically zero, but not zero-y &lt;em&gt;enough&lt;/em&gt; to notÂ exist.&lt;/p&gt;
&lt;p&gt;In high school calculus, &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is the differenceÂ between&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the value of a function, &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, near its limit point, &lt;span class="math"&gt;\(x \rightarrow a\)&lt;/span&gt;,Â and&lt;/li&gt;
&lt;li&gt;the hypothetical value &lt;span class="math"&gt;\(L\)&lt;/span&gt; that the function appears to tend toward at the actual limit point &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the distance between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(a\)&lt;/span&gt; (which is called &lt;span class="math"&gt;\(\delta\)&lt;/span&gt;, delta) shrinks, so does &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, so the upper bounds of the two quantities move inÂ tandem.&lt;/p&gt;
&lt;p&gt;You can also use &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; to describe derivatives; i.e., if &lt;span class="math"&gt;\(dx = \delta\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{dy}{dx} = \epsilon\)&lt;/span&gt;, you can imagine how shrinking one shrinks the other, and more importantly, that the ideal value for both of them is &lt;em&gt;as close as you can get to zero without vanishing&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="derivative animation" src="../../images/derivative_animation.gif" title="source: https://sites.google.com/a/student.ashcoll.school.nz/bcashcoll/13-mac/differentiation-as-3-6-91578"/&gt;&lt;/p&gt;
&lt;h1 id="machine-epsilon"&gt;MachineÂ epsilon&lt;/h1&gt;
&lt;p&gt;There is also a quantity that computer scientists call &lt;strong&gt;machine epsilon&lt;/strong&gt;, which defines the smallest number that a given computing environment canÂ represent.&lt;/p&gt;
&lt;p&gt;It is the largest &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; thatÂ satisfies&lt;/p&gt;
&lt;div class="math"&gt;$$ 1 + \epsilon = 1 $$&lt;/div&gt;
&lt;p&gt;which looks like another math koan, or at least some Orwellian â€œnewmathâ€ like &lt;span class="math"&gt;\(2 + 2 = 5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Actually, that &lt;span class="math"&gt;\(+\)&lt;/span&gt; should have a &lt;span class="math"&gt;\(\bigcirc\)&lt;/span&gt; around it: &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;, givingÂ us&lt;/p&gt;
&lt;div class="math"&gt;$$ 1 \oplus \epsilon = 1 $$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; means &lt;em&gt;floating-point addition&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;Now, in my programming life, I have not had a need for significant precision until now, with my new interest in fields that require numericalÂ computing.&lt;/p&gt;
&lt;p&gt;I also started with Python (well, JavaScript if you go way back), so I never even really had to distinguish between &lt;code class="highlight"&gt;float&lt;/code&gt;s and &lt;code class="highlight"&gt;int&lt;/code&gt;s in my codeâ€”much less think about the consequences or even know the difference, really. I just knew that if you wanted a number with a decimal point, it had to be a &lt;code class="highlight"&gt;float&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you get into doing real math with computers, though, you canâ€™t just keep &lt;code class="highlight"&gt;float&lt;/code&gt;ing along like that, because the limitations of machines come intoÂ play.&lt;/p&gt;
&lt;h2 id="floating-point-arithmetic"&gt;Floating-pointÂ arithmetic&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Floating-point arithmetic&lt;/em&gt; was devised because computer memory, especially in machines from the very early days of computing, can only allocate a finite amount of resources toÂ data.&lt;/p&gt;
&lt;p&gt;Floating-point is basically scientific notation. Letâ€™s consider the mass of a proton in scientific notation (this post isnâ€™t about physics, but as an astoundingly small value, itâ€™s a goodÂ example):&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} &amp;amp; 0.0000000000000000000000000016726 \space \textrm{kg} \\
&amp;amp;= \underbrace{1.6726}_{\textrm{significand}} \times \underbrace{10}_{\textrm{base}}  \!\!\!\!^{\overbrace{-27}^{\textrm{exponent}}} \space \textrm{kg}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;The goal of scientific notation is to translate numbers of extreme magnitude into more human-friendly terms. But computers donâ€™t operate like humans do. So how can we translate this number into &lt;em&gt;computer&lt;/em&gt;-friendlyÂ terms?&lt;/p&gt;
&lt;p&gt;First, computers use &lt;em&gt;binary&lt;/em&gt;, not base 10 (decimal system). Of course, we know that, but what does that actually mean? I always thought binary sequences were random collections of ones and zerosâ€”as if computer programs were like canvasses assaulted by a digital Jackson Pollock armed with buckets of bits andÂ bytes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="binary counter" src="https://upload.wikimedia.org/wikipedia/commons/7/75/Binary_counter.gif" title="(source: Ephert [CC BY-SA 4.0] https://creativecommons.org/licenses/by-sa/4.0, from Wikimedia Commons)"/&gt;&lt;/p&gt;
&lt;p&gt;I am not well-versed enough in binary to explain this in words, but you can definitely see some sort of pattern in the movement of the valuesÂ here.&lt;/p&gt;
&lt;p&gt;Anyway, letâ€™s rewrite the mass of a proton in binary (scroll sideways, itâ€™s a longÂ one):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100001001000010
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And in binary scientific notation (sortÂ of):&lt;/p&gt;
&lt;div class="math"&gt;$$ 1.00001001000010_{\textrm{bin}} \times 2^{-89} $$&lt;/div&gt;
&lt;p&gt;Almost there. Letâ€™s take a look at how computers actually store floating-pointÂ numbers:&lt;/p&gt;
&lt;p&gt;&lt;img alt="64-bit floating point storage structure" src="https://upload.wikimedia.org/wikipedia/commons/a/a9/IEEE_754_Double_Floating_Point_Format.svg" title="source: Codekaizen - Own work, [CC BY-SA 4.0] https://commons.wikimedia.org/w/index.php?curid=3595583"/&gt;&lt;/p&gt;
&lt;p&gt;Every number is allocated 64 bits of memory (this can vary, but letâ€™s stick with 64). There are 52 bits for the &lt;em&gt;mantissa&lt;/em&gt; (computer science term for significand), 11 for the exponent, and 1 for the sign (positve or negative). Each bit can be 1 or 0, which is the whole reason for using binary in the firstÂ place.&lt;/p&gt;
&lt;p&gt;Storing the mantissa, then, is just a matter of finding a way to fit the relevant sequence of ones and zeros into the 52 slots allocated for it. If there are too many digits, chop them off; if there are too few (as in this case), add padding. This is where floating-point differs from scientificÂ notation.&lt;/p&gt;
&lt;div class="math"&gt;$$ 00000000 \space 00000000 \space 00000000 \space 00000000 \space
00000\textcolor{teal}{100 \space 00100100 \space 0010} \times 2^{-51} $$&lt;/div&gt;
&lt;p&gt;Looks like a lot of zeros. Incidentally, the method to obtain a base-10 number from this can be expressed mathematically in a coolÂ way:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
&amp;amp; \Bigg(\sum_{n=0}^{p-1} \textrm{bit}_n \times 2^{-n} \Bigg) \times 2^e \\
&amp;amp;= (0 \times 2^{-0} + 0 \times 2^{-1} + 0 \times 2^{-2} + \cdots + 1 \times 2^{-50} + 0 \times 2^{-51}) \times 2^{-51} \\
&amp;amp;= (1 \times 2^{-37} + 1 \times 2^{-42} + 1 \times 2^{-45} + 1 \times 2^{-50}) \times 2^{-51}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\textrm{bit}_n\)&lt;/span&gt; is the binary value of the bit at index &lt;span class="math"&gt;\(n\)&lt;/span&gt; (from the left, starting at 0), &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the precision (number of bits), and &lt;span class="math"&gt;\(e\)&lt;/span&gt; is the exponent (which just happens to be &lt;span class="math"&gt;\(-51\)&lt;/span&gt; here; it has nothing to do with the number of bits for theÂ mantissa).&lt;/p&gt;
&lt;p&gt;Okay, that was fun. NowÂ what?&lt;/p&gt;
&lt;h2 id="rounding-errors"&gt;RoundingÂ errors&lt;/h2&gt;
&lt;p&gt;&lt;img alt="xkcd comic" src="https://imgs.xkcd.com/comics/e_to_the_pi_minus_pi.png" title="XKCD #217 https://xkcd.com/217/"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If there are too many digits, chop them off&lt;/em&gt;. This opens up a big can of worms. Or an infinitesimal one, given the topic at hand (harÂ har).&lt;/p&gt;
&lt;p&gt;In exchange for the computability (read: speed and power) and storeability of floating-point numbers, we have to accept a limit to their precision. The digits of &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;, for example, repeat endlessly. We canâ€™t expect a computer to handle them all, or a calculator, for thatÂ matter.&lt;/p&gt;
&lt;p&gt;Even totally pedestrian numbers like &lt;span class="math"&gt;\(\frac{1}{3}\)&lt;/span&gt; go on forever. This means that computers cannot reason about fractional quantities in the way that the human mindÂ can.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
1 + \frac{1}{3} &amp;amp;= \frac{4}{3}_{\textrm{human}} \\[0.8em]
&amp;amp;= 1.333\overline{3}_{\textrm{dec}} \\
&amp;amp;= 1.1010\overline{10}_{\textrm{bin}}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;That is, unless you are working in some baller language like Clojure that has &lt;code class="highlight"&gt;Ratio&lt;/code&gt; dataÂ types:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_0"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c1"&gt;; 1/3&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;type&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;; clojure.lang.Ratio&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;; 4/3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_1" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_1"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 1.3333333333333333&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Most languages seem to have implemented some sort of corrective mechanism that works in simple cases, but notice what happens with the following expressions inÂ Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="c1"&gt;# 1.0&lt;/span&gt;
&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 1.0&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="c1"&gt;# 0.9999999999999998&lt;/span&gt;
&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="c1"&gt;# False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Keep in mind that weâ€™re working in binary here. Unexpectedly (if you are not proficient in binary), many â€œsimpleâ€ numbers that donâ€™t repeat in decimal do inÂ binary:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\frac{1}{10} &amp;amp;= 0.1_{\textrm{dec}} \\
&amp;amp;= 0.1100\overline{1100}_{\textrm{bin}}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;All of this leads to &lt;strong&gt;floating-point rounding errors&lt;/strong&gt; that can quickly snowball into massively erroneous output over many iterations. Hereâ€™s the example from the &lt;a href="https://youtu.be/8iGzBMboA0I?t=3247"&gt;Rachel Thomas lecture&lt;/a&gt;. Start with &lt;span class="math"&gt;\(x = \frac{1}{10}\)&lt;/span&gt; and keep applying the function to the output youÂ get:&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \begin{cases}
2x     &amp;amp; \textrm{if } x \leq \frac{1}{2} \\
2x - 1 &amp;amp; \textrm{if } x &amp;gt; \frac{1}{2}
\end{cases} $$&lt;/div&gt;
&lt;p&gt;If you do this by hand youÂ get&lt;/p&gt;
&lt;div class="math"&gt;$$ \{\tfrac{1}{10}, \tfrac{1}{5}, \tfrac{2}{5}, \tfrac{4}{5}, \tfrac{3}{5}, \overline{\tfrac{1}{5}, \tfrac{2}{5}, \tfrac{4}{5}, \tfrac{3}{5}}, \cdots\} $$&lt;/div&gt;
&lt;p&gt;but when you try to execute this on aÂ computer:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_1" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;use&lt;/span&gt; &lt;span class="ss"&gt;'clojure.pprint&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt;= &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;- &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;pprint&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;take &lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;iterate &lt;/span&gt;&lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;it doesnâ€™t take long before the value converges on 1, which is veryÂ bizarre.&lt;/p&gt;
&lt;h2 id="granularity"&gt;Granularity&lt;/h2&gt;
&lt;p&gt;Limited storage space leads to limited precision. A related consequence of this is that floating-point numbers are discrete, not continuous like the number line we picture in our minds. (As a kid, I think I imagined a spectrum. Fun? Yes. Accurate? Not sure.) Computers are capable of pretty precise calculations, but not perfectlyÂ precise.&lt;/p&gt;
&lt;p&gt;Think of floating-point numbers like pixels. While it is true that computer displays have become less and less â€œpixelly-lookingâ€ over the years, and text rendered on a Retina screen can almost look like a printed page, we know that such output still consists ofÂ pixels.&lt;/p&gt;
&lt;p&gt;The same is true for floating-point numbers. They allow for some degree of precision, but the number line they form is more like a dotted line than a solid line. (Even stranger, the density of the line changes at different scales, but Iâ€™ll leave that one for someone else toÂ explain!)&lt;/p&gt;
&lt;p&gt;Take the binary number &lt;code class="highlight"&gt;1.0&lt;/code&gt; (regular binary, not floating-point). This is equal to the decimal number 1 as well. If we keep moving the 1 to the right and adding zeros accordingly, the value isÂ halved:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
1.0_\textrm{bin} = 1_\textrm{dec} \\
0.1_\textrm{bin} = \tfrac{1}{2}_\textrm{dec} \\
0.01_\textrm{bin} = \tfrac{1}{4}_\textrm{dec} \\
0.001_\textrm{bin} = \tfrac{1}{8}_\textrm{dec}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;It is pretty clear that with 52 slots for binary values, you are going to run out of room at some pointâ€”even with the orders-of-magnitude wiggle room that the exponentÂ provides.&lt;/p&gt;
&lt;p&gt;This means that after enough iterations, a value can no longer be halved. &lt;em&gt;That unhalvable value&lt;/em&gt; is the smallest difference that that computing environment can represent, and it is the distance between a given number and its closest possible neighbor on the floating-point numberÂ line.&lt;/p&gt;
&lt;p&gt;You could think of that as the size of a â€number pixel.â€ The whole pixel has the value of (say) its left edge, and any quantity that falls within the space of the pixel gets rounded to the edge of that pixel or the nextÂ one.&lt;/p&gt;
&lt;p&gt;Machine epsilon, then, is the &lt;em&gt;largest quantity&lt;/em&gt; that is less than the width of a number pixel. So it makes sense, then, that &lt;span class="math"&gt;\(1 \oplus \epsilon = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This means that machine epsilon is the &lt;em&gt;largest possible rounding error&lt;/em&gt; of a floating-pointÂ system.&lt;/p&gt;
&lt;p&gt;It also means that on our continuous human number line, as soon as you move &lt;em&gt;rightward&lt;/em&gt; of machine epsilon, you have entered the territory of the next machine-perceptible number. That is how machine epsilon defines the smallest possible difference that the system canÂ represent.&lt;/p&gt;
&lt;h2 id="calculating-machine-epsilon"&gt;Calculating machineÂ epsilon&lt;/h2&gt;
&lt;p&gt;Before I started writing this post, I just wanted to see the code to calculate machine epsilon. But I didnâ€™t fully understand what was going on, and my dissatisfaction with that led me to go back through all the stuff I just explained. (Of course, it didnâ€™t help that I wasnâ€™t sure about the syntax of &lt;code class="highlight"&gt;loop&lt;/code&gt; in ClojureÂ ğŸ™ˆ)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;; Source: https://github.com/log0ymxm/gorilla-worksheets/blob/master/src/machine-epsilon.clj&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;loop &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;or &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt;= &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;gt; &lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;dec &lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;, &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="nv"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;inc &lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="nv"&gt;s&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="c1"&gt;; [53 2.220446049250313E-16]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This loop is trying to find the unhalvable value I wrote about earlier. Start with &lt;code class="highlight"&gt;s = 1.0&lt;/code&gt; and keep halving that (moving the binary 1 rightward) until &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt;= &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt; or &lt;span class="math"&gt;\(1 + s \leq 1\)&lt;/span&gt;. That is, until the computer no longer recognizes &lt;code class="highlight"&gt;s&lt;/code&gt; as havingÂ value.&lt;/p&gt;
&lt;p&gt;Once that happens, youâ€™ve gone too far and fallen within the bounds of a number pixel. To find the edge of the next pixelâ€”that is, the next adjacent perceptibly different numberâ€”move the binary 1 left by one place (in decimal, thatâ€™s multiplying byÂ 2).&lt;/p&gt;
&lt;p&gt;Thatâ€™s the reason for the final &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="nv"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt; once the terminating condition isÂ met.&lt;/p&gt;
&lt;p&gt;&lt;code class="highlight"&gt;k&lt;/code&gt; tells us that the 1 is in the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th place after the decimal point. Here, thatâ€™s the 53rd place. Thatâ€™s no surprise; we know itâ€™s a 64-bit number. But &lt;code class="highlight"&gt;k&lt;/code&gt; could be larger or smaller depending on the precision of the floating-point system, with higher values meaning more available places and thus higherÂ precision.&lt;/p&gt;
&lt;p&gt;Julia, NumPy, and R have built-in ways to find machine epsilon (or rather, the value just a hair larger than machine epsilon). Of the three, Juliaâ€™s value is the most precise (to the same level of precision as ClojureÂ above).&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_0"&gt;Julia&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# source: https://docs.julialang.org/en/v1/manual/integers-and-floating-point-numbers/index.html&lt;/span&gt;
&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;         
&lt;span class="mf"&gt;2.220446049250313e-16&lt;/span&gt;
&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;# 32-bit (single-precision) number&lt;/span&gt;
&lt;span class="mf"&gt;1.1920929f-7&lt;/span&gt;
&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_3_1" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_1"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# source: https://stackoverflow.com/questions/19141432/python-numpy-machine-epsilon&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;        
&lt;span class="mf"&gt;2.22044604925e-16&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="c1"&gt;# 32-bit (single-precision) number&lt;/span&gt;
&lt;span class="mf"&gt;1.19209e-07&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
 &lt;span class="sb"&gt;``&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;

&lt;span class="sb"&gt;``&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;tab&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="c1"&gt;# source: https://stackoverflow.com/questions/2619543/how-do-i-obtain-the-machine-epsilon-in-r&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Machine&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mf"&gt;2.220446e-16&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Machine&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Just for completeness, this is that value in mathÂ notation:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\epsilon_{\textrm{mach}} &amp;amp;= 2.220446049250313 \times 10^{-16} \\
&amp;amp;= 0.000 \space 000 \space 000 \space 000 \space 000 \space 000 \space 000 \space
222 \space 044 \space 604 \space 9250
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;So, even though I donâ€™t immediately see myself caring about the specific &lt;em&gt;value&lt;/em&gt; of machine epsilon (as opposed to its &lt;em&gt;implications&lt;/em&gt;), thatâ€™s prettyÂ neat.&lt;/p&gt;
&lt;p&gt;Speaking of the implications of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;â€”of the machine and non-machine varietyâ€”I have to bring the discussion back to what brought me to &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; in the first place: convergence in linearÂ regression.&lt;/p&gt;
&lt;h1 id="linear-regression"&gt;LinearÂ regression&lt;/h1&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; has &lt;em&gt;two&lt;/em&gt; meanings in linear regression depending on whether you are creating the model or using theÂ model.&lt;/p&gt;
&lt;h2 id="convergence-threshold"&gt;ConvergenceÂ threshold&lt;/h2&gt;
&lt;p&gt;&lt;a href="/2019/01/loopless-loop"&gt;Previously&lt;/a&gt;, we performed stochastic gradient descent in a way that we could see the cost decreasing with every iteration, but we still had to adjust the number of iterations manually and judge convergence by looking through the output to find the point where the next iteration wasnâ€™t really worthÂ it.&lt;/p&gt;
&lt;p&gt;Itâ€™s conceptually very simple to have the computer do this for you. On each iteration, just keep track of the cost obtained after the previous iteration and compare it to the current cost. If the difference is below a certain threshold value, stopÂ iterating.&lt;/p&gt;
&lt;div class="math"&gt;$$ | J(\theta_{\textrm{current}}) - J(\theta_{\textrm{previous}}) | &amp;lt; \epsilon $$&lt;/div&gt;
&lt;p&gt;In this context, the threshold value is called &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Something like &lt;span class="math"&gt;\(0.0001\)&lt;/span&gt; might beÂ adequate.&lt;/p&gt;
&lt;p&gt;Letâ€™s take first part of the code from the lastÂ post:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# padding for bias column&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# this just fixes a quirk of sklearn's output&lt;/span&gt;

&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;cost_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and modify our &lt;code class="highlight"&gt;train_model&lt;/code&gt; function to show us the difference between the current and the last cost on every 50th iteration, and train for 1500Â epochs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="hll"&gt;    &lt;span class="n"&gt;last_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/span&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
&lt;span class="hll"&gt;        &lt;span class="n"&gt;this_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;this_cost&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;last_cost&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;        &lt;span class="n"&gt;last_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;this_cost&lt;/span&gt;
&lt;/span&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At 600 epochs, the difference falls to about &lt;code class="highlight"&gt;0.001&lt;/code&gt; and at 700 epochs, it falls to about &lt;code class="highlight"&gt;0.0001&lt;/code&gt;. Considering the difference between the initial 20 iterations was in the hundreds, I think &lt;code class="highlight"&gt;0.001&lt;/code&gt; is a sufficiently good &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Letâ€™s rewrite the function to stop training based on a value of &lt;code class="highlight"&gt;epsilon&lt;/code&gt; rather than a number &lt;code class="highlight"&gt;epochs&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="hll"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;last_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="hll"&gt;    &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="c1"&gt;# just to know the value; not critical for training&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;        &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/span&gt;        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
        &lt;span class="n"&gt;this_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="hll"&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;this_cost&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;last_cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;span class="hll"&gt;            &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/span&gt;        &lt;span class="n"&gt;last_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;this_cost&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Completed {epochs} epochs of training."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Final cost: {cost(X, y, weights)}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Completed 612 epochs of training.&lt;/span&gt;
&lt;span class="c1"&gt;# Final cost: 0.05001815274191081&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Nice!&lt;/p&gt;
&lt;h2 id="random-error"&gt;RandomÂ error&lt;/h2&gt;
&lt;p&gt;I sort of lied when I said linear regressionÂ is&lt;/p&gt;
&lt;div class="math"&gt;$$ h_{\theta}(x) = \theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n $$&lt;/div&gt;
&lt;p&gt;I mean, it is. But that function gives the idealized, &lt;em&gt;predicted&lt;/em&gt; value, the &lt;span class="math"&gt;\(\hat y\)&lt;/span&gt;. The &lt;em&gt;real&lt;/em&gt; valueÂ is&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n + \textcolor{magenta}{\epsilon} $$&lt;/div&gt;
&lt;p&gt;(And in statistics they use &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; rather than &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, just to keep you on yourÂ toes.)&lt;/p&gt;
&lt;p&gt;This &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is called the &lt;strong&gt;random error&lt;/strong&gt;, which sounds like the consequence of a really sloppy programmer, but is simply a way of dealing with the unavoidable fact that no model can beÂ perfect.&lt;/p&gt;
&lt;p&gt;Remember how we used the &lt;em&gt;actual&lt;/em&gt; error (&lt;span class="math"&gt;\(\hatY - Y\)&lt;/span&gt;) to compute the distance between our hypothetical line of best fit and each data point? The random error is basically sayingÂ that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The regression line is the line that best fitsâ€”not &lt;em&gt;perfectly&lt;/em&gt; fitsâ€”the data used to create it,Â and&lt;/li&gt;
&lt;li&gt;Because of that, predictions made with the model are not likely to fall directly &lt;em&gt;on&lt;/em&gt; theÂ line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That doesnâ€™t mean they &lt;em&gt;wonâ€™t&lt;/em&gt;, but we canâ€™t know for sure, and itâ€™s not likely. This uncertainty is captured by &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; (which we should hope is a small value, if not infinitesimal ğŸ˜…). &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is considered to be takenÂ from&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a normally distributed (bell curve) set ofÂ values&lt;/li&gt;
&lt;li&gt;with a mean ofÂ 0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second property makes sense; our line is the best-fitting line, so its underestimates should equal its overestimates). The first property is arbitraryâ€”you could choose any kind of probability distribution, but a bell curve is apparently the simplest one that is correct enough frequentlyÂ enough.&lt;/p&gt;
&lt;p&gt;This is more of a theoretical shim than anything that concerns atually coding a model, but itâ€™s still an importantÂ variable.&lt;/p&gt;
&lt;p&gt;In a future post, I will talk about a topic that takes &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; to the next level. StayÂ tuned!&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/Floating_Point/Epsilon"&gt;Floating Point/Epsilon&lt;/a&gt;,Â Wikibooks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic"&gt;Floating-point arithmetic&lt;/a&gt;,Â Wikipedia&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www0.gsb.columbia.edu/faculty/pglasserman/B6014/Regression.pdf"&gt;B6014 Managerial Statistics: Linear Regression&lt;/a&gt;, Columbia BusinessÂ School&lt;/li&gt;
&lt;li&gt;&lt;a href="https://math.stackexchange.com/questions/1051863/assumption-of-a-random-error-term-in-a-regression"&gt;Assumption of a Random error term in a regression&lt;/a&gt;, StackExchangeÂ Mathematics&lt;/li&gt;
&lt;/ul&gt;</content><category term="convergence"></category><category term="floating-point"></category><category term="error"></category><category term="epsilon"></category></entry><entry><title>PIM notes, Chapter 2Â (Polynomials)</title><link href="http://tabidots.github.io/2019/01/pim-notes-chapter-2" rel="alternate"></link><published>2019-01-17T12:54:19+07:00</published><updated>2019-01-17T12:54:19+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-17:/2019/01/pim-notes-chapter-2</id><summary type="html">&lt;p&gt;My first experience with something that resembles a math textbook in many years, but this time with â€œbig kid math.â€ Itâ€™s&amp;nbsp;hard!&lt;/p&gt;</summary><content type="html">
&lt;p&gt;I started reading Jeremy Kunâ€™s &lt;em&gt;&lt;a href="https://pimbook.org/"&gt;A Programmerâ€™s Introduction to Mathematics&lt;/a&gt;&lt;/em&gt;. This is just a collection of my notes from Chapter 2, or code/math that I felt like writing/typesetting as an exercise while working through theÂ chapter.&lt;/p&gt;
&lt;p&gt;I activated the SuperFences Markdown plugin in the blogâ€™s settings, so itâ€™s really cool to write code for the same thing in different languages side-by-side. (The Java lexer is a little off,Â though.)&lt;/p&gt;
&lt;p&gt;Note: The chapter is divided into the â€œmain material,â€ an implementation of something that uses the relevant math, and exercises. I had actually gotten through the material and code part of the chapter last week, before I wrote about Markov matrices, and thought Iâ€™d be able to publish the complete notes in oneÂ go.&lt;/p&gt;
&lt;p&gt;However, the exercises are time-consuming and quickly get very difficult, so my impatience compels me to split this post in two, since itâ€™d be strange to be writing about &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; notation after covering way more advanced material! Iâ€™ll publish my answers to the exercises anotherÂ week.&lt;/p&gt;
&lt;h1 id="sum-summation"&gt;&lt;span class="math"&gt;\(\sum\)&lt;/span&gt;Â (Summation)&lt;/h1&gt;
&lt;p&gt;Summation notation wasnâ€™t new to me (I learned it the hard way trying to make sense of the linear regression stuff), nor was the Python equivalent. However, since it was presented in Java, I thought it was good opportunity to see the correspondence between Python and Java. When I first started learning to code (beyond web development), I ran away from Java with my tail between my legs, but now it makes a lot more sense. Itâ€™s just terribly verbose andÂ inefficient.&lt;/p&gt;
&lt;p&gt;Also, I wanted to use an easy example to get reacquainted with Clojure, my other favorite language, and pick up some R and Julia along the wayÂ too.&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^x i $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;list_comp_sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_1" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;sum-to&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sum-to&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_2" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;sumTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;mySum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;mySum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mySum&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;sumTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$9&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_3" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_3"&gt;Julia&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_4" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_4"&gt;R&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sum_to&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
  &lt;span class="nf"&gt;for &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;[1]&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="prod-pi-product"&gt;&lt;span class="math"&gt;\(\prod\)&lt;/span&gt;Â (Pi-product)&lt;/h1&gt;
&lt;p&gt;This was new to me, and so was the existence of the &lt;code class="highlight"&gt;*=&lt;/code&gt; operator, which I guess I had never had a needÂ for.&lt;/p&gt;
&lt;div class="math"&gt;$$ g(x) = \prod_{j=1}^x i $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_1" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;mult-all&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;*&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;mult-all&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_2" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;multAll&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;myProd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;myProd&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;myProd&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;multAll&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$10&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_3" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_3"&gt;Julia&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_4" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_4"&gt;R&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mult_all&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
  &lt;span class="nf"&gt;for &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;[1]&lt;/span&gt; &lt;span class="m"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="sumprod-nested-product-in-sum"&gt;&lt;span class="math"&gt;\(\sum\prod\)&lt;/span&gt; (Nested product inÂ sum)&lt;/h1&gt;
&lt;p&gt;Pretty wild. I tried really hard to translate this into Clojure, but I couldnâ€™tÂ as-is.&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^n \textrm{bar}(i) \Bigg(\prod_{j \not = i} \textrm{foo}(i, j)\Bigg) $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;inner&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;
        &lt;span class="n"&gt;inner&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_3_1" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_1"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;innerProd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;innerProd&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="horners-method"&gt;Hornerâ€™sÂ Method&lt;/h1&gt;
&lt;p&gt;I had never heard of this until I encountered it in the code for the authorâ€™s &lt;code class="highlight"&gt;Polynomial&lt;/code&gt; class. It is definitely easier to understand how it works in Python than it is to understand why it worksÂ mathematically!&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \sum_{i=0}^{n-1} a_i x^i &amp;amp;= a_0 + a_1 x + a_2 x^2 + \cdots + a_{n-1} x^{n-1} \\
&amp;amp;= a_0 + x(a_1 + x(a_2 + \cdots + x\,a_{n-1})) \end{aligned} $$&lt;/div&gt;
&lt;p&gt;Cooler still, the recursion evident in the Python code means that it can be implemented as a &lt;code class="highlight"&gt;reduce&lt;/code&gt; in functional programming, making it extremely concise andÂ loopless.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(f(x) = 2x^3 + 4x + 3\)&lt;/span&gt;, letâ€™s find &lt;span class="math"&gt;\(f(2)\)&lt;/span&gt; with Hornerâ€™sÂ method.&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_4_0" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;horners_method&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;horners_method&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_4_1" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;horners-method&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;coefs&lt;/span&gt; &lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;%2&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="nv"&gt;%1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reverse &lt;/span&gt;&lt;span class="nv"&gt;coefs&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;horners-method&lt;/span&gt; &lt;span class="nv"&gt;coefs&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;27&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_4_2" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="nf"&gt;hornersMethod&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;];&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;hornersMethod&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;},&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$32&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;27.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="nested-polynomials"&gt;NestedÂ polynomials&lt;/h1&gt;
&lt;p&gt;Speaking of nested polynomials, in the section on interpolating polynomials (normal ones), I was stuck on this line in the function &lt;code class="highlight"&gt;single_term()&lt;/code&gt; for aÂ bit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;single_term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;

        &lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="hll"&gt;        &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;yi&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function is supposed to get us the term of the polynomial for point &lt;span class="math"&gt;\(i\)&lt;/span&gt; of the points we feed it. Itâ€™s this, without theÂ summation:&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^n y_i \Bigg(\prod_{j \not= i} \frac{x - x_j}{x_i - x_j}\Bigg) $$&lt;/div&gt;
&lt;p&gt;How does the fraction &lt;span class="math"&gt;\(\frac{x - x_j}{x_i - x_j}\)&lt;/span&gt; get broken down into &lt;code class="highlight"&gt;Polynomial(&lt;/code&gt;&lt;span class="math"&gt;\(\frac{-x_j}{x_i - x_j}, \frac{1}{x_i - x_j}\)&lt;/span&gt;&lt;code class="highlight"&gt;)&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;code class="highlight"&gt;&lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;/code&gt; produces a polynomial &lt;span class="math"&gt;\(a\textcolor{lightgray}{x^0} + b\textcolor{orange}x\textcolor{lightgray}{^1} + c\textcolor{orange}{x^2}\)&lt;/span&gt;, so &lt;code class="highlight"&gt;&lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;&lt;/code&gt; yields&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{-\textcolor{maroon}{x_j}}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}} + \frac{1}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}}\textcolor{orange}x = \frac{\textcolor{orange}x - \textcolor{maroon}{x_j}}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}}$$&lt;/div&gt;
&lt;p&gt;Ah, makes sense. Itâ€˜s easy to miss (for me, anyway), but the function &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; isnâ€™t the only polynomial hereâ€”the term within the &lt;span class="math"&gt;\(\prod\)&lt;/span&gt; is itself also aÂ polynomial.&lt;/p&gt;
&lt;p&gt;At first, I thought it was just a clever trick, but the reason for factoring out the &lt;span class="math"&gt;\(x\)&lt;/span&gt; without a subscript is basically that it, unlike everything else in the entire function, &lt;em&gt;not&lt;/em&gt; being iterated over by either the &lt;span class="math"&gt;\(\prod\)&lt;/span&gt; (iterator &lt;span class="math"&gt;\(j\)&lt;/span&gt;) or &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; (iterator &lt;span class="math"&gt;\(i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Itâ€™s the general indeterminate quantity &lt;span class="math"&gt;\(\textcolor{orange}x\)&lt;/span&gt;, and not &lt;span class="math"&gt;\(\textcolor{teal}{x_i}\)&lt;/span&gt; or &lt;span class="math"&gt;\(\textcolor{maroon}{x_j}\)&lt;/span&gt; (i.e., the x-coordinate of one of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; points that we provided to the function), which are actually part of the coefficients here. Incidentally, separating the static &lt;code class="highlight"&gt;x&lt;/code&gt; from the dynamic &lt;code class="highlight"&gt;x&lt;/code&gt;s was a stumbling block for me as I imagined how to tackleÂ this.&lt;/p&gt;</content><category term="pimbook"></category></entry><entry><title>Zen Coding: A Markov chain inÂ Clojure</title><link href="http://tabidots.github.io/2019/01/markov-chain-in-clojure" rel="alternate"></link><published>2019-01-16T21:06:57+07:00</published><updated>2019-01-16T21:06:57+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-16:/2019/01/markov-chain-in-clojure</id><summary type="html">&lt;p&gt;Clojure is so terse and Zen-like. I love it. This is the Clojure translation of the Markov chain implementation I wrote in Python&amp;nbsp;yesterday.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Iâ€™m currently in the middle of, among learning many new things, getting back into Clojure. I never really could imagine deploying production-grade Clojure programs, but writing code in a functional paradigm is always a fascinating, challenging, and rewardingÂ exercise.&lt;/p&gt;
&lt;p&gt;Plus, any Lisp looks pretty cool syntactically, and such languages are always extremelyÂ terse.&lt;/p&gt;
&lt;p&gt;Hereâ€™s a Clojure adaptation of the &lt;span class="caps"&gt;HIV&lt;/span&gt;/&lt;span class="caps"&gt;AIDS&lt;/span&gt; Markov chain scenario model that I wrote about and implemented in Python yesterday. Look at how short itÂ is!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;clojupyter.misc.helper&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;helper&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;helper/add-dependencies&lt;/span&gt; &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;net.mikera/core.matrix&lt;/span&gt; &lt;span class="s"&gt;"0.62.0"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;use&lt;/span&gt; &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;clojure.core.matrix&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;m&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;hiv&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt; &lt;span class="mf"&gt;0.07&lt;/span&gt; &lt;span class="mf"&gt;0.02&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mf"&gt;0.93&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="mf"&gt;0.02&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mf"&gt;0.85&lt;/span&gt; &lt;span class="mf"&gt;0.15&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;p-0&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;0.85&lt;/span&gt;, &lt;span class="mf"&gt;0.1&lt;/span&gt;, &lt;span class="mf"&gt;0.05&lt;/span&gt;, &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;evolutions&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;p-matrix&lt;/span&gt; &lt;span class="nv"&gt;initial&lt;/span&gt; &lt;span class="nv"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;reductions&lt;/span&gt; &lt;span class="nv"&gt;m/mmul&lt;/span&gt;
              &lt;span class="nv"&gt;initial&lt;/span&gt;
              &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;repeat &lt;/span&gt;&lt;span class="nv"&gt;steps&lt;/span&gt; &lt;span class="nv"&gt;p-matrix&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;future-distribution&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;p-matrix&lt;/span&gt; &lt;span class="nv"&gt;initial&lt;/span&gt; &lt;span class="nv"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;last&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;evolutions&lt;/span&gt; &lt;span class="nv"&gt;p-matrix&lt;/span&gt;
                &lt;span class="nv"&gt;initial&lt;/span&gt;
                &lt;span class="nv"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;watch-evolve&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;column&lt;/span&gt; &lt;span class="nv"&gt;p-matrix&lt;/span&gt; &lt;span class="nv"&gt;initial&lt;/span&gt; &lt;span class="nv"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;evolutions&lt;/span&gt; &lt;span class="nv"&gt;p-matrix&lt;/span&gt;
                  &lt;span class="nv"&gt;initial&lt;/span&gt;
                  &lt;span class="nv"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;get-column&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;get-column&lt;/span&gt; &lt;span class="nv"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Itâ€™s written in a way that works in &lt;a href="https://blog.nteract.io/hydrogen-interactive-computing-in-atom-89d291bcc4dd"&gt;Hydrogen&lt;/a&gt; for Atom (i.e., via a hidden Jupyter Notebook). Iâ€™m not sure how youâ€™d go about importing &lt;a href="https://github.com/mikera/core.matrix/"&gt;core.matrix&lt;/a&gt; in a one-off &lt;span class="caps"&gt;REPL&lt;/span&gt; session (not a Leiningen project), but now that Iâ€™ve discovered Hydrogen, I doubt there will be many terminal &lt;span class="caps"&gt;REPL&lt;/span&gt; sessions in myÂ future!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Coding in Clojure means non-object-oriented and no mutable state. You can write a function to compute the probability distribution at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, but the system wonâ€™t stay in thatÂ state.&lt;/p&gt;
&lt;p&gt;In fact, there &lt;em&gt;is&lt;/em&gt; no systemâ€”&lt;strong&gt;only evolution&lt;/strong&gt;. In that sense, you quite literally end up with a &lt;em&gt;memoryless&lt;/em&gt; Markov chain. The design philosophy of Clojure is all about impermanence, which is very Zen, when you think about it. (Would that make each program aÂ koan?)&lt;/p&gt;
&lt;p&gt;Letâ€™s take a quick tour of theÂ functions.&lt;/p&gt;
&lt;p&gt;First, there is &lt;code class="highlight"&gt;evolutions [p-matrix initial steps]&lt;/code&gt;. Thatâ€™s a base function that works as a building block for the other functions. In pseudo-math, it would look likeÂ this:&lt;/p&gt;
&lt;div class="math"&gt;$$ E(P, \pi^{(0)}, t) = {\pi^{(1)}, \cdots, \pi^{(t)}} $$&lt;/div&gt;
&lt;p&gt;Itâ€™s easy to see that computing some &lt;span class="math"&gt;\(\pi^{(n)}\)&lt;/span&gt;, the probability distribution (state of the system) at a given time, can be accomplished with a single &lt;code class="highlight"&gt;reduce&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But Clojure has an interesting function &lt;code class="highlight"&gt;reductions&lt;/code&gt; (&lt;a href="https://clojuredocs.org/clojure.core/reductions"&gt;docs&lt;/a&gt;), which stores all of the intermediate outputs in a list, with the expected output of &lt;code class="highlight"&gt;reduce&lt;/code&gt; as the last entry. So it makes sense to write two functionsÂ here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one, &lt;code class="highlight"&gt;evolutions&lt;/code&gt;, that gives us &lt;em&gt;all&lt;/em&gt; &lt;span class="math"&gt;\(\{\pi^{(1)}, \cdots, \pi^{(t)}\}\)&lt;/span&gt;,Â and&lt;/li&gt;
&lt;li&gt;another, &lt;code class="highlight"&gt;future-distribution&lt;/code&gt;, that takes the same inputs and outputs only the &lt;code class="highlight"&gt;last&lt;/code&gt; evolution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;InÂ pseudo-math:&lt;/p&gt;
&lt;div class="math"&gt;$$ F(P, \pi^{(0)}, t) = \pi^{(t)} $$&lt;/div&gt;
&lt;p&gt;InÂ action:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;future-distribution&lt;/span&gt; &lt;span class="nv"&gt;hiv&lt;/span&gt; &lt;span class="nv"&gt;p-0&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;; =&amp;gt; [[0.10334015640198398 0.24687062185910383 0.12037223090781371 0.5294169908310989]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What else can we do with the output of &lt;code class="highlight"&gt;evolutions&lt;/code&gt;? Well, we can choose one state of the system and watch its distribution evolve over time by picking out the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th item of each evolution (a rowÂ vector).&lt;/p&gt;
&lt;p&gt;In this case, we can observe how the proportion of a segment of the population (asymptomatic, symptomatic, has &lt;span class="caps"&gt;AIDS&lt;/span&gt;, dead) changes over &lt;span class="math"&gt;\(t\)&lt;/span&gt;Â time-steps.&lt;/p&gt;
&lt;p&gt;InÂ pseudo-math:&lt;/p&gt;
&lt;div class="math"&gt;$$ W(s, P, \pi^{(0)}, t) = \Big\{ P(s_0), P(s_1 | s_0), \cdots, P(s_t | s_{t-1}) \Big\} $$&lt;/div&gt;
&lt;p&gt;InÂ action:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;watch-evolve&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="nv"&gt;hiv&lt;/span&gt; &lt;span class="nv"&gt;p-0&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;; =&amp;gt; [0.85 0.765 0.6885 0.61965 0.5576850000000001 0.5019165000000001]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What a coolÂ language.&lt;/p&gt;</content><category term="clojure"></category><category term="recursion"></category><category term="markov chain"></category></entry><entry><title>Outer State Space Race: Markov chains andÂ matrices</title><link href="http://tabidots.github.io/2019/01/outer-state-space-race" rel="alternate"></link><published>2019-01-16T02:10:30+07:00</published><updated>2019-01-16T02:10:30+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-16:/2019/01/outer-state-space-race</id><summary type="html">&lt;p&gt;â€State spaceâ€ is the coolest term Iâ€™ve learned in a while. Also, I implemented a simple Markov chain, from scratch, as a Python&amp;nbsp;class.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;I had heard of Markov chains before, but because my perspective had always been tightly focused on language, I never felt they were reallyÂ useful.&lt;/p&gt;
&lt;p&gt;To review, a Markov chain basically describes a scenario or system where one event leads to other events (and so on, perhaps in a cycle) with varying probabilities, but &lt;em&gt;the probability of any one event leading to another given event is completely independent of all other probabilities&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.youtube.com/watch?v=0Il-y_WLTo4"&gt;the &lt;span class="caps"&gt;HIV&lt;/span&gt;/&lt;span class="caps"&gt;AIDS&lt;/span&gt; scenario model&lt;/a&gt; that originally got me interested in learning about Markov chains, the chances of your next â€œstepâ€â€”future health outcomeâ€”depend &lt;em&gt;only&lt;/em&gt; on your presentÂ status.&lt;/p&gt;
&lt;p&gt;This is called the &lt;strong&gt;Markov property&lt;/strong&gt;. Itâ€™s like the opposite of the Monty Hall problem I wrote about previously. The probability of there being a car behind Door 2 &lt;em&gt;changes&lt;/em&gt; after Monty opens Door 3. Meanwhile, in a Markov chain, every probability is independent of theÂ others.&lt;/p&gt;
&lt;p&gt;Markov chains, as far as I know, have limited usefulness in natural language processing, because sequences of language dataâ€”letters, words, sentences, paragraphsâ€”are highly dependent on not only the previous item, but to some extent &lt;em&gt;all&lt;/em&gt; previousÂ items.&lt;/p&gt;
&lt;p&gt;You can see the implications of this in Markov text generators, based on everything from Shakespeare to Eminem, that struggle to produce anything meaningful. Or in the case of part-of-speech tagging, it is simply inaccurate that a &lt;span class="caps"&gt;POS&lt;/span&gt; of a word could depend &lt;em&gt;only&lt;/em&gt; on the &lt;span class="caps"&gt;POS&lt;/span&gt; of the previousÂ word.&lt;/p&gt;
&lt;p&gt;(Even human brains falter in processing so-called &lt;em&gt;garden path sentences&lt;/em&gt;, such as â€œThe horse raced past the barn fell,â€ where you expect perhaps an adverb of some sort after the wordÂ â€œbarn.â€)&lt;/p&gt;
&lt;h1 id="matrix-izing-a-markov-chain"&gt;Matrix-izing a MarkovÂ chain&lt;/h1&gt;
&lt;p&gt;The simplest representation of a Markov chain is called a &lt;em&gt;transition diagram&lt;/em&gt;, which basically looks like a flowchart (although it could be cyclical) with arrows going between different states and probability values attached to thoseÂ arrows.&lt;/p&gt;
&lt;p&gt;Now, what recently got me interested in Markov chains, as I &lt;a href="2019/01/monty-hall-beginner"&gt;wrote previously&lt;/a&gt;, is the fact that they can also be represented by matrices, and manipulated with the tools of linearÂ algebra.&lt;/p&gt;
&lt;p&gt;The following is a Markov chain as a matrix, also known as a &lt;strong&gt;transition matrix&lt;/strong&gt;, since the values in the matrix represent the probabilities of transitioning from one state to another. In this case, suppose our system has fiveÂ states.&lt;/p&gt;
&lt;p&gt;(Annotating a matrix is beyond my &lt;span class="math"&gt;\(\LaTeX\)&lt;/span&gt; skillsâ€”and perhaps the capabilities of the &lt;span class="math"&gt;\(\KaTeX\)&lt;/span&gt; rendererâ€”so I will use a codeÂ block.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                  &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;want&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;go&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;
                      &lt;span class="n"&gt;A&lt;/span&gt;  &lt;span class="n"&gt;B&lt;/span&gt;  &lt;span class="n"&gt;C&lt;/span&gt;  &lt;span class="n"&gt;D&lt;/span&gt;  &lt;span class="n"&gt;E&lt;/span&gt;
                  &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;reading&lt;/span&gt; &lt;span class="n"&gt;across&lt;/span&gt;
                  &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;          &lt;span class="s"&gt;""&lt;/span&gt;
&lt;span class="hll"&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="kp"&gt;in&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;          &lt;span class="s"&gt;""&lt;/span&gt;
&lt;/span&gt;                  &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;          &lt;span class="s"&gt;""&lt;/span&gt;
                  &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;          &lt;span class="s"&gt;""&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The collection of &lt;em&gt;possible states&lt;/em&gt; of a system is called its &lt;strong&gt;state space&lt;/strong&gt; (what a cool term!) and notated as &lt;span class="math"&gt;\(S = \{1,2,3,\cdots,N\}\)&lt;/span&gt; in the general case. For our system, itâ€™s &lt;span class="math"&gt;\(S = \{A, B, C, D, E\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To give a more concrete example, for a text-generating Markov chain, the state space would include all the relevant units (letters of the alphabet plus spaces and punctuation for a character-by-character generator, words in the vocabulary for a word-by-word generator,Â etc.).&lt;/p&gt;
&lt;p&gt;In formal notation, a possible &lt;strong&gt;transition matrix&lt;/strong&gt; (a.k.a. Markov matrix, stochastic matrix) might look something likeÂ this:&lt;/p&gt;
&lt;div class="math"&gt;$$ P = \begin{bmatrix}
1\over4 &amp;amp; 1\over2 &amp;amp; 1\over4 \\[0.5em]
1\over3 &amp;amp; 0       &amp;amp; 2\over3 \\[0.5em]
1\over2 &amp;amp; 0       &amp;amp; 1\over2 \end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Capital &lt;span class="math"&gt;\(P\)&lt;/span&gt; represents the whole matrix and each small &lt;span class="math"&gt;\(p\)&lt;/span&gt; with subscripts (&lt;span class="math"&gt;\(p_{ij}\)&lt;/span&gt;, like &lt;span class="math"&gt;\(p_{DE}\)&lt;/span&gt;) represents the probability of going from state &lt;span class="math"&gt;\(i\)&lt;/span&gt; (some row) to state &lt;span class="math"&gt;\(j\)&lt;/span&gt; (some column in that row). &lt;span class="math"&gt;\(p_{ij}\)&lt;/span&gt; can also be used in a general sense to mean &lt;em&gt;all&lt;/em&gt; of the probabilities in theÂ matrix.&lt;/p&gt;
&lt;p&gt;Each row must add up to 1 because it includes every possible next step from thatÂ state.&lt;/p&gt;
&lt;p&gt;Also note that the probabilities of the state &lt;em&gt;staying the same&lt;/em&gt; from one time step to the next run along the diagonal from top left to bottom right, which means that the identity matrix would represent a static systemâ€”one that &lt;em&gt;never changes&lt;/em&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ P = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} $$&lt;/div&gt;
&lt;h1 id="when-pi-is-not-pi"&gt;When &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; is not &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;Now, a transition matrix by itself doesnâ€™t really have any added value compared to the transition diagram it represents, other than looking nicely organized. So letâ€™s make a simple matrix with some random values and do matrix-y stuff with it, like multiply it byÂ something.&lt;/p&gt;
&lt;p&gt;For the time being, I live in Southeast Asia and I eat mostly local food, which means that for any given meal, thereâ€™ll either be rice or noodles on my plate. (Not that Iâ€™m complainingâ€”everything is ridiculouslyÂ tasty!)&lt;/p&gt;
&lt;p&gt;Consider a system &lt;span class="math"&gt;\(X\)&lt;/span&gt; (thatâ€™s the conventional choice of letter) that describes my meals with a state space &lt;span class="math"&gt;\(S = \{N, R\}\)&lt;/span&gt;. The transition matrix &lt;span class="math"&gt;\(P\)&lt;/span&gt; isÂ then&lt;/p&gt;
&lt;div class="math"&gt;$$ P = \begin{bmatrix}
0.4 &amp;amp; 0.6 \\
0.2 &amp;amp; 0.8 \end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Again, I canâ€™t label the rows and columns of the matrix, so letâ€™s stick to alphabetical order and make &lt;span class="math"&gt;\(N\)&lt;/span&gt; first.Â Thus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{NN}\)&lt;/span&gt;, probability of repeating noodles: &lt;span class="math"&gt;\(0.4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{NR}\)&lt;/span&gt;, probability of rice after noodles: &lt;span class="math"&gt;\(0.6\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{RN}\)&lt;/span&gt;, probability of noodles after rice: &lt;span class="math"&gt;\(0.2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{RR}\)&lt;/span&gt;, probability of repeating rice: &lt;span class="math"&gt;\(0.8\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The initial state can be represented by a row vector with &lt;span class="math"&gt;\(S\)&lt;/span&gt; values (i.e., as many values as there are possible states), with a 1 in the spot corresponding to the current state and a 0 for all otherÂ states.&lt;/p&gt;
&lt;p&gt;If my first meal was a noodle dish, then I can express that initial state likeÂ this:&lt;/p&gt;
&lt;div class="math"&gt;$$ \pi^{(0)} = \begin{bmatrix}1 &amp;amp; 0\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;The math notation for this is not soÂ straightforward.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lowercase &lt;em&gt;pi&lt;/em&gt; here represents a given state, &lt;strong&gt;not&lt;/strong&gt; the constant &lt;em&gt;pi&lt;/em&gt; that relates to circles! I donâ€™t know who came up with that idea, but I donâ€™t think it was a wiseÂ choice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The superscript number is also not what it seems: itâ€™s not an exponent, but rather the &lt;em&gt;number of time steps from now&lt;/em&gt;. So &lt;span class="math"&gt;\(\pi^{(0)}\)&lt;/span&gt; means our state at 0 time steps from now, or in other words,Â now.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why is this vector of the more uncommon horizontal variety (i.e., a row vector)? Thatâ€™s a more complicated question (and the reason that I got stuck on the warmup problem in the Linear Algebra for CodersÂ course).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is the part where I turned to Python because I couldnâ€™t get any farther with the mathÂ alone.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;meals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="the-next-step-and-the-ones-thereafter"&gt;The next step (and the onesÂ thereafter)&lt;/h1&gt;
&lt;p&gt;To find the probabilities for the next meal, we can multiply this vector and the transition matrix together. If you &lt;a href="https://www.youtube.com/watch?v=kYB8IZa5AuE"&gt;imagine matrix-vector multiplication as a vector undergoing a linear transformation&lt;/a&gt;, then this sort of makesÂ sense.&lt;/p&gt;
&lt;p&gt;Thereâ€™s a catch, though. Transition matrices are set up to be read &lt;em&gt;across&lt;/em&gt;, from left to right, with each row telling us the next-step probabilities &lt;em&gt;if&lt;/em&gt; we are in thatÂ state:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;want&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;go&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;
                      &lt;span class="n"&gt;N&lt;/span&gt;    &lt;span class="n"&gt;R&lt;/span&gt;
&lt;span class="hll"&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="kp"&gt;in&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;  &lt;span class="mf"&gt;0.6&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;                  &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;  &lt;span class="mf"&gt;0.8&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So if we already &lt;em&gt;know&lt;/em&gt; that we will be in state &lt;span class="math"&gt;\(N\)&lt;/span&gt; to begin with, we can essentially filter out the other rows, leaving a singleÂ row:&lt;/p&gt;
&lt;div class="math"&gt;$$ \pi^{(1)} = \begin{bmatrix}p_{NN} &amp;amp; p_{NR}\end{bmatrix} = \begin{bmatrix}0.4 &amp;amp; 0.6\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Now, if we suppose that our state vector is a column vector &lt;span class="math"&gt;\(\vec v\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$ \vec v = \begin{bmatrix}1 \\ 0\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;and multiply it by theÂ matrix,&lt;/p&gt;
&lt;div class="math"&gt;$$ \pi^{(1)} \stackrel{?}{=} P\vec v
= \begin{bmatrix}
0.4 &amp;amp; 0.6 \\
0.2 &amp;amp; 0.8 \end{bmatrix} \begin{bmatrix}1 \\ 0\end{bmatrix}
= \begin{bmatrix}
1\cdot0.4 + \textcolor{lightgray}{0\cdot0.6} \\
1\cdot0.2 + \textcolor{lightgray}{0\cdot0.8} \end{bmatrix}
= \begin{bmatrix} 0.4 \\ 0.2 \end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Hmm, thatâ€™s not quite right. The values donâ€™t match what we know about our system, and we want a row, not aÂ column.&lt;/p&gt;
&lt;p&gt;Since the product of a matrix and a vector is the shape of the vector, &lt;span class="math"&gt;\(\pi^{(0)}\)&lt;/span&gt; should be a row vector if we want &lt;span class="math"&gt;\(\pi^{(1)}\)&lt;/span&gt; to be a row vector. We could also transpose the matrix, but then the rows of the matrix no longer represent probabilities of transitioning from a given state. Using a row vector maintains the idea that rows describe origin states and the property that values of rows add up toÂ 1.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\pi^{(1)} = \pi^{(0)}P &amp;amp;= \begin{bmatrix}1 &amp;amp; 0\end{bmatrix} \begin{bmatrix}
0.4 &amp;amp; 0.6 \\
0.2 &amp;amp; 0.8 \end{bmatrix} \\
&amp;amp;= \begin{bmatrix}
1\cdot0.4 + \textcolor{lightgray}{0\cdot0.2} &amp;amp;
1\cdot0.6 + \textcolor{lightgray}{0\cdot0.8} \end{bmatrix} \\
&amp;amp;= \begin{bmatrix} 0.4 &amp;amp; 0.6 \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Intuitively, you can imagine that youâ€™re â€œfeedingâ€ the collection of probabilities through the state and getting the â€œtransformedâ€Â probabilities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;meals&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, since the probabilities at time &lt;span class="math"&gt;\(t + 1\)&lt;/span&gt; only depend on the probabilities at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;, we can do this recursively to find &lt;span class="math"&gt;\(\pi^{(n)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;meals&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;meals&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.72&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt; &lt;span class="c1"&gt;# this is pi^(2), or probabilities at time 2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On a day where I had noodles for breakfast, then, I have a 40% chance of having noodles and a 60% chance of having rice for lunch, and then a 28% chance of having noodles and a 72% chance of having rice forÂ dinner.&lt;/p&gt;
&lt;h1 id="other-possibilities-for-the-state-vector"&gt;Other possibilities for the stateÂ vector&lt;/h1&gt;
&lt;p&gt;Actually, I oversimplified the above example a bit. The state vector doesnâ€™t have to be a binary thing. Since its values must sum to 1, it can also reflect an initial probability &lt;em&gt;distribution&lt;/em&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ \pi^{(0)} = \begin{bmatrix}0.45 &amp;amp; 0.55\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;As you can see, this changes the subsequentÂ probabilities:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;meals&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.29&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.71&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the &lt;span class="caps"&gt;HIV&lt;/span&gt;/&lt;span class="caps"&gt;AIDS&lt;/span&gt; scenario I referred to at the beginning of this post, the initial state vector is used to represent the population (since subsets of a population can be expressed as the probability that a random person will belong to thatÂ subset).&lt;/p&gt;
&lt;p&gt;Here are the givens of that problem. The states are &lt;em&gt;&lt;span class="caps"&gt;HIV&lt;/span&gt; asymptomatic, &lt;span class="caps"&gt;HIV&lt;/span&gt; symptomatic, &lt;span class="caps"&gt;AIDS&lt;/span&gt;,&lt;/em&gt; and &lt;em&gt;death&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered} P = \begin{bmatrix}
0.97 &amp;amp; 0.07 &amp;amp; 0.02 &amp;amp; 0.01 \\
0    &amp;amp; 0.93 &amp;amp; 0.05 &amp;amp; 0.02 \\
0    &amp;amp; 0    &amp;amp; 0.85 &amp;amp; 0.15 \\
0    &amp;amp; 0    &amp;amp; 0    &amp;amp; 1.00 \end{bmatrix} \\
\pi^{(0)} = \begin{bmatrix} 0.85 &amp;amp; 0.10 &amp;amp; 0.05 &amp;amp; 0 \end{bmatrix}
\end{gathered} $$&lt;/div&gt;
&lt;p&gt;This means that the initial population was 85% asymptomatic, 10% symptomatic, 5% &lt;span class="caps"&gt;AIDS&lt;/span&gt; patients, and 0%Â dead.&lt;/p&gt;
&lt;p&gt;If we used a vector with a single 1 and the rest 0s, then that would represent the outcomes for a homogenous population, or more likely, a singleÂ person.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hiv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.02&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.93&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.02&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.85&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;population&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.85&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;population&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;hiv&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.765&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1525&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0645&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.018&lt;/span&gt; &lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So the outcomes after 1 year (&lt;span class="math"&gt;\(t = 1\)&lt;/span&gt;) are 76.5% asymptomatic, 15.25% symptomatic, 6.45% &lt;span class="caps"&gt;AIDS&lt;/span&gt; patients, and 1.8%Â dead.&lt;/p&gt;
&lt;h1 id="python-izing-a-markov-chain"&gt;Python-izing a MarkovÂ chain&lt;/h1&gt;
&lt;p&gt;Maybe this is kind of overkill, but we can implement our system as aÂ class.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MarkovChain&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Validate the matrix&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The Markov matrix and state vector must be NumPy arrays.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Markov matrices must be square.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probabilities cannot be negative.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Each row in a Markov matrix must sum to 1.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Validate the init_state&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The initial state vector must be a NumPy array.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'The state vector must have a shape of 1 x {matrix.shape[1]}.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The values in the state vector must sum to 1.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;            
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;status&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;probs_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"%"&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'At time {self.time}, the transition probabilities are'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;restart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
          &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;init_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_state&lt;/span&gt;
          &lt;span class="c1"&gt;# call with no args to start over with same initial state&lt;/span&gt;
          &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can see what the population will be like after many years with just a fewÂ keystrokes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MarkovChain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hiv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;population&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;At&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'76.5%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'15.25%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'6.45%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'1.8%'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;At&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'26.67%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'31.53%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'13.58%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'28.21%'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;At&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'9.3%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'23.68%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'11.67%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'55.34%'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;43&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;At&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'3.24%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'14.4%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'7.71%'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'74.65%'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;75% dead after 30 yearsâ€”grim outlook, to say the least. (Though to be fair, the model takes into account that people may and do die of things other than &lt;span class="caps"&gt;HIV&lt;/span&gt;/&lt;span class="caps"&gt;AIDS&lt;/span&gt;. This is represented in the matrix by the nonzero probabilities of asymptomatic people transitioning right toÂ death.)&lt;/p&gt;
&lt;p&gt;â€¦aaaand on that note, Iâ€™ll end this post here for now. I will write about Markov chains again in the nearÂ future.&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=0Il-y_WLTo4"&gt;Concepts of Markov Chains&lt;/a&gt; (video), PaulÂ Harper&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.stat.auckland.ac.nz/~fewster/325/notes/ch8.pdf"&gt;Stats 325, Chapter 8: Markov Chains&lt;/a&gt;, RachelÂ Fewster&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@balamurali_m/markov-chain-simple-example-with-python-985d33b14d19"&gt;Markov Chain: Simple example with Python&lt;/a&gt;, BalamuraliÂ M&lt;/li&gt;
&lt;/ul&gt;</content><category term="Markov chain"></category><category term="linear algebra"></category></entry><entry><title>Monty, Monte, andâ€¦ Bayes: Statistics andÂ probability</title><link href="http://tabidots.github.io/2019/01/monty-hall-beginner" rel="alternate"></link><published>2019-01-12T10:54:19+07:00</published><updated>2019-01-12T10:54:19+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-12:/2019/01/monty-hall-beginner</id><summary type="html">&lt;p&gt;Getting stumped by some basic probability questions led me down a rabbit hole whereupon I stumbled across Bayesâ€™&amp;nbsp;theorem.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;I thought I had familiarized myself enough with the basic workings of linear algebra after Week 1 that I thought Rachel Thomasâ€™ (&lt;a href="www.fast.ai"&gt;fast.ai&lt;/a&gt;)â€™s &lt;a href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;amp;index=2&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;t=0s"&gt;Linear algebra for coders&lt;/a&gt; course would be a good way to deepen my knowledge by applying these newly learned concepts inÂ Python.&lt;/p&gt;
&lt;p&gt;But embarrassingly, even the first warmup problem was beyond me. It required the use of a Markov matrix, which is the application of linear algebra to probability. Then I realized I had no background in probability, so I tried Brilliantâ€™s &lt;a href="https://brilliant.org/courses/probability/introduction-to-probability/"&gt;Intro to Probability&lt;/a&gt;, whereupon I roundly botched a bunch of basic probabilityÂ questions.&lt;/p&gt;
&lt;p&gt;I found it really odd that I had just learned these mathematical techniques (basic linear algebra and multivariable calculus) to solve problems in a new way, yet while these probability problems &lt;em&gt;could&lt;/em&gt; be solved computationally, they are meant to be solved â€œintuitively.â€ Of course, that seems like a cruel joke, since their counterintuitive solutions make such problems seem more like ZenÂ koans.&lt;/p&gt;
&lt;h1 id="the-monty-hall-problem"&gt;The Monty HallÂ problem&lt;/h1&gt;
&lt;p&gt;Take, for example, the &lt;a href="https://en.wikipedia.org/wiki/Monty_Hall_problem"&gt;Monty Hall problem&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose youâ€™re on a game show, and youâ€™re given the choice of three doors: Behind one door is a car; behind the others,Â goats.&lt;/p&gt;
&lt;p&gt;You pick a door, say No. 1, and the host, who knows whatâ€™s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, â€œDo you want to pick door No.Â 2?â€&lt;/p&gt;
&lt;p&gt;Is it to your advantage to switch yourÂ choice?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Spoiler alert&lt;/strong&gt;: The answer is that you &lt;em&gt;do&lt;/em&gt; switch, because you would then have a &lt;span class="math"&gt;\(\frac{2}{3}\)&lt;/span&gt; probability of choosing theÂ car.&lt;/p&gt;
&lt;p&gt;Wait, what? How does a single door go from having a &lt;span class="math"&gt;\(\frac{1}{3}\)&lt;/span&gt; chance that the car is behind it to a &lt;span class="math"&gt;\(\frac{2}{3}\)&lt;/span&gt; chance? It seemed to me that you would now have a 50/50 chance, since there were two doorsÂ left.&lt;/p&gt;
&lt;p&gt;People tend to explain the rationale for the solution by increasing the number of doors to 100, and the number of doors revealed to 98. But this didnâ€™t change anything for me; you still ended up with 2 doors, and thus a 50/50Â chance.&lt;/p&gt;
&lt;p&gt;No number of doors, visual explanation, or animation seemed to help me understand it. Finally, I managed to find a &lt;a href="https://www.reddit.com/r/explainlikeimfive/comments/58cdw3/eli5_the_monty_hall_problem/d8zanoq"&gt;comment&lt;/a&gt; in an old &lt;a href="https://www.reddit.com/r/explainlikeimfive"&gt;&lt;span class="caps"&gt;ELI5&lt;/span&gt;&lt;/a&gt; thread that made it click forÂ me:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As simply as possible: Donâ€™t think of it as three doors. Think of it as your door, and Montyâ€™s doors. The odds that you picked the right door are 1 in 3, and the odds that you didnâ€™t are 2 in 3,Â right?&lt;/p&gt;
&lt;p&gt;When Monty gets rid of one bad choice, he doesnâ€™t change the odds that your door is right - itâ€™s still 1 in 3. That means heâ€™s also not changing the odds that you arenâ€™t right - itâ€™s still 2 inÂ 3.&lt;/p&gt;
&lt;p&gt;Therefore youâ€™re not picking one door - youâ€™re picking two doors at the same time and getting the best possible outcome. If either of Montyâ€™s doors was right, you win; If both of Montyâ€™s doors were bad, youÂ lose.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Explained this way, it then makes sense that there would only be a 50/50 chance if you had two doors &lt;em&gt;to begin with&lt;/em&gt;. Or, if you had fallen into â€œthe probability trap of treating non-random information as if it were random,â€ as it has been described &lt;a href="https://sites.google.com/site/psmartinsite/Home/bridge-articles/the-monty-hall-trap"&gt;elsewhere&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="statistical-approach"&gt;StatisticalÂ approach&lt;/h1&gt;
&lt;p&gt;Letâ€™s first try to simulate the game in Python. Iâ€™ll skip over explaining whatâ€™s going on in the code here, as itâ€™s prettyÂ elementary.&lt;/p&gt;
&lt;p&gt;I just added perpetual loops to avoid the problem of bad input (if you were going to get other people to play, forÂ example).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;monty_hall&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;doors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;"b"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;"c"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;first_choice&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Choose a door (a/b/c): "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;monty_opens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Monty opens door {monty_opens}; do you switch? (y/n) "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"y"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# switch doors&lt;/span&gt;
            &lt;span class="n"&gt;final_choice&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doors&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;monty_opens&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"n"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;final_choice&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Your final choice is door {final_choice}."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;final_choice&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"You win! The car is behind door {car}."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"You lose. The car was behind door {car}."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, letâ€™s automate the process so that the computer decides randomly whether or not to switch, and returns us the information we need to analyze the outcomes (&lt;em&gt;stay/switch&lt;/em&gt; and &lt;em&gt;win/lose&lt;/em&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;monty_hall&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;doors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;"b"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;"c"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;first_choice&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;monty_opens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt;

    &lt;span class="n"&gt;switch_doors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;final_choice&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doors&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;door&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;monty_opens&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;switch_doors&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;first_choice&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;final_choice&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"switch_doors"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;switch_doors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"result"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monty_hall&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'switch_doors'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'result'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Just as we can get the computer to flip a million coins for us, we can also get it to confront the Monty Hall problem a million times (if we want). This is called a &lt;strong&gt;Monte Carlo simulation&lt;/strong&gt;. I was already familiar with this concept, but to offer a quick summary here, it basically leverages the &lt;strong&gt;law of large numbers&lt;/strong&gt;: If you perform the same experiment enough times, the real result eventually converges on the theoreticalÂ result.&lt;/p&gt;
&lt;p&gt;So letâ€™s code it with a simple loop, tallying the results in two lists, and taking advantage of the truthiness of booleans in Python to quickly analyze theÂ results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;switch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;monty_hall&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"switch_doors"&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;switch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"result"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;stay&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"result"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;switch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# avoid division by zero errors if you do 1 or 2 trials&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Switching won {sum(switch)} out of {len(switch)} times; probability of {round(sum(switch)/len(switch) * 100, 2)}%."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;stay&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Staying won {sum(stay)} out of {len(stay)} times; probability of {round(sum(stay)/len(stay) * 100, 2)}%."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To reiterate the explanation above, &lt;em&gt;if you perform the same experiment enough times, the real result eventually converges on the theoretical result.&lt;/em&gt; Watch what happens with just 25Â simulations:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;58.33&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;30.77&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is already clear after just 25 simulations that the probability of switching is significantly greater than that of staying. Letâ€™s keepÂ going.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;62.07&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.33&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;51&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;62.75&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;49&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;32.65&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;330&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;496&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;66.53&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;169&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;504&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.53&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;3376&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5053&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;66.81&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;1675&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;4947&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.86&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;33289&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;49759&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;66.9&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;16789&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;50241&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.42&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;333204&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;500463&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;66.58&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;166298&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;499537&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.29&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;27&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;monte_monty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Switching&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;3331217&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;4998552&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;66.64&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;span class="n"&gt;Staying&lt;/span&gt; &lt;span class="n"&gt;won&lt;/span&gt; &lt;span class="mi"&gt;1666441&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5001448&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mf"&gt;33.32&lt;/span&gt;&lt;span class="o"&gt;%.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After 1000 runs, we pretty much obtain the theoretical probability of &lt;span class="math"&gt;\(\frac{2}{3}\)&lt;/span&gt; chance of a winning switch and &lt;span class="math"&gt;\(\frac{1}{3}\)&lt;/span&gt; chance of a winning stay, and after 10 million runs, we come within a hairâ€™s breadth ofÂ it.&lt;/p&gt;
&lt;p&gt;In this way, the Monte Carlo method is not that different from stochastic gradientÂ descent.&lt;/p&gt;
&lt;h1 id="probabilistic-approach"&gt;ProbabilisticÂ approach&lt;/h1&gt;
&lt;p&gt;There is another, less brute-force, more theoretical way of working out this answer using &lt;strong&gt;Bayesian inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Letâ€™s rephrase the problem. Instead of focusing on the probability of a certain action (&lt;em&gt;stay/switch&lt;/em&gt;) winning, weâ€™ll examine the probability of a car being behind door &lt;span class="math"&gt;\(x\)&lt;/span&gt; after Monty opens door &lt;span class="math"&gt;\(y\)&lt;/span&gt;, or in the language of &lt;strong&gt;conditional probability&lt;/strong&gt;, &lt;em&gt;given&lt;/em&gt; that Monty opens door &lt;span class="math"&gt;\(y\)&lt;/span&gt;. ThatÂ is,&lt;/p&gt;
&lt;div class="math"&gt;$$ P(\textrm{car}_x | \textrm{Monty}_y) $$&lt;/div&gt;
&lt;p&gt;Letâ€™s start with the &lt;em&gt;unconditional&lt;/em&gt; probability (i.e., before the game starts) of the car being behind each of the doors, &lt;span class="math"&gt;\(P(\textrm{car}_x)\)&lt;/span&gt;. There are three doors and one car,Â so&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered} P(\textrm{car}_x) = \tfrac{1}{3} \\
\therefore P(\textrm{car}_a) = P(\textrm{car}_b) = P(\textrm{car}_c) = \tfrac{1}{3} \end{gathered} $$&lt;/div&gt;
&lt;p&gt;Next, letâ€™s determine the &lt;em&gt;unconditional&lt;/em&gt; probabilities (i.e., before the game starts) of Monty choosing each of the doors. Since he cannot choose the door you will have chosen, that leaves him with twoÂ doors:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered} P(\textrm{Monty}_x) = \tfrac{1}{2} \\
\therefore P(\textrm{Monty}_a) = P(\textrm{Monty}_b) = P(\textrm{Monty}_c) = \tfrac{1}{2} \end{gathered} $$&lt;/div&gt;
&lt;p&gt;Now that we have our building blocks, letâ€™s plug in what we can into theÂ theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$ P(H|E) = \frac{P(E|H) P(H)}{P(E)} $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(H\)&lt;/span&gt; is the hypothesis weâ€™re interested in and &lt;span class="math"&gt;\(E\)&lt;/span&gt; is the plot-twist, game-changingÂ event.&lt;/p&gt;
&lt;p&gt;Letâ€™s assume our first choice is door A. If we want to know the probability of the car being behind door A after Monty opens door C (i.e., we win by staying), we can fill in what weÂ know:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} P(\textrm{car}_a|\textrm{Monty}_c) &amp;amp;= \frac{P(\textrm{Monty}_c|\textrm{car}_a) P(\textrm{car}_a)}{P(\textrm{Monty}_c)} \\
&amp;amp;= \frac{P(\textrm{Monty}_c|\textrm{car}_a) \cdot \tfrac{1}{3} }{\tfrac{1}{2}} \end{aligned} $$&lt;/div&gt;
&lt;p&gt;which leaves one last probability to calculate, &lt;span class="math"&gt;\(P(\textrm{Monty}_c|\textrm{car}_a)\)&lt;/span&gt;. That is the probability that Monty will open door C if the car is behind doorÂ A.&lt;/p&gt;
&lt;p&gt;Monty canâ€™t choose your door (A) or the door with the car (also A), leaving him two choices. He is therefore equally likely to choose door B or C, which means that &lt;span class="math"&gt;\(P(\textrm{Monty}_c|\textrm{car}_a) = \tfrac{1}{2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ P(\textrm{car}_a|\textrm{Monty}_c) = \frac{P(\textrm{Monty}_c|\textrm{car}_a) P(\textrm{car}_a)}{P(\textrm{Monty}_c)} = \frac{\tfrac{1}{2} \cdot \tfrac{1}{3} }{\tfrac{1}{2}} = \frac{1}{3} $$&lt;/div&gt;
&lt;p&gt;Though the theorem is quite convoluted, this answer agrees with our intuition and the Monte CarloÂ results.&lt;/p&gt;
&lt;p&gt;What about the probability that we win by switching (i.e., the probability that the car is behind door B after Monty opens doorÂ C)?&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} P(\textrm{car}_b|\textrm{Monty}_c) &amp;amp;= \frac{P(\textrm{Monty}_c|\textrm{car}_b) P(\textrm{car}_b)}{P(\textrm{Monty}_c)} \\
&amp;amp;= \frac{P(\textrm{Monty}_c|\textrm{car}_b) \cdot \tfrac{1}{3} }{\tfrac{1}{2}} \end{aligned} $$&lt;/div&gt;
&lt;p&gt;In the case of &lt;span class="math"&gt;\(P(\textrm{Monty}_c|\textrm{car}_b)\)&lt;/span&gt;, we have chosen door A and the car is behind door B. Monty knows this, and he also canâ€™t choose the door youâ€™ve chosen, doorÂ A.&lt;/p&gt;
&lt;p&gt;His &lt;em&gt;only&lt;/em&gt; choice, then, is to choose door C, and therefore &lt;span class="math"&gt;\(P(\textrm{Monty}_c|\textrm{car}_b) = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ P(\textrm{car}_b|\textrm{Monty}_c) = \frac{P(\textrm{Monty}_c|\textrm{car}_b) P(\textrm{car}_b)}{P(\textrm{Monty}_c)} = \frac{1 \cdot \tfrac{1}{3} }{\tfrac{1}{2}} = \frac{2}{3} $$&lt;/div&gt;
&lt;p&gt;Again, the theorem agrees with the Monte Carlo results, although whether or not it also agrees with your intuition is a differentÂ question!&lt;/p&gt;
&lt;h1 id="where-do-we-go-from-here"&gt;Where do we go fromÂ here?&lt;/h1&gt;
&lt;p&gt;I know that the Monte Carlo method can be used to make predictions about the movements of the stock market. However, Iâ€™m totally new to probability theory, so Iâ€™m not yet sure where to go after Bayesâ€™ theorem. Iâ€™ll update this section once I find out and post about more generalized and sophisticated ways to apply Monte Carlo and Bayesâ€™ theorem. StayÂ tuned!&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://datant.wordpress.com/2017/01/16/the-monty-hall-problem-and-3-ways-to-solve-it/"&gt;The Monty Hall problem and 3 ways to solve it&lt;/a&gt;, AnthonyÂ Oâ€™Farrell&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego"&gt;Bayesâ€™ Theorem with Lego&lt;/a&gt;, WillÂ Kurt&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sc5.io/posts/how-to-solve-the-monty-hall-problem-using-bayesian-inference/#gref"&gt;How to solve the Monty Hall problem using Bayesian inference&lt;/a&gt;, MaxÂ Pagels&lt;/li&gt;
&lt;/ul&gt;</content><category term="Monte Carlo"></category><category term="Bayesian inference"></category></entry><entry><title>The Loopless Loop (or How I made my code run 7,000 timesÂ faster)</title><link href="http://tabidots.github.io/2019/01/loopless-loop" rel="alternate"></link><published>2019-01-10T19:00:46+07:00</published><updated>2019-01-10T19:00:46+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-10:/2019/01/loopless-loop</id><summary type="html">&lt;p&gt;In which I (1) discover that the purpose of linear algebra is not to just manipulate spreadsheets and move vectors around but to make your code faster and cleanerâ€”in other words, to give it a Zen uppercut; and (2) learn LaTeX and start a blog just to see syntax-highlighted code and properly typeset math on the same&amp;nbsp;page.&lt;/p&gt;</summary><content type="html">
&lt;h1 id="part-1-ugly-math-ugly-code"&gt;Part 1: Ugly math, uglyÂ code&lt;/h1&gt;
&lt;p&gt;As I embarked on my journey to learn the math side of machine learning, all of the blog posts seemed to point to linear algebra as the starting point. The problem was, nothing I read made it immediately clear &lt;em&gt;how&lt;/em&gt; linear algebra played a role in machineÂ learning.&lt;/p&gt;
&lt;p&gt;Worse yet, the whole discussion of vectors, matrices, linear combinations, and linear transformations seemed completely disconnected from my laymanâ€™s understanding of machineÂ learning.&lt;/p&gt;
&lt;p&gt;The Khan Academy videos on linear algebra are quite tedious and I didnâ€™t feel I was getting anywhere. &lt;a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;3blue1brownâ€™s Linear Algebra series&lt;/a&gt; is much more engaging and lucid, but I was still getting bogged down in the theory without seeing theÂ application.&lt;/p&gt;
&lt;p&gt;Needless to say, it was pretty slowÂ going.&lt;/p&gt;
&lt;h2 id="my-linear-algebra-a-ha-moment"&gt;My linear algebra &lt;em&gt;a-ha&lt;/em&gt;Â moment&lt;/h2&gt;
&lt;p&gt;It wasnâ€™t until I switched gears and decided, on a whim, to tackle &lt;em&gt;linear regression&lt;/em&gt; that linear algebra really started to click for me. On a practical level, code that uses linear-algebraic methods simplifies work for the computer by orders of magnitude, making it possible to process massive datasetsâ€”and process them rapidly. This is obviously a critical requirement in the age of BigÂ Data.&lt;/p&gt;
&lt;p&gt;And on a conceptual level, it gave me my first mathematical &lt;em&gt;a-ha&lt;/em&gt;Â moment:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;By manipulating matrices and vectors, you can achieve the same outcome as a loop without explicitly loopingâ€”a loopless loop&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;That&lt;/em&gt; is why linear algebra is the cornerstone of machineÂ learning.&lt;/p&gt;
&lt;p&gt;The only thing is, no single resource I found on the internet seemed to really clarify the mathematical and programmatic aspects &lt;em&gt;at the same time&lt;/em&gt; without getting too abstract on the math side ofÂ things.&lt;/p&gt;
&lt;p&gt;As a self-taught and formerly math-phobic coder, I needed a guide that progressed from from inelegant code (which I could understand) and inelegant math to (mind-blowingly) elegant math, which would then lay the groundwork for writing extremely elegantâ€”and performantâ€”code (which is incomprehensible without understanding theÂ math).&lt;/p&gt;
&lt;p&gt;This is that guide, created from my notes from Week 1 of my machine learning journey. Iâ€™ve split it up into multiple posts, since itâ€™s quiteÂ long.&lt;/p&gt;
&lt;h2 id="how-to-make-a-computer-explode"&gt;How to make a computerÂ explode&lt;/h2&gt;
&lt;p&gt;I always thought that for-loops were simply a fact ofÂ life.&lt;/p&gt;
&lt;p&gt;While I understood the basic principle behind stochastic gradient descent, I had never implemented it myself before. If I had tried my hand at it before learning the math involved, I probably would have come up with thisÂ monster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# DON'T TRY THIS&lt;/span&gt;

&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;lists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;actual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# 0 to start&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sum_of_sq_errs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
                &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;sum_of_sq_errs&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;partial_deriv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt;
            &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;partial_deriv&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="c1"&gt;# I REALLY HOPE YOU DIDN'T TRY THIS&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay, maybe not. (It would be pretty hard to write this without understanding the mathÂ involved.)&lt;/p&gt;
&lt;p&gt;Count those loopsâ€”&lt;em&gt;three&lt;/em&gt;, to be exact! Terrifying. Now, being handy with Python, I could probably have calculated &lt;code class="highlight"&gt;partial_deriv&lt;/code&gt; in one line with an even more terrifying list comprehension, just to showÂ off:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;partial_deriv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;â€¦which shaves off four lines, at the expense of all readability. But bigger problemsÂ remain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The time complexity of this program is off the charts.&lt;/strong&gt; In &lt;a href="https://en.wikipedia.org/wiki/Time_complexity"&gt;Big-O time complexity&lt;/a&gt; terms, it is &lt;em&gt;at least&lt;/em&gt; &lt;span class="math"&gt;\(O(n^3)\)&lt;/span&gt;, if not more, which is cubic time, or (literally) &lt;em&gt;exponentially &lt;a href="http://bigocheatsheet.com/"&gt;horrible&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It doesnâ€™t even work.&lt;/strong&gt; Even with a dataset of unremarkable size, youâ€™re bound to get a &lt;code class="highlight"&gt;&lt;span class="n"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;overflow&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;/code&gt; that canâ€™t be avoided even if you set the &lt;code class="highlight"&gt;learning_rate&lt;/code&gt; to an impractically small value like &lt;code class="highlight"&gt;0.00001&lt;/code&gt;. Trust me, Iâ€™veÂ tried.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there any way beyond thisÂ impasse?&lt;/p&gt;
&lt;h2 id="meeting-a-zen-master-on-the-road"&gt;Meeting a Zen master on theÂ road&lt;/h2&gt;
&lt;p&gt;In Zen Buddhism, there is a famous book called &lt;em&gt;The Gateless Gate&lt;/em&gt;, which is a collection of &lt;em&gt;koans&lt;/em&gt;. A Zen &lt;em&gt;koan&lt;/em&gt; is a riddle that cannot be approached with the rational mind. ForÂ example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Goso said: â€œWhen you meet a Zen master on the road, you cannot talk to him, but neither can you face him with silence. What are you going toÂ do?â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To solve it, you have to transcend the duality of &lt;em&gt;this&lt;/em&gt; and &lt;em&gt;not-this&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Give him an uppercut&lt;br/&gt;
And you will be called one who understands Zen.&lt;/em&gt;&lt;br/&gt;
â€”The Gateless Gate, KoanÂ #36&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you might imagine, linear algebraâ€”&lt;strong&gt;the loopless loop&lt;/strong&gt;â€”is the Zen uppercut of coding. What, then is the target of ourÂ uppercut?&lt;/p&gt;
&lt;h2 id="linear-regression-a-basic-overview"&gt;Linear regression: A basicÂ overview&lt;/h2&gt;
&lt;p&gt;Basically, linear regression is used to make predictions about a &lt;em&gt;thing&lt;/em&gt; based on its characteristics, assumingÂ that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that there is some correlation among those characteristicsÂ and&lt;/li&gt;
&lt;li&gt;that you have plenty of data about other &lt;em&gt;things&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The intuition here can be explained with middle school algebra. Imagine you know the square footage and price of 200 houses and you want to estimate the price of a house with a given squareÂ footage.&lt;/p&gt;
&lt;p&gt;Obviously, this is an oversimplified correlation for the sake of example. (And for some reason, everyone seems to explain this concept using houses, so why reinvent theÂ wheel?)&lt;/p&gt;
&lt;p&gt;If you were to make a scatter plot of that data, with the area along the x-axis and the price along the y-axis, the pattern might roughly look like it follows a lineâ€”not perfectly linear, but linear &lt;em&gt;enough&lt;/em&gt; to predict the price of a house with &lt;span class="math"&gt;\(x\)&lt;/span&gt; square footage. You can use linear regression to work backward from the data to determine this line, &lt;strong&gt;the line of best fit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In middle school algebra, lines are written in theÂ form&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{orange}{b} $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt; is the input, &lt;span class="math"&gt;\(\textcolor{magenta}{m}\)&lt;/span&gt; is the slope of the line, &lt;span class="math"&gt;\(\textcolor{orange}{b}\)&lt;/span&gt; moves the line up and down on the graph, and &lt;span class="math"&gt;\(y\)&lt;/span&gt; is the height of the line at point &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our house problem can also be framed as a line, where &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt; is the square footage, which influences the price by some value &lt;span class="math"&gt;\(\textcolor{magenta}{m}\)&lt;/span&gt;, to which we add some kind of base price to bring us to the final price, &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Well, that was easy enough,Â right?&lt;/p&gt;
&lt;h2 id="multiple-linear-regression-because-more-is-better-or-something"&gt;Multiple linear regression: Because more is better (orÂ something)&lt;/h2&gt;
&lt;p&gt;In the real world, of course, area is not the only factor that decides the price of a house. There are many others. Can we still adapt our middle school equation to this problem if each house has 3 featuresâ€”say, area, nearby property values, and age of theÂ building?&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{magenta}{n}\textcolor{teal}{z} + \textcolor{magenta}{o}\textcolor{teal}{a} + \textcolor{orange}{b} $$&lt;/div&gt;
&lt;p&gt;We can, but itâ€™s messy. (Itâ€™s also no longer a line, but letâ€™s ignore that for now.) First, letâ€™s rewrite that â€œbase priceâ€ as &lt;span class="math"&gt;\(b \cdot 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{magenta}{n}\textcolor{teal}{z} + \textcolor{magenta}{o}\textcolor{teal}{a} + \textcolor{magenta}{b}\cdot\textcolor{teal}{1} $$&lt;/div&gt;
&lt;p&gt;This gives us a nice symmetry: Notice that all of the teal variables are features, which are multiplied by their degree of influence (called a &lt;em&gt;coefficient&lt;/em&gt; in statistics, or a &lt;em&gt;weight&lt;/em&gt; in machine learning). When you add all these together, you get the price of theÂ house.&lt;/p&gt;
&lt;p&gt;This is called &lt;strong&gt;multiple linear regression&lt;/strong&gt;. Most people wouldnâ€™t skip directly to multiple &lt;span class="caps"&gt;LR&lt;/span&gt; after introducing single &lt;span class="caps"&gt;LR&lt;/span&gt;, but single &lt;span class="caps"&gt;LR&lt;/span&gt; is pretty easy to digest if you can understand high school calculus (derivatives), so it didnâ€™t level up my mathÂ knowledge.&lt;/p&gt;
&lt;p&gt;Now, letâ€™s code our equation, putting all the feature values into a &lt;em&gt;list&lt;/em&gt; &lt;code class="highlight"&gt;features&lt;/code&gt; and the weights into another &lt;em&gt;list&lt;/em&gt; &lt;code class="highlight"&gt;weights&lt;/code&gt;. In Python, in increasing order of elegance, we can write theÂ following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Using completely random numbers just to show the code&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 3 features; first value is the â€œdummy featureâ€&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Brute-force addition&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we use &lt;span class="math"&gt;\(x\)&lt;/span&gt;s to denote our features, &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt;s to denote our weights, and subscript numbers to denote the position of each item in a list (series), then we can rewrite our equation in a slightly more organizedÂ way:&lt;/p&gt;
&lt;div class="math"&gt;$$ h_Î¸(x) = \textcolor{magenta}{Î¸_0}\textcolor{lightgray}{x_0} + \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1} + \cdots + \textcolor{magenta}{Î¸_n}\textcolor{teal}{x_n}  $$&lt;/div&gt;
&lt;p&gt;which just happens to be the &lt;strong&gt;generalized form of linear regression&lt;/strong&gt;â€”&lt;em&gt;general&lt;/em&gt; in the sense that it can accommodate any number of features, whether thatâ€™s 1 orÂ 1,000.&lt;/p&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is the collection of all feature values &lt;span class="math"&gt;\(\textcolor{teal}{x_1}\)&lt;/span&gt; through &lt;span class="math"&gt;\(\textcolor{teal}{x_n}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of features. (And &lt;span class="math"&gt;\(x_{1000}\)&lt;/span&gt; actually isnâ€™t too crazy in terms of real-world datasets!) It also includes the dummy feature &lt;span class="math"&gt;\(\textcolor{lightgray}{x_0} = 1\)&lt;/span&gt;. Likewise, &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; is the collection of all weights, including &lt;span class="math"&gt;\(\textcolor{magenta}{Î¸_0}\)&lt;/span&gt;, the â€œbase priceâ€ in our example. In machine learning, this is called the &lt;em&gt;bias&lt;/em&gt;Â value.&lt;/p&gt;
&lt;p&gt;Finally, the function notation &lt;span class="math"&gt;\(h_Î¸(x)\)&lt;/span&gt; indicates that this is the &lt;strong&gt;hypothesis&lt;/strong&gt; for item (house) &lt;span class="math"&gt;\(x\)&lt;/span&gt; given the collection of weights &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="how-to-python-in-math-lesson-1"&gt;How to Python in math (LessonÂ 1)&lt;/h2&gt;
&lt;p&gt;If you know anything about programming, you know that the last line of code above is no way to write a program. Accommodating 100 features would be a chore, and accommodating a variable number of features would be impossible. Naturally, we would use the magic ofÂ iteration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Super-basic iteration over the lists&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;No one actually writes the linear regression formula like this, but if you wanted to, you could express the above code in math using a &lt;em&gt;summation&lt;/em&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ h_Î¸(x) = \sum_{j=0}^n \textcolor{magenta}{Î¸_j}\textcolor{teal}{x_j} $$&lt;/div&gt;
&lt;p&gt;That Greek letter &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; (sigma) means &lt;em&gt;summation&lt;/em&gt;. Basically, run a for-loop that adds the result of the following expression for each sample (house) starting at &lt;span class="math"&gt;\(j=0\)&lt;/span&gt; until &lt;span class="math"&gt;\(j=n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you know Python better than you know math (as I did), then you might try further refactoring theÂ code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Functional programming version&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;

&lt;span class="c1"&gt;# More Pythonic version of the above&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I didnâ€™t realize this when I wrote the first draft of this post, but even expressing the simple Python function &lt;code class="highlight"&gt;zip(x, y)&lt;/code&gt; in math requires linear algebra. Iâ€™ll get back to the non-fancy version of linear regression in just a minute, but for sake of thoroughness, if &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; are both vectors,Â then&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\vec x = \begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{bmatrix} &amp;amp; \qquad{}
\vec Î¸ = \begin{bmatrix} Î¸_0 \\ Î¸_1 \\ \vdots \\ Î¸_n \end{bmatrix} &amp;amp; \quad{}
z(\vec x, \vec Î¸) = \vec Î¸\vec x \end{aligned} $$&lt;/div&gt;
&lt;p&gt;Similarly, we can rewrite our equation as a function in Python, too. Letâ€™s leave vectors aside for now, but keep the &lt;code class="highlight"&gt;zip&lt;/code&gt; and encapsulate it into a function, because itâ€™s clean and easy toÂ understand.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="einstein-notation-making-math-less-confusing-by-making-it-more-confusing"&gt;Einstein notation: Making math less confusing by making it moreÂ confusing&lt;/h2&gt;
&lt;p&gt;This is all great if thereâ€™s only one &lt;span class="math"&gt;\(x\)&lt;/span&gt; (house). But we will need tons of houses to make a decent prediction. Our list &lt;code class="highlight"&gt;houses&lt;/code&gt; needs to be changed into a &lt;em&gt;list of lists&lt;/em&gt;. For the sake of example, if we had three houses and three features, &lt;code class="highlight"&gt;houses&lt;/code&gt; would look like this (remember the dummyÂ feature):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# random values&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This also allows us to refer to specific features of specific houses using two indexes, &lt;code class="highlight"&gt;houses[i][j]&lt;/code&gt;. How do we do this in math, though? Enter &lt;strong&gt;Einstein notation&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ h_Î¸(x^i) = Î¸_0x_0^i + Î¸_1x_1^i + \cdots + Î¸_nx_n^i  $$&lt;/div&gt;
&lt;p&gt;The superscript numbers here &lt;em&gt;arenâ€™t&lt;/em&gt; exponents. You would think Einstein, of all people, could come up with something less confusing, but that is the convention, so itâ€™s important to become familiar withÂ it.&lt;/p&gt;
&lt;p&gt;Just remember that in linear regression, there are conventional choices for the variable names. &lt;span class="math"&gt;\(i\)&lt;/span&gt; denotes the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th house, and &lt;span class="math"&gt;\(j\)&lt;/span&gt; the &lt;span class="math"&gt;\(j\)&lt;/span&gt;thÂ feature.&lt;/p&gt;
&lt;div class="math"&gt;$$ x^{\textcolor{orange}{i \textrm{th sample}}}_{\textcolor{blue}{j \textrm{th feature}}} $$&lt;/div&gt;
&lt;p&gt;So if we were to start describing each hypothesis for our datasetÂ individually,&lt;/p&gt;
&lt;div class="math"&gt;$$ h_Î¸(x^i) = \left\{\begin{array}{ll}
h_Î¸(x^0) = Î¸_0x_0^0 + Î¸_1x_1^0 + \cdots + Î¸_nx_n^0 \\[0.5em]
h_Î¸(x^1) = Î¸_0x_0^1 + Î¸_1x_1^1 + \cdots + Î¸_nx_n^1 \\[0.5em]
h_Î¸(x^2) = Î¸_0x_0^2 + Î¸_1x_1^2 + \cdots + Î¸_nx_n^2 \\[0.5em]
h_Î¸(x^3) = Î¸_0x_0^2 + Î¸_1x_1^2 + \cdots + Î¸_nx_n^2
\end{array}\right. $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;hyps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="cost-function-how-accurate-are-our-predictions"&gt;Cost function: How accurate are ourÂ predictions?&lt;/h2&gt;
&lt;p&gt;Seeing as the goal of linear regression is to come up with a line that best fits the data, we need some way to evaluate a lineâ€™s &lt;strong&gt;goodness of fit&lt;/strong&gt; to the data. One measure of that is the &lt;strong&gt;mean squared error&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="error"&gt;Error&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Error&lt;/em&gt; is how far the prediction for one sample (house) is from its actualÂ value.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{error}_i &amp;amp;= \textrm{prediction}_i - \textrm{actual}_i\\
&amp;amp;= \hat{Y}_i - Y_i
\end{aligned} $$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The subscript &lt;span class="math"&gt;\(i\)&lt;/span&gt; here is &lt;strong&gt;not&lt;/strong&gt; Einstein notation, because these are just lists of values, not a â€œspreadsheetâ€ of rows and columns. The Einstein notation in this discussion of linear regression only applies to &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat Y\)&lt;/span&gt; is read â€œY-hat,â€ which is just a statistical convention. It can be substituted with our function &lt;span class="math"&gt;\(h_Î¸(x^i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \textrm{error}_i = h_Î¸(x^i) - Y_i $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;actual_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 3 houses; random values&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="squared-error"&gt;SquaredÂ error&lt;/h3&gt;
&lt;p&gt;We &lt;em&gt;square&lt;/em&gt; it so that (1) all values are positive, preventing underestimates and overestimates from canceling each other out; and (2) larger errors are considered proportionally â€œmore erroneousâ€ than smallerÂ errors.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{SE}_i &amp;amp;= \textrm{error}^2\\
&amp;amp;= (\hat{Y}_i - Y_i)^2\\
&amp;amp;= (h_Î¸(x^i) - Y_i)^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="mean-squared-error"&gt;Mean squaredÂ error&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;mean&lt;/em&gt; value of the squared error for all samples can give us an idea about our lineâ€™s goodness ofÂ fit.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{MSE} &amp;amp;= \frac{\textrm{SE}_\textcolor{red}{1} \textcolor{blue}{+ \cdots + } \textrm{ SE}_\textcolor{red}{\textrm{number of samples}}}{\textrm{number of samples}}\\
&amp;amp;= \frac{\textcolor{blue}{\sum}_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} \textrm{SE}}{m}\\
&amp;amp;= \frac{\sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (\hat{Y}_{\textcolor{red}i} - Y_{\textcolor{red}i})^2}{m}\\
&amp;amp;= \frac{1}{m} \sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (\hat{Y}_{\textcolor{red}i} - Y_{\textcolor{red}i})^2\\
&amp;amp;= \frac{1}{m} \sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (h_Î¸(x^{\textcolor{red}i}) - Y_{\textcolor{red}i})^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;

&lt;span class="c1"&gt;# More Pythonic and more similar to the actual math notation&lt;/span&gt;
&lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;span class="caps"&gt;MSE&lt;/span&gt; gives us an indication of goodness of fit, but itâ€™s difficult to tie that value directly to the data. You can use the &lt;span class="caps"&gt;RMSE&lt;/span&gt; (root mean squared error), which is just the square root of the &lt;span class="caps"&gt;MSE&lt;/span&gt;, to reframe the average error in terms of the data. In this case, the &lt;span class="caps"&gt;RMSE&lt;/span&gt; would tell us how much (in dollars) that our prediction line was offÂ by.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Letâ€™s use the mean squared error to rewrite this equation as a function of the collection of weights. Every time we change the weights, we will obtain a different line with a different goodness of fit (&lt;span class="caps"&gt;MSE&lt;/span&gt;), and this relationship can be illustrated by a function called the &lt;em&gt;cost function&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Well, almost. This function is conventionally named &lt;span class="math"&gt;\(J\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ J(Î¸) = \frac{1}{\textcolor{magenta}{2}m} \sum_{i=1}^m (h_Î¸(x^i) - Y_i)^2 $$&lt;/div&gt;
&lt;p&gt;Where did that &lt;span class="math"&gt;\(\textcolor{magenta}{2}\)&lt;/span&gt; come from? Again, this is just a matter of convention. The &lt;span class="math"&gt;\(\textcolor{magenta}{2}\)&lt;/span&gt; will cancel out in the the nextÂ step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Keep in mind that &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; are lists and &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a list of lists. What this means is that in a situation with two features (plus the dummyÂ feature),&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} h_{Î¸_\textcolor{teal}{0}, Î¸_\textcolor{teal}{1}, Î¸_\textcolor{teal}{2}}(x) &amp;amp;= h_{Î¸_\textcolor{teal}{0}}(x_\textcolor{teal}{0}) + h_{Î¸_\textcolor{teal}{1}}(x_\textcolor{teal}{1}) + h_{Î¸_2}(x_\textcolor{teal}{2}) \\
h_{Î¸_\textcolor{teal}{0}, Î¸_\textcolor{teal}{1}, Î¸_\textcolor{teal}{2}}(x^\textcolor{red}{i}) &amp;amp;= h_{Î¸_\textcolor{teal}{0}}(x_\textcolor{teal}{0}^\textcolor{red}{i}) + h_{Î¸_\textcolor{teal}{1}}(x_\textcolor{teal}{1}^\textcolor{red}{i}) + h_{Î¸_\textcolor{teal}{2}}(x_\textcolor{teal}{2}^\textcolor{red}{i}) \\
J(Î¸_\textcolor{teal}{0}, Î¸_\textcolor{teal}{1}, Î¸_\textcolor{teal}{2}) &amp;amp;= \frac{1}{2m} \sum_{\textcolor{red}{i=1}}^\textcolor{red}{m} \Big[h_{Î¸_\textcolor{teal}{0}}(x_\textcolor{teal}{0}^\textcolor{red}{i}) + h_{Î¸_\textcolor{teal}{1}}(x_\textcolor{teal}{1}^\textcolor{red}{i}) + h_{Î¸_\textcolor{teal}{2}}(x_\textcolor{teal}{2}^\textcolor{red}{i}) - Y_{\textcolor{red}i}\Big]^2
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Numbers in teal represent feature numbers; numbers in red represent sampleÂ numbers.&lt;/p&gt;
&lt;h2 id="multivariable-calculus-how-much-effect-does-each-weight-have"&gt;Multivariable calculus: How much effect does each weightÂ have?&lt;/h2&gt;
&lt;p&gt;Now, imagine each feature as knobs on a radio. Increasing or decreasing the weight of each feature is like turning up or down the knob for that feature. We want to â€œtuneâ€ our line to be as close to the data as possible by â€œdialingâ€ the features up and down. In order to do this, we need to determine the effect that a given combination of knob settings has on the finalÂ output.&lt;/p&gt;
&lt;p&gt;In math terms, this is akin to asking â€œHow much does &lt;span class="math"&gt;\(J\)&lt;/span&gt; change when &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; changes?â€ Sounds like derivatives from high schoolÂ calculus.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} f(x) &amp;amp;= 5x^2 \\
\frac{df}{dx} &amp;amp;= 5\cdot2x^{2-1} \\
&amp;amp;= 10x
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Our function &lt;span class="math"&gt;\(J\)&lt;/span&gt; is actually one function inside of another, so the chain rule applies. Bonus points if you remember that from high schoolâ€”IÂ didnâ€™t.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
J(Î¸) &amp;amp;= \textcolor{purple}{\frac{1}{2m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{\Big[} \textcolor{orange}{h_Î¸(x^i)} \textcolor{purple}{- Y_i)\Big]^2} \\
\frac{dJ(Î¸)}{dÎ¸} &amp;amp;= d_\textcolor{purple}{\textrm{outer}} \cdot d_\textcolor{orange}{\textrm{inner}}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\textcolor{purple}{\textrm{outer}} &amp;amp;= \textcolor{purple}{\frac{1}{2m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{(} \textcolor{orange}{\textrm{inner}} \textcolor{purple}{- Y_i)^2} &amp;amp;
\textcolor{orange}{\textrm{inner}} &amp;amp;= \textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0} + \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1} + \cdots + \textcolor{magenta}{Î¸_n}\textcolor{teal}{x_n} \\
d_\textcolor{purple}{\textrm{outer}} &amp;amp;= \textcolor{purple}{\frac{\cancel{2}}{\cancel{2}m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{\Big[} \textcolor{orange}{h_Î¸(x^i)} \textcolor{purple}{{- Y_i\Big]}^2} &amp;amp; d_\textcolor{orange}{\textrm{inner}} &amp;amp;= ???
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;This is where I got stuck. &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; is a collection of values, not just a single value. Each knob on our radio affects the output individually, and we have to determine the individual effect of eachÂ knob.&lt;/p&gt;
&lt;p&gt;It helps to start by breaking down what the chain rule is actuallyÂ saying.&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{d[\textcolor{purple}{\textrm{outer}}(\textcolor{orange}{\textrm{inner}}(x))]}{dx} = \frac{d_\textcolor{purple}{\textrm{outer}}}{d_\textcolor{orange}{\textrm{inner}}} \cdot \frac{d_\textcolor{orange}{inner}}{dx} $$&lt;/div&gt;
&lt;p&gt;This means our â€œouter derivativeâ€ &lt;span class="math"&gt;\(d_\textcolor{purple}{\textrm{outer}}\)&lt;/span&gt; tells us how much our cost function &lt;span class="math"&gt;\(J(Î¸)\)&lt;/span&gt; changes in response to a given change in our hypothesis &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;. We now need to find the â€œinner derivativeâ€ &lt;span class="math"&gt;\(d_{\textcolor{orange}{\textrm{inner}}}\)&lt;/span&gt;, which tells us how much our hypothesis &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; changes in response to a given change in our weights &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But since &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; is a collection of values, there isnâ€™t a single derivative, but rather several &lt;em&gt;partial derivatives&lt;/em&gt;, which indicate how much our hypothesis &lt;span class="math"&gt;\(h(x^i)\)&lt;/span&gt; for a specific sample (house) &lt;span class="math"&gt;\(x^i\)&lt;/span&gt; changes in response to a given change in &lt;em&gt;each&lt;/em&gt; of the weights &lt;span class="math"&gt;\(Î¸_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Another labeling conventionâ€”just as &lt;span class="math"&gt;\(i\)&lt;/span&gt; is used to refer to, or â€œindex,â€ samples from &lt;span class="math"&gt;\(1\)&lt;/span&gt; through &lt;span class="math"&gt;\(m\)&lt;/span&gt;, the total number of samples, lowercase &lt;span class="math"&gt;\(j\)&lt;/span&gt; is used to index features from &lt;span class="math"&gt;\(0\)&lt;/span&gt; through &lt;span class="math"&gt;\(n\)&lt;/span&gt;, the total number of features. To return to our Einstein notation,
&lt;div class="math"&gt;$$ x^{1 \leq \space i^\textrm{th} \textrm{ sample} \space \leq \space m \textrm{ samples}}_{0 \space \leq \space j^\textrm{th} \textrm{ feature} \space \leq \space n \textrm{ features}} $$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In math notation, this is written with a funny â€œdâ€ called a â€œdel,â€ &lt;span class="math"&gt;\(\partial\)&lt;/span&gt;, likeÂ this:&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_Î¸(x^i)}{\partial Î¸_j} $$&lt;/div&gt;
&lt;p&gt;This looks crazy, but the process of finding these partial derivatives is really the same as finding a normal derivative, except you &lt;em&gt;treat all the other variables as constant&lt;/em&gt;, effectively ignoring them. So, for we now only have to concern ourselvesÂ with&lt;/p&gt;
&lt;div class="math"&gt;$$ h_{Î¸_j}(x^i) = \begin{cases}
\textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0^i} \textcolor{lightgray}{+Î¸_1x_1^i + Î¸_2x_2^i} &amp;amp; \text{when } j=0 \\
\textcolor{lightgray}{Î¸_0x_0^i} \textcolor{lightgray}{+} \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1^i} \textcolor{lightgray}{+Î¸_2x_2^i} &amp;amp; \text{when } j=1 \\
\textcolor{lightgray}{Î¸_0x_0^i + Î¸_1x_1^i} \textcolor{lightgray}{+} \textcolor{magenta}{Î¸_2}\textcolor{teal}{x_2^i} &amp;amp; \text{when } j=2 \\
\end{cases} $$&lt;/div&gt;
&lt;p&gt;The derivative of a variable times something else is just the &lt;em&gt;something else&lt;/em&gt;. (For a line &lt;span class="math"&gt;\(y = 2x\)&lt;/span&gt;, &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt; is just &lt;span class="math"&gt;\(2\)&lt;/span&gt;, since its slope will be 2 at every point along the line.)Â Thus,&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_Î¸(x)}{\partial Î¸_j} = \begin{cases}
\frac{\partial h_Î¸(x)}{\partial Î¸_0} &amp;amp;= \textcolor{lightgray}{Î¸_0} \textcolor{teal}{x_0^i} &amp;amp; \text{when } j=0 \\[0.5em]
\frac{\partial h_Î¸(x)}{\partial Î¸_1} &amp;amp;= \textcolor{lightgray}{Î¸_1} \textcolor{teal}{x_1^i} &amp;amp; \text{when } j=1 \\[0.5em]
\frac{\partial h_Î¸(x)}{\partial Î¸_2} &amp;amp;= \textcolor{lightgray}{Î¸_2} \textcolor{teal}{x_2^i} &amp;amp; \text{when } j=2
\end{cases} $$&lt;/div&gt;
&lt;p&gt;OrÂ just&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_Î¸(x)}{\partial Î¸_j} = \left\{\begin{array}{lr}
x_0^i &amp;amp; \text{when } j=0 \\[0.5em]
x_1^i &amp;amp; \text{when } j=1 \\[0.5em]
x_2^i &amp;amp; \text{when } j=2
\end{array}\right\}
= \textcolor{red}{x_j^i} $$&lt;/div&gt;
&lt;p&gt;Thatâ€™s &lt;span class="math"&gt;\(d_{\textcolor{orange}{\textrm{inner}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\frac{\partial{J(Î¸)}}{\partial{Î¸_j}} &amp;amp;= d_\textcolor{purple}{\textrm{outer}} \cdot d_\textcolor{orange}{\textrm{inner}} \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m (h_Î¸(x^i) - Y_i) \cdot \textcolor{red}{x_j^i}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Phew! That was a lot of abstract math. Finally, we have something that can be translated intoÂ code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;which_weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;which_weight&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;effect_of_all_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="stochastic-gradient-descent-making-regression-lines-fit-again"&gt;Stochastic gradient descent: Making regression lines fitÂ again&lt;/h2&gt;
&lt;p&gt;Strictly speaking, &lt;span class="math"&gt;\(\frac{1}{m} \sum_{i=1}^m (h_Î¸(x^i) - Y_i) \cdot x_j^i\)&lt;/span&gt; is not a derivative, but a &lt;strong&gt;gradient&lt;/strong&gt;â€”a collection of partial derivatives. In high school calculus, the derivative at a given point is visualized as the line that is tangent to the graphâ€™s curve at that point. In multivariable calculus, the gradient at a given point is visualized as the &lt;em&gt;plane&lt;/em&gt; that is tangent to the graphâ€™s surface at theÂ point.&lt;/p&gt;
&lt;p&gt;In more concrete terms, imagine running a small piece of cardboard around the sides of a coffee mug so that the cardboard follows the curvature of the mug. Every point on the surface of the mug corresponds to some combination of weights, and the closer we are to the top of the mug, the greater the value of our cost function is, and so the more inaccurate our prediction is. We want to find the bottom of the mug, where the piece of cardboard is parallel to the ground, because that is where the value of the cost function is as low asÂ possible.&lt;/p&gt;
&lt;p&gt;When that value is zero, the line would fit our data perfectly. However, thatâ€™s not possible for real-world data, so we will settle for the lowest valueâ€”that is, we want to &lt;em&gt;minimize&lt;/em&gt; the costÂ function.&lt;/p&gt;
&lt;div class="math"&gt;$$ \underset{Î¸}{\arg\min} \, J(Î¸) $$&lt;/div&gt;
&lt;p&gt;In the language of math (and neural networks), this is called &lt;strong&gt;stochastic gradient descent&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;gradient&lt;/em&gt; is the thing weâ€™re trying toÂ minimize.&lt;/li&gt;
&lt;li&gt;This process is &lt;em&gt;stochastic&lt;/em&gt; (random) because we start with random weights (all zeros), which puts us at a random point on theÂ mug.&lt;/li&gt;
&lt;li&gt;It is a &lt;em&gt;descent&lt;/em&gt; because we want to move down to a progressively flatter region of the mug with each attempt (combination ofÂ weights).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The descent occurs in â€œsteps.â€ Imagine, for a moment, a basic parabola &lt;span class="math"&gt;\(f(x) = x^2\)&lt;/span&gt; instead of a mug. The derivative at any point &lt;span class="math"&gt;\(x\)&lt;/span&gt; is &lt;span class="math"&gt;\(2x\)&lt;/span&gt;. Positive &lt;span class="math"&gt;\(x\)&lt;/span&gt; values give us positive derivatives and negative &lt;span class="math"&gt;\(x\)&lt;/span&gt; values give us negative derivatives. If we started at some point to the right of 0 and wanted to follow the parabola to its trough, we could do that by subtracting something from &lt;span class="math"&gt;\(x\)&lt;/span&gt;. Likewise, if we started at some point to the left of 0, weâ€™d want to add something to &lt;span class="math"&gt;\(x\)&lt;/span&gt;â€”or rather, subtract a negativeÂ value.&lt;/p&gt;
&lt;p&gt;This means that if we start at any point &lt;span class="math"&gt;\(x\)&lt;/span&gt; and subtract &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt;, we will tend toward the trough. We donâ€™t necessarily know exactly what our new &lt;span class="math"&gt;\(x\)&lt;/span&gt; value will be, but we can assume that subtracting &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt; again will take us closer to the trough, although slightly less closer. Each step brings us increasingly closer but in progressively smaller steps. At some point, we will reach &lt;strong&gt;convergence&lt;/strong&gt;, or a point that is close enough toÂ minimum.&lt;/p&gt;
&lt;p&gt;The same applies to gradients. The gradient for any set of weights &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; tells us the &lt;em&gt;opposite&lt;/em&gt; direction we should go in to find the bottom of the mug. That means that if we start with some initial collection of weights &lt;span class="math"&gt;\(Î¸\)&lt;/span&gt; and keep subtracting the gradient, which is notated &lt;span class="math"&gt;\(\nabla J\)&lt;/span&gt;, we should eventually arrive at theÂ bottom.&lt;/p&gt;
&lt;div class="math"&gt;$$ \textrm{repeat } Î¸ := Î¸ - \nabla J \textrm{ until convergence} $$&lt;/div&gt;
&lt;p&gt;But try translating this intoÂ Python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gradient_of_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;last_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;gradient_of_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
    &lt;span class="n"&gt;last_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_weights&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convergence&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;

&lt;span class="n"&gt;minimum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In doing so, a couple of questions arise (besides the suspicion that there are way too many loops andÂ functions):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do you compare weights so that you can know which of two collections is â€œlesserâ€ and which isÂ â€œgreaterâ€?&lt;/li&gt;
&lt;li&gt;How do you know when youâ€™ve reachedÂ convergence?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Itâ€˜s clear that we have more or less come as far as we can with the level of math and coding that we have used soÂ far.&lt;/p&gt;
&lt;h1 id="part-2-crazy-math-beautiful-code"&gt;Part 2: Crazy math, beautifulÂ code&lt;/h1&gt;
&lt;p&gt;When you think about it, it almost seems a little backwards to call linear algebra the starting point of machine learning. After all, weâ€™ve come this far without it. Multivariable calculus strikes me as more fundamental, although I suppose that it might be hard to imagine the output of a two-variable function as a bowl- or mug-shaped object without the concept ofÂ vectors.&lt;/p&gt;
&lt;p&gt;In any case, itâ€™s time for that ZenÂ uppercut.&lt;/p&gt;
&lt;p&gt;I wonâ€™t go into the mechanics of vector and matrix operations here; they are too tedious to write about, and Iâ€™m certainly not the best person to explain them. What Iâ€™m more interested in is the concept of &lt;strong&gt;vectorization&lt;/strong&gt;: the â€œtranslationâ€ (pun intended) of the algebra and calculus above into linear algebra and multivariable calculus, as well as what that looks like in Python (usingÂ NumPy).&lt;/p&gt;
&lt;h2 id="vectorize-all-the-things"&gt;Vectorize all theÂ things!!!&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ninja mode activated! (That was easy, eh?) First of all, letâ€™s convert all lists to &lt;em&gt;vectors&lt;/em&gt; and all lists of lists to &lt;em&gt;matrices&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textbf X =
\begin{bmatrix}
  x_1^1 &amp;amp; x_2^1 &amp;amp; \dots  &amp;amp; x_n^1 \\[0.5em]
  x_1^2 &amp;amp; x_2^2 &amp;amp; \dots  &amp;amp; x_n^2 \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  x_1^m &amp;amp; x_2^m &amp;amp; \dots  &amp;amp; x_n^m
\end{bmatrix} \qquad{}
&amp;amp; \vec Î¸ = \begin{bmatrix} Î¸_0 \\ Î¸_1 \\ Î¸_2 \\ \vdots \\ Î¸_n \end{bmatrix}
&amp;amp; \vec y = \begin{bmatrix} y_0 \\ y_1 \\ \vdots \\ y_m \end{bmatrix}
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;Letâ€™s use &lt;code class="highlight"&gt;scikit-learn&lt;/code&gt; to generate a suitable dataset for us. Since we wonâ€™t have to do any of the computations by hand, letâ€™s go wild with the number of features and samples. We also need a vector with our initial weights (allÂ zeros).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# as many rows as X has columns, and 1 column&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One important thing to note is that the dummy feature &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; needs to be added to the data. Also, &lt;code class="highlight"&gt;scikit-learn&lt;/code&gt; generates a &lt;code class="highlight"&gt;y&lt;/code&gt; array that doesnâ€™t have the proper dimensions of a vector for someÂ reason.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# padding for bias column&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# this just fixes a quirk of sklearn's output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \textbf X =
\begin{bmatrix}
  \textcolor{red}1 &amp;amp; x_1^1 &amp;amp; x_2^1 &amp;amp; \dots &amp;amp; x_n^1 \\[0.5em]
  \textcolor{red}1 &amp;amp; x_1^2 &amp;amp; x_2^2 &amp;amp; \dots &amp;amp; x_n^2 \\[0.5em]
  \textcolor{red}\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{red}1 &amp;amp; x_1^m &amp;amp; x_2^m &amp;amp; \dots &amp;amp; x_n^m
\end{bmatrix} \qquad{}
&amp;amp; \vec Î¸ = \begin{bmatrix} Î¸_0 \\ Î¸_1 \\ Î¸_2 \\ \vdots \\ Î¸_n \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;The hypothesis function &lt;span class="math"&gt;\(h_Î¸(x^i)\)&lt;/span&gt; can now be written succinctly as the product of the â€œhousesâ€ matrix &lt;span class="math"&gt;\(\textbf X\)&lt;/span&gt; and the weights vector &lt;span class="math"&gt;\(\vec Î¸\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \textbf X\vec Î¸ &amp;amp;= \begin{bmatrix}
  \textcolor{teal}{x_0^1} &amp;amp; \textcolor{teal}{x_1^1} &amp;amp; \textcolor{teal}{x_2^1} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^1} \\[0.5em]
  \textcolor{teal}{x_0^2} &amp;amp; \textcolor{teal}{x_1^2} &amp;amp; \textcolor{teal}{x_2^2} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^2} \\[0.5em]
  \textcolor{teal}\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{teal}{x_0^m} &amp;amp; \textcolor{teal}{x_1^m} &amp;amp; \textcolor{teal}{x_2^m} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^m}
\end{bmatrix} \begin{bmatrix} \textcolor{magenta}{Î¸_0} \\ \textcolor{magenta}{Î¸_1} \\ \textcolor{magenta}{Î¸_2} \\ \vdots \\ \textcolor{magenta}{Î¸_n} \end{bmatrix} \\
&amp;amp;= \begin{bmatrix}
  \textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0^1} &amp;amp; \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1^1} &amp;amp; \textcolor{magenta}{Î¸_2}\textcolor{teal}{x_2^1} &amp;amp; \dots &amp;amp; \textcolor{magenta}{Î¸_n}\textcolor{teal}{x_n^1} \\[0.5em]
  \textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0^2} &amp;amp; \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1^2} &amp;amp; \textcolor{magenta}{Î¸_2}\textcolor{teal}{x_2^2} &amp;amp; \dots &amp;amp; \textcolor{magenta}{n_2}\textcolor{teal}{x_n^2} \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0^m} &amp;amp; \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1^m} &amp;amp; \textcolor{magenta}{Î¸_2}\textcolor{teal}{x_2^m} &amp;amp; \dots &amp;amp; \textcolor{magenta}{Î¸_n}\textcolor{teal}{x_n^m}
\end{bmatrix} \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Here is the generalized form of linear regression before and after vectorization, followed by the vectorized NumPyÂ version.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Notice how the non-vectorized version inherently refers to only one sample at a time, with the superscript &lt;span class="math"&gt;\(i\)&lt;/span&gt;. This implies the equation is true for each &lt;span class="math"&gt;\(i\)&lt;/span&gt;, but the vectorized version automatically includes every sample at the same time without us even having to know how many there are. So letâ€™s use the plural &lt;code class="highlight"&gt;hypotheses&lt;/code&gt; to name theÂ result.&lt;/li&gt;
&lt;li&gt;Look at how simple the math expression and the code become! (Of course, it does rely on a sufficient understanding of matrixÂ multiplication.)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
h_Î¸(x^i) &amp;amp;= \textcolor{magenta}{Î¸_0}\textcolor{teal}{x_0^i} + \textcolor{magenta}{Î¸_1}\textcolor{teal}{x_1^i} + \cdots + \textcolor{magenta}{Î¸_n}\textcolor{teal}{x_n^i} \\
h_Î¸(\textbf X) &amp;amp;= \textbf X\vec Î¸
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hypotheses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="c1"&gt;# @ is short for matrix multiplication&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="vectorizing-the-cost-function-so-fresh-so-clean"&gt;Vectorizing the cost function: So fresh, soÂ clean&lt;/h2&gt;
&lt;p&gt;This makes it easy to express the error as the difference between the hypothesis and the actualÂ value:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{error} &amp;amp;= \hat{Y}_i - Y_i \\
&amp;amp;= h_Î¸(x^i) - y_i \\
\vec e &amp;amp;= \textbf X\vec Î¸ - \vec y
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hypotheses&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Recall that the cost function involves the sum of squared errors. In linear algebra, summation can be expressed as the product of a transposed vector of ones and a vector with the values to be summed, which struck me as a very cleverÂ manipulation.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\vec o = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
&amp;amp; \quad{} \vec e = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_m \end{bmatrix}
= \begin{bmatrix} h_Î¸(x^1) - y_1 \\ h_Î¸(x^2) - y_2 \\ \vdots \\ h_Î¸(x^m) - y_m \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\vec o^T\vec e &amp;amp;= \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_m \end{bmatrix} \\
&amp;amp;= \textcolor{lightgray}1 \cdot e_1 + \textcolor{lightgray}1 \cdot e_2 + \cdots + \textcolor{lightgray}1 \cdot e_m = \sum_{i=1}^m e_i
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;The cost function thenÂ becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
J(Î¸) &amp;amp;= \frac{1}{2m}\sum_{i=1}^m \Big[h_Î¸(x^i) - Y_i\Big]^2 \\
&amp;amp;= \frac{1}{2m}\sum_{i=1}^m {(e_i)}^2 \\
&amp;amp;= \frac{1}{2m} \vec o^T \vec e^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the NumPy function &lt;code class="highlight"&gt;np.square(z)&lt;/code&gt; is faster than &lt;code class="highlight"&gt;z ** 2&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="c1"&gt;# just a random big dataset for testing&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mf"&gt;7.99&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mf"&gt;1.19&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mf"&gt;7.01&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and &lt;code class="highlight"&gt;o.T @ np.square(z))&lt;/code&gt; (that is, &lt;span class="math"&gt;\(\vec o^T\textbf Z^2\)&lt;/span&gt;)  blows &lt;code class="highlight"&gt;sum(z ** 2)&lt;/code&gt; (that is, &lt;span class="math"&gt;\(\sum_{i=1}^m (z_i)^2\)&lt;/span&gt;) out of theÂ water:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1.36&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mf"&gt;16.3&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.37&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mf"&gt;12.1&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# because rando has more than 1 column&lt;/span&gt;
&lt;span class="mf"&gt;25.1&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mi"&gt;603&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, itâ€™s helpful to turn our cost function into a Python function that takes &lt;span class="math"&gt;\(\textbf X, \vec y, \vec Î¸\)&lt;/span&gt; as its inputs. This will make it easier to evaluate our modelÂ later.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;cost_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 1x1 matrix&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# plain number&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="vectorizing-the-gradient-ninja-mode-on-overdrive"&gt;Vectorizing the gradient: Ninja mode onÂ overdrive&lt;/h2&gt;
&lt;p&gt;On to the gradient. This is where linear algebra really kicks this thing into highÂ gear.&lt;/p&gt;
&lt;p&gt;(This is also where I get to show off my LaTeXÂ chops.)&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\nabla J = \frac{\partial J(Î¸)}{\partial Î¸_j} &amp;amp;= \Bigg\{\frac{\partial J(Î¸)}{\partial Î¸_0}, \frac{\partial J(Î¸)}{\partial Î¸_1}, \cdots, \frac{\partial J(Î¸)}{\partial Î¸_n}\Bigg\} \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m \Big[h_Î¸(x^i) - Y_i\Big]^2 \cdot x_j^i \qquad{} \textrm{for } 0 \leq j \leq n \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m {(e_i)}^2 x_j^i \qquad{}\qquad{}\qquad{}\qquad{}\qquad{}  " \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m x_j^i {(e_i)}^2 \qquad{}\qquad{}\qquad{}\qquad{}\qquad{}  " \\
&amp;amp;= \Bigg\{ \textcolor{teal}{
  \frac{1}{m} \sum_{i=1}^m x_0^i {(e_i)}^2, \frac{1}{m} \sum_{i=1}^m x_1^i {(e_i)}^2, \cdots, \frac{1}{m} \sum_{i=1}^m x_n^i {(e_i)}^2
  }\Bigg\}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Transposing &lt;span class="math"&gt;\(\textbf X\)&lt;/span&gt; and squaring &lt;span class="math"&gt;\(\vec e\)&lt;/span&gt; givesÂ us:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textbf X^T = \begin{bmatrix}
  x_0^1 &amp;amp; x_0^2 &amp;amp; \dots &amp;amp; x_0^m \\[0.5em]
  x_1^1 &amp;amp; x_1^2 &amp;amp; \dots &amp;amp; x_1^m \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  x_n^1 &amp;amp; x_n^2 &amp;amp; \dots  &amp;amp; x_n^m
\end{bmatrix} &amp;amp; \quad{} \vec e^2 = \begin{bmatrix} {(e_1)}^2 \\[0.5em] {(e_2)}^2 \\[0.5em] \vdots \\[0.5em] {(e_m)}^2 \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$ \textbf X^T \vec e^2 = \begin{bmatrix}
  x_0^1 {(e_1)}^2 + x_0^2 {(e_2)}^2 + x_0^3 {(e_3)}^2 + \cdots + x_0^m {(e_m)}^2 \\[0.5em]
  x_1^1 {(e_1)}^2 + x_1^2 {(e_2)}^2 + x_1^3 {(e_3)}^2 + \cdots + x_1^m {(e_m)}^2 \\[0.5em]
  \vdots \\[0.5em]
  x_n^1 {(e_1)}^2 + x_n^2 {(e_2)}^2 + x_n^3 {(e_3)}^2 + \cdots + x_n^m {(e_m)}^2
\end{bmatrix} = \begin{bmatrix}
  \sum_{i=1}^m x_0^i{(e_1)}^2 \\[0.5em]
  \sum_{i=1}^m x_1^i{(e_1)}^2 \\[0.5em]
  \vdots \\[0.5em]
  \sum_{i=1}^m x_n^i{(e_1)}^2 \\
\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Notice how multiplying this result by &lt;span class="math"&gt;\(\frac{1}{m}\)&lt;/span&gt; gives us a vector containing the same values highlighted above inÂ teal.&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{1}{m} \textbf X^T \vec e^2 = \frac{1}{m} \begin{bmatrix}
  \sum_{i=1}^m x_0^i{(e_1)}^2 \\[0.5em]
  \sum_{i=1}^m x_1^i{(e_1)}^2 \\[0.5em]
  \vdots \\[0.5em]
  \sum_{i=1}^m x_n^i{(e_1)}^2 \\
\end{bmatrix} = \begin{bmatrix}
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_0^i{(e_1)}^2 } \\[0.5em]
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_1^i{(e_1)}^2 } \\[0.5em]
  \textcolor{teal}{ \vdots } \\[0.5em]
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_n^i{(e_1)}^2 } \\
\end{bmatrix} = \begin{bmatrix}
  \frac{\partial J(Î¸)}{\partial Î¸_0} \\[0.5em]
  \frac{\partial J(Î¸)}{\partial Î¸_1} \\[0.5em]
  \vdots \\[0.5em]
  \frac{\partial J(Î¸)}{\partial Î¸_n}
\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Astonishingly, that gigantic mess can be expressedÂ as&lt;/p&gt;
&lt;div class="math"&gt;$$ \nabla J = \frac{1}{m} \textbf X^T \vec e^2 $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Finally&lt;/em&gt;, we can work out the last function, the gradient descentÂ function:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered} \vec Î¸ := \vec Î¸ - Î±\frac{1}{m} \textbf X^T \vec e^2 \\
\textrm{repeat until convergence} \end{gathered} $$&lt;/div&gt;
&lt;p&gt;Hey, whereâ€™d that &lt;span class="math"&gt;\(Î±\)&lt;/span&gt; come from? Thatâ€™s the &lt;strong&gt;learning rate&lt;/strong&gt;, a small number that adjusts the size of each training step. Too large and you jump right over the minimum; too small and you never reach theÂ minimum.&lt;/p&gt;
&lt;p&gt;For now, letâ€™s choose an arbitrary value for &lt;span class="math"&gt;\(Î±\)&lt;/span&gt; and disregard the whole bit about convergence. If we wanted to perform stochastic gradient descent with 100 steps, this is how weâ€™d doÂ it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Letâ€™s refactor this as a function &lt;code class="highlight"&gt;train_lr_model&lt;/code&gt; that takes &lt;span class="math"&gt;\(\textbf X, \vec y\)&lt;/span&gt;, and the number of steps (training epochs) as its inputs, and outputs the weights &lt;span class="math"&gt;\(\vec Î¸\)&lt;/span&gt;. Along the way, letâ€™s have it tell us the cost. If all goes well, we should see that number approach zero as trainingÂ progresses.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# dot products are single values, but NumPy returns them as 1x1 matrices&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;weights_300_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can predict the output for a random set of featureÂ values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;mystery_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mystery_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;mystery_input&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# dummy feature&lt;/span&gt;
&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mystery_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_300_epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I like to geek out on math notation and wrote this function to generate LaTeX for the equation of theÂ model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;expand_model_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{round(w[0], 2)}x_{i}"&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"$$ h_Î¸(x) &lt;/span&gt;&lt;span class="se"&gt;\a&lt;/span&gt;&lt;span class="s2"&gt;pprox {' + '.join(terms)} $$"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ h_Î¸(x) \approx -2.36x_0 + 63.26x_1 + 61.15x_2 + 88.99x_3 + 0.82x_4 + 58.95x_5 $$&lt;/div&gt;
&lt;p&gt;Weâ€™re not done with linear regression, but letâ€™s recap so that this post canÂ end.&lt;/p&gt;
&lt;h2 id="summary-so-far"&gt;Summary soÂ far&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;Broke&lt;/th&gt;
&lt;th align="center"&gt;Woke&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear regression model&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$h_Î¸(x) = Î¸_0x_0 + Î¸_1x_1 + \cdots + Î¸_nx_n$$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$h_Î¸(\textbf X) = \textbf X\vec Î¸$$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cost function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$J(Î¸) = \frac{1}{2m}\sum_{i=1}^m{\Big[h_Î¸(x^i) - y_i\Big]}^2$$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \begin{gathered} \vec e = h_Î¸{\textbf X} - \vec y \\ J(Î¸) = \frac{1}{2m}\vec o^T\vec e^2 \end{gathered} $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient of cost function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \frac{\partial J(Î¸)}{\partial Î¸_j} = \frac{1}{m}\sum_{i=1}^m\Big[h_Î¸(x^i) - y_i\Big]x_j^i $$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \nabla J = \frac{1}{m}\textbf X^T\vec e $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient descent function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ Î¸_j := Î¸_j - Î±\frac{\partial J(Î¸)}{\partial Î¸_j} $$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \vec Î¸ := \vec Î¸ - Î±\nabla J $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Such complex math can be applied to a linear regression model trained in some 20 lines ofÂ Python!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# padding for bias column&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# this just fixes a quirk of sklearn's output&lt;/span&gt;

&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;cost_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Completed {epochs} epochs of training."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Final cost: {cost(X, y, weights)}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="comparison-of-performance"&gt;Comparison ofÂ performance&lt;/h2&gt;
&lt;p&gt;This is a contrived comparison, but it helps to illustrate very clearly the point of jumping through all of these mathematicalÂ hoops.&lt;/p&gt;
&lt;p&gt;On the 500-sample, 5-feature dataset weâ€™ve been using, the vectorized gradient descent function runs over &lt;strong&gt;7,000 times faster&lt;/strong&gt; than the terrible, monstrous, donâ€™t-say-I-didnâ€™t-warn-you procedural version from the beginning of thisÂ post.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;train_lr_model_procedurally&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;14.2&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mi"&gt;609&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;train_lr_model_vectorizedly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;2.28&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="mf"&gt;64.9&lt;/span&gt; &lt;span class="err"&gt;Âµ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;Â±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="remaining-questions-for-future-posts-links-will-be-included-upon-publication"&gt;Remaining questions for future posts (links will be included uponÂ publication)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Are there any other simple techniques to speed this up? (MeanÂ normalization)&lt;/li&gt;
&lt;li&gt;Are there any complex techniques to speed this up? (AutomaticÂ differentiation)&lt;/li&gt;
&lt;li&gt;&lt;a href="from-zero-to-ero"&gt;How do we know when weâ€™ve reached convergence?Â (Epsilon)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How can &lt;em&gt;linear&lt;/em&gt; (as opposed to &lt;em&gt;logistic&lt;/em&gt;) regression be applied to language tasks? (General backpropagation of neuralÂ networks)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;The Essence of Linear Algebra&lt;/a&gt; (video series), GrantÂ Sanderson&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2"&gt;Linear Regression using Python&lt;/a&gt;, AnimeshÂ Agarwal&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ritchieng.com/multi-variable-linear-regression/"&gt;Linear Regression with Multiple Variables&lt;/a&gt;, RitchieÂ Ng&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb"&gt;Understanding and Calculating the Cost Function for Linear Regression&lt;/a&gt;, LachlanÂ Miller&lt;/li&gt;
&lt;li&gt;&lt;a href="http://anwarruff.com/the-linear-regression-cost-function-in-matrix-form/"&gt;The Linear Regression Cost Function in Matrix Form&lt;/a&gt;, AnwarÂ Ruff&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version"&gt;Multivariable chain rule, simple version&lt;/a&gt;, KhanÂ Academy&lt;/li&gt;
&lt;/ul&gt;</content><category term="koan"></category><category term="multivariable calculus"></category><category term="stochastic gradient descent"></category><category term="linear algebra"></category></entry></feed>