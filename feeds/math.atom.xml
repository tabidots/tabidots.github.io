<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Judo Salt Genius - Math</title><link href="http://tabidots.github.io/" rel="alternate"></link><link href="http://tabidots.github.io/feeds/math.atom.xml" rel="self"></link><id>http://tabidots.github.io/</id><updated>2019-03-31T19:47:21+07:00</updated><entry><title>Number Theory in Clojure: The Fundamental Theorem of Arithmetic</title><link href="http://tabidots.github.io/2019/03/fundamental-theorem-arithmetic" rel="alternate"></link><published>2019-03-31T19:47:21+07:00</published><updated>2019-03-31T19:47:21+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-03-31:/2019/03/fundamental-theorem-arithmetic</id><summary type="html">&lt;p&gt;The first post in a new series exploring number theory in Clojure, starting with a discussion of primality testing and prime&amp;nbsp;factorization.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;I’ve had to hit the pause button on the blog for a bit because life got in the way, but I haven’t stopped my mathematical explorations. Actually, I got turned on to &lt;a href="https://projecteuler.net"&gt;Project Euler&lt;/a&gt; in the meantime, which became something of an addiction. Most people solve the problems in C/C++ or Python, but I decided to take the opportunity to sharpen my skills in Clojure (which is, let’s face it, the best-designed programming language out there 😉).&lt;/p&gt;
&lt;p&gt;In the beginning, the “mathematical discovery factor” increased with the difficulty, but after solving about 90 problems, those returns have plateaued and are now declining in relation to the diffficulty. So now it’s time to start posting about some of the cool things I’ve discovered along the way, and use this blog to explore those topics further.&lt;/p&gt;
&lt;h1 id="prime-time"&gt;Prime time&lt;/h1&gt;
&lt;p&gt;Thanks to Chapter 1 of &lt;a href="https://pimbook.org"&gt;A Programmer’s Introduction to Mathematics&lt;/a&gt;, I had already developed an interest in number theory and cryptography. This interest was further ignited by Project Euler, in whose problems number theory factors heavily (pun intended).&lt;/p&gt;
&lt;p&gt;Number theory is basically the study of the integers, &lt;span class="math"&gt;\(ℤ\)&lt;/span&gt;. This in turn shines the spotlight on prime numbers, a very special category of the integers due to their many interesting properties.&lt;/p&gt;
&lt;p&gt;In order to do anything interesting in number theory, we need to find a way to &lt;strong&gt;test the primality&lt;/strong&gt; of a number and to &lt;strong&gt;generate a sequence of prime numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In imperative languages, it’s typical to separate these two tasks, and approach the latter by implementing what is called a “prime sieve,” such as the &lt;a href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes"&gt;Sieve of Eratosthenes&lt;/a&gt;, which walks up an infinite sequence of numbers and crosses off more and more composite numbers with each iteration. You might start with an enormous array of &lt;code class="highlight"&gt;True&lt;/code&gt;s and switch the indices of composite numbers to &lt;code class="highlight"&gt;False&lt;/code&gt; as you go along.&lt;/p&gt;
&lt;p&gt;Clojure, on the other hand, is a functional language, in which mutating sequences in-place is neither idiomatic nor clean. &lt;a href="http://clj-me.cgrand.net/2009/07/30/everybody-loves-the-sieve-of-eratosthenes/"&gt;Lazy (lazily evaluated) infinite sequences are the way to go&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the above link shows, it is possible to implement a Sieve of Eratosthenes in Clojure, but it’s quite difficult to read, and in any less than expert hands, &lt;a href="http://www.learningclojure.com/2009/11/sieve-of-eratosthenes.html"&gt;quite ugly&lt;/a&gt; as well. Meanwhile, I’ve found that it’s simpler to &lt;code class="highlight"&gt;filter&lt;/code&gt; primes in Clojure than sieving them, which also conveniently integrates the task of testing primality as well.&lt;/p&gt;
&lt;p&gt;This approach is highly readable and, when memoized, its performance does not catastrophically degrade until you start needing primes greater than 1 million or so. Obviously, actual cryptographic applications would require a more industrial-strength implementation, as real-world cryptography deals with integers that could be as large as 256 bits (hundreds of digits long), but for armchair explorations of number theory, this approach will suffice.&lt;/p&gt;
&lt;h2 id="primes-on-trial"&gt;Primes on trial&lt;/h2&gt;
&lt;p&gt;This method is called “trial division,” since it involves dividing &lt;span class="math"&gt;\(n\)&lt;/span&gt; by numbers in a range to test for divisibility. First, we have to define divisibility. A dividend &lt;span class="math"&gt;\(n\)&lt;/span&gt; is divisible by a divisor &lt;span class="math"&gt;\(d\)&lt;/span&gt; if no remainder is left after the division, or in other words, if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is a multiple of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In terms of modular arithmetic, this is conveniently expressed as a &lt;em&gt;congruence&lt;/em&gt;: &lt;span class="math"&gt;\(n \equiv 0 \mod d\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;divisible?&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zero? &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;mod&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The most naïve, brute-force approach is to literally divide &lt;span class="math"&gt;\(n\)&lt;/span&gt; by all numbers in &lt;span class="math"&gt;\([1, n)\)&lt;/span&gt; and return &lt;code class="highlight"&gt;true&lt;/code&gt; (&lt;span class="math"&gt;\(n\)&lt;/span&gt; is prime) if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is divisible by &lt;em&gt;none&lt;/em&gt; of those numbers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;brute-force-prime?&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;not-any? &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;partial &lt;/span&gt;&lt;span class="nv"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is extremely slow, though. For a number like &lt;span class="math"&gt;\(45565962173\)&lt;/span&gt;, this is completely intractable, but even  generating primes up to &lt;span class="math"&gt;\(100000\)&lt;/span&gt; is impractical with this method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;time &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;doall &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;brute-force-prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="c1"&gt;;; Elapsed time: 67653.900002 msecs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A smarter way to do this starts with noticing that only potential divisors up to &lt;span class="math"&gt;\(\sqrt{n}\)&lt;/span&gt; need to be tried, because any divisor less than that will have a complementary divisor on the other side of &lt;span class="math"&gt;\(\sqrt{n}\)&lt;/span&gt;. For example, &lt;span class="math"&gt;\(\sqrt{28} \approx 5.3\)&lt;/span&gt;. &lt;span class="math"&gt;\(28\)&lt;/span&gt; is divisible by &lt;span class="math"&gt;\(2\)&lt;/span&gt;, and this division yields the complementary divisor &lt;span class="math"&gt;\(14\)&lt;/span&gt;. Likewise, &lt;span class="math"&gt;\(4\)&lt;/span&gt; yields &lt;span class="math"&gt;\(7\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="integer-square-root"&gt;Integer square root&lt;/h2&gt;
&lt;p&gt;An &lt;a href="https://en.wikipedia.org/wiki/Integer_square_root"&gt;upper integer bound &lt;span class="math"&gt;\(\lfloor \sqrt{n} \rfloor\)&lt;/span&gt;&lt;/a&gt; capable of handling fairly large numbers can be implemented quite simply as &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;bigint&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;Math/sqrt&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/code&gt;, taking advantage of Java interop.&lt;/p&gt;
&lt;p&gt;This has been effective for all of the Project Euler problems I’ve solved so far, though it is true that a proper arithmetical solution would not involve floating-point numbers. For sufficiently large &lt;span class="math"&gt;\(n\)&lt;/span&gt;, rounding errors would yield an upper bound that is too low to find all possible factors, especially in the worst case (where a very large &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the product of two very large primes that are similar but not equal in value).&lt;/p&gt;
&lt;p&gt;I use the &lt;a href="https://github.com/clojure/math.numeric-tower"&gt;clojure.math.numeric-tower&lt;/a&gt; library here as
using Java interop for exponentiation (&lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;Math/pow&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt;) similarly introduces floating-point numbers, and &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;tower/expt&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt; is ever-so-slightly faster than &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;repeat &lt;/span&gt;&lt;span class="nv"&gt;e&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a bulletproof integer square root function, which uses the simpler, faster method for &lt;span class="math"&gt;\(n &amp;lt; 10^24\)&lt;/span&gt; and a &lt;a href="https://cs.stackexchange.com/a/30383"&gt;more sophisticated algorithm&lt;/a&gt; (which I don’t really understand) for larger numbers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;clojure.math.numeric-tower&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;tower&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;isqrt&lt;/span&gt;
  &lt;span class="s"&gt;"floor(√n). When incremented, provides an upper bound for factorization."&lt;/span&gt;
  &lt;span class="c1"&gt;;; Java interop is super fast but not accurate for n &amp;gt; 1E24 (approx) due to&lt;/span&gt;
  &lt;span class="c1"&gt;;; floating-point rounding. Uses a slightly slower but pinpoint-precise method for n &amp;gt; 1E24.&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt; &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nv"&gt;E24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;Math/sqrt&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;bigint&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;;; https://cs.stackexchange.com/a/30383&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;half-bit-length&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;quot &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.bitLength&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;bigint&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;loop &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;tower/expt&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;half-bit-length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
             &lt;span class="nv"&gt;b&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt;
             &lt;span class="nv"&gt;c&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zero? &lt;/span&gt;&lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;gt; &lt;/span&gt;&lt;span class="nv"&gt;c&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;-&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;quot &lt;/span&gt;&lt;span class="nv"&gt;b&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;c&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="mi"&gt;-2&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
          &lt;span class="ss"&gt;:else&lt;/span&gt;     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;+&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;quot &lt;/span&gt;&lt;span class="nv"&gt;b&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;c&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt; &lt;span class="nv"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))))))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="testing-primality"&gt;Testing primality&lt;/h2&gt;
&lt;p&gt;Next, let’s implement some basic checks to eliminate the need to do any calculations in the majority of cases. When testing for primality, we generally only consider &lt;span class="math"&gt;\(ℤ\)&lt;/span&gt;, the positive integers. Furthermore, &lt;span class="math"&gt;\(1\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt; considered prime. So &lt;span class="math"&gt;\(n \leq 1\)&lt;/span&gt; is &lt;code class="highlight"&gt;false&lt;/code&gt; right off the bat. Next, all primes are odd except &lt;span class="math"&gt;\(2\)&lt;/span&gt;, so make an exception for &lt;span class="math"&gt;\(2\)&lt;/span&gt; and return &lt;code class="highlight"&gt;false&lt;/code&gt; for all &lt;code class="highlight"&gt;even?&lt;/code&gt; numbers.&lt;/p&gt;
&lt;p&gt;Clojure’s built-in &lt;code class="highlight"&gt;even?&lt;/code&gt; function has the added bonus of throwing an exception for &lt;code class="highlight"&gt;Ratio&lt;/code&gt;s and &lt;code class="highlight"&gt;float&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;This leaves all odd numbers &lt;span class="math"&gt;\(\geq 3\)&lt;/span&gt;. Weeding out all even numbers means that there is no need to check for even divisors (odd numbers only have odd divisors).&lt;/p&gt;
&lt;p&gt;The most idiomatic way to write this would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt;= &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="nv"&gt;false&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="nv"&gt;true&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;even?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;false&lt;/span&gt; &lt;span class="c1"&gt;;; Will also weed out non-integers&lt;/span&gt;
  &lt;span class="ss"&gt;:else&lt;/span&gt;     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;not-any? &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;inc &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;isqrt&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;but this is actually about 1.5x as slow as a more verbose translation using &lt;code class="highlight"&gt;loop&lt;/code&gt;/&lt;code class="highlight"&gt;recur&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;naive-prime?&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;lt;= &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="nv"&gt;false&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="nv"&gt;true&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;even?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;false&lt;/span&gt; &lt;span class="c1"&gt;;; Will also weed out non-integers&lt;/span&gt;
    &lt;span class="ss"&gt;:else&lt;/span&gt;     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;lim&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;int-root&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;loop &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;gt;= &lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="nv"&gt;lim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="nv"&gt;true&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;false&lt;/span&gt;
                    &lt;span class="ss"&gt;:else&lt;/span&gt;            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))))))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;memoize&lt;/span&gt; &lt;span class="nv"&gt;naive-prime?&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;whose performance, non-memoized and memoized, is leaps and bounds above the original brute-force trial division function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;criterium.core&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;c&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;naive-prime?&lt;/span&gt; &lt;span class="mi"&gt;45565962173&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 16.516873 ms&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 1.053979 ms&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime?&lt;/span&gt; &lt;span class="mi"&gt;45565962173&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 340.284934 ns&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 10.658400 ns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="leveraging-the-primality-test"&gt;Leveraging the primality test&lt;/h2&gt;
&lt;p&gt;This may not exactly be an industrial-strength method, but for my current purposes, it arguably allows more idiomatic and readable ways to accomplish tasks such as &lt;code class="highlight"&gt;filter&lt;/code&gt;, &lt;code class="highlight"&gt;take&lt;/code&gt;, and &lt;code class="highlight"&gt;nth&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;doall &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;naive-prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 265.236965 ms&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 6.784358 ms&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;doall &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 83.140024 ms&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 1.655784 ms&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;take &lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 88.645751 ns&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 27.586353 ns&lt;/span&gt;

&lt;span class="c1"&gt;;; (dec n) gets the nth prime, because the sequence is zero-indexed&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c/quick-bench&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;nth &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dec &lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time mean : 2.473162 ms&lt;/span&gt;
&lt;span class="c1"&gt;;; Execution time std-deviation : 123.136771 µs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="prime-factorization"&gt;Prime factorization&lt;/h1&gt;
&lt;p&gt;With that out of the way, we come to the cornerstone of number theory:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All integers greater than 1 are either a prime number or can be expressed as a unique product of prime numbers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The algorithm to find the prime factors of an integer &lt;span class="math"&gt;\(n\)&lt;/span&gt; is surprisingly simple to implement. Start with an empty list of factors and an infinite list of primes. Begin with the first prime (that is, &lt;span class="math"&gt;\(2\)&lt;/span&gt;) and divide &lt;span class="math"&gt;\(n\)&lt;/span&gt; by &lt;span class="math"&gt;\(2\)&lt;/span&gt; until you can’t anymore. Each time you divide, add a &lt;span class="math"&gt;\(2\)&lt;/span&gt; to your list of factors.&lt;/p&gt;
&lt;p&gt;Then, proceed to the next prime (&lt;span class="math"&gt;\(3\)&lt;/span&gt;) and repeat up the list, adding factors, until the result of your division is &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s try &lt;span class="math"&gt;\(168 = 2 \times 2 \times 2 \times 3 \times 7\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;loop &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;       &lt;span class="mi"&gt;168&lt;/span&gt;
       &lt;span class="nv"&gt;primes&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
       &lt;span class="nv"&gt;factors&lt;/span&gt; &lt;span class="p"&gt;[]]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;first &lt;/span&gt;&lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;          &lt;span class="nv"&gt;factors&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;primes&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;conj &lt;/span&gt;&lt;span class="nv"&gt;factors&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="ss"&gt;:else&lt;/span&gt;            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;rest &lt;/span&gt;&lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;factors&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="c1"&gt;;; [2 2 2 3 7]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Checks out. You can try a bunch of random numbers and you’ll instantly get its prime factorization, which is something like a fingerprint, since it’s unique for every number. Pretty cool!&lt;/p&gt;
&lt;p&gt;But again, a ridiculously large number like &lt;span class="math"&gt;\(245454537724879\)&lt;/span&gt; is too much for a simple algorithm. Actually, it’s not so much that the number is too large, but that it has very large prime factors, so it takes a very long time to iterate that far up the list of primes.&lt;/p&gt;
&lt;p&gt;This can be worked around &lt;em&gt;somewhat&lt;/em&gt; if we find the factors of &lt;span class="math"&gt;\(n\)&lt;/span&gt; first, then &lt;code class="highlight"&gt;filter&lt;/code&gt; the &lt;code class="highlight"&gt;prime?&lt;/code&gt; ones.&lt;/p&gt;
&lt;h2 id="regular-factorization"&gt;Regular factorization&lt;/h2&gt;
&lt;p&gt;This will look a bit like our initial foray into primality testing. Rather than checking if &lt;span class="math"&gt;\(n\)&lt;/span&gt; has no (that is, &lt;code class="highlight"&gt;not-any?&lt;/code&gt;) divisors &lt;span class="math"&gt;\(1 &amp;lt; d &amp;lt; n\)&lt;/span&gt;, we just filter all divisors &lt;span class="math"&gt;\(1 \leq d &amp;lt; n\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;brute-force-factors&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;partial &lt;/span&gt;&lt;span class="nv"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can imagine, since this is running through every possible number, this will take way too long for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;. We can use the same upper bound as before, although that means that for every positive result, we have to add not only &lt;span class="math"&gt;\(d\)&lt;/span&gt;, but &lt;span class="math"&gt;\(\frac{n}{d}\)&lt;/span&gt; as well.&lt;/p&gt;
&lt;p&gt;Here is a clean way of doing that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;factors&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;-&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;inc &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;isqrt&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
       &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;%&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
       &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;mapcat &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;d&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;d&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)]))&lt;/span&gt;
       &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;into &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sorted-set&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="more-optimized-prime-factorization"&gt;More optimized prime factorization&lt;/h2&gt;
&lt;p&gt;With this, we can now improve the performance of our prime factorization function by checking the size of the number first. If &lt;span class="math"&gt;\(n\)&lt;/span&gt; is smaller than some threshold (let’s say 1 million), then iterate up an infinite list of primes as before. If &lt;span class="math"&gt;\(n\)&lt;/span&gt; is larger than the threshold, though, then &lt;code class="highlight"&gt;filter&lt;/code&gt; the &lt;code class="highlight"&gt;prime?&lt;/code&gt; &lt;code class="highlight"&gt;factors&lt;/code&gt; first, and iterate up &lt;em&gt;that&lt;/em&gt; list instead.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;prime-factorization&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="s"&gt;"Returns the prime factorization of an integer, e.g., 168 -&amp;gt; [2 2 2 3 7]."&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;when &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;and &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;integer?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;pos? &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c1"&gt;;; Sanity check&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;loop &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;       &lt;span class="nv"&gt;n&lt;/span&gt;
           &lt;span class="nv"&gt;primes&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;&amp;gt; &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;factors&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
           &lt;span class="nv"&gt;factors&lt;/span&gt; &lt;span class="p"&gt;[]]&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;k&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;first &lt;/span&gt;&lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;cond&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;          &lt;span class="nv"&gt;factors&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;divisible?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;primes&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;conj &lt;/span&gt;&lt;span class="nv"&gt;factors&lt;/span&gt; &lt;span class="nv"&gt;k&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
          &lt;span class="ss"&gt;:else&lt;/span&gt;            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;recur&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;rest &lt;/span&gt;&lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;factors&lt;/span&gt;&lt;span class="p"&gt;))))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This isn’t a perfect heuristic, as large numbers do not necessarily have large prime factors. For example, 12 bazillion-gajillion (&lt;span class="math"&gt;\(12 \times 10^{??}\)&lt;/span&gt;) still has only &lt;span class="math"&gt;\((2, 3, 5)\)&lt;/span&gt; as prime factors, just like &lt;span class="math"&gt;\(120\)&lt;/span&gt;. But at least it can reduce the time required for difficult cases, such as &lt;span class="math"&gt;\(n = 23897538974893789\)&lt;/span&gt;, and make them tractable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;time &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-factorization&lt;/span&gt; &lt;span class="mi"&gt;23897538974893789&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;;; Elapsed time: 30579.928312 msecs&lt;/span&gt;
&lt;span class="c1"&gt;;; [211 23357 4849016507]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(The brute-force version didn’t finish even after 5 minutes, so I abandoned it.)&lt;/p&gt;
&lt;h2 id="prime-omega-functions"&gt;Prime omega functions&lt;/h2&gt;
&lt;p&gt;There are a couple arithmetic that look and sound really impressive but are trivial to implement in code once you can factor an integer into primes.&lt;/p&gt;
&lt;p&gt;One is the little omega function &lt;span class="math"&gt;\(\omega(n)\)&lt;/span&gt;, which counts the number of distinct prime factors of &lt;span class="math"&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;distinct &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-factorization&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The other is the big omega function &lt;span class="math"&gt;\(\Omega(n)\)&lt;/span&gt;, which counts the number of prime factors of &lt;span class="math"&gt;\(n\)&lt;/span&gt; with multiplicity (i.e., if &lt;span class="math"&gt;\(2\)&lt;/span&gt; appears more than once, count both instances).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-factorization&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="prime-power-representation"&gt;Prime power representation&lt;/h1&gt;
&lt;p&gt;Given the prime factorization of a number, we can use this information to formulate its unique representation as a product of prime powers.&lt;/p&gt;
&lt;div class="math"&gt;$$ n = p_{1}^{e_{1}}p_{2}^{e_{2}}\cdots p_{k}^{e_{k}}
     = \prod_{i=1}^{k}p_{i}^{e_{i}} $$&lt;/div&gt;
&lt;p&gt;Returning to our simple example &lt;span class="math"&gt;\(168 = 2 \times 2 \times 2 \times 3 \times 7\)&lt;/span&gt; above, this can be written more succinctly as&lt;/p&gt;
&lt;div class="math"&gt;$$ 168 = 2^3 \times 3 \times 7 $$&lt;/div&gt;
&lt;p&gt;which is also called its &lt;strong&gt;canonical representation&lt;/strong&gt; or &lt;strong&gt;standard form&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If we fill in the missing primes above with &lt;span class="math"&gt;\(p_{i}^{0}\)&lt;/span&gt;, which doesn’t affect the final product, then the sequence of &lt;span class="math"&gt;\(e_1 \cdots e_k\)&lt;/span&gt; in the above notation can also be extracted from the prime factorization, which gives us an &lt;strong&gt;exponent vector&lt;/strong&gt; (or &lt;em&gt;unique prime signature&lt;/em&gt;). For &lt;span class="math"&gt;\(168\)&lt;/span&gt;, this would be&lt;/p&gt;
&lt;div class="math"&gt;$$ 168 = 2^{\color{red}{3}} \times 3^{\color{red}{1}} \times 5^{\color{red}{0}} \times 7^{\color{red}{1}}
       = \begin{bmatrix}3 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Here’s a simple way to implement that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;prime-powers-integer&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="s"&gt;"Returns the exponent vector for the prime power representation of an integer,&lt;/span&gt;
&lt;span class="s"&gt;  e.g., 168 = 2*2*2*3*7 = 2^3 * 3^1 * 5^0 * 7^1 -&amp;gt; (3 1 0 1)"&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;when &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;and &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;integer?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;pos? &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;pf&lt;/span&gt;      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-factorization&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nv"&gt;primes&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;inc &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;peek &lt;/span&gt;&lt;span class="nv"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
            &lt;span class="nv"&gt;freqs&lt;/span&gt;   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;frequencies&lt;/span&gt; &lt;span class="nv"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nv"&gt;get-exp&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;p&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;if-let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;exp&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;get &lt;/span&gt;&lt;span class="nv"&gt;freqs&lt;/span&gt; &lt;span class="nv"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
                              &lt;span class="nv"&gt;exp&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;get-exp&lt;/span&gt; &lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n = 168\)&lt;/span&gt;, the procedure works like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;pf&lt;/span&gt; &lt;span class="nv"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;primes&lt;/span&gt; &lt;span class="nv"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;;; one of each prime up to the last prime in pf&lt;/span&gt;
&lt;span class="nv"&gt;freqs&lt;/span&gt; &lt;span class="nv"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;, &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;, &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;get-exp&lt;/span&gt; &lt;span class="nv"&gt;primes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;get-exp&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="nv"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This works fine when the prime factors are small, but it isn’t the cleanest approach. Namely, one list of primes may already be generated by &lt;code class="highlight"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-factorization&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt;, so generating another (for the purpose of determining that &lt;span class="math"&gt;\(87\)&lt;/span&gt; is the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th prime, for example) is not very efficient. If we limited the &lt;code class="highlight"&gt;prime-factorization&lt;/code&gt; function to the iterative approach, we could generate a map containing &lt;code class="highlight"&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:i&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="ss"&gt;:p&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="ss"&gt;:e&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;/code&gt; for each prime, which is a starting point for generating both the canonical representation and the exponent vector.&lt;/p&gt;
&lt;p&gt;However, since this is mostly for curiosity’s sake than actual applications, I’m leaving it at that. For cryptographic applications, what is more important than generating exponent vectors are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Finding the factors (prime or otherwise) of an integer themselves, and&lt;/li&gt;
&lt;li&gt;Specialized techniques for factoring specific types of numbers, such as semiprimes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, for completeness’ sake, let’s write a function to convert an exponent vector back to an integer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;prime-powers-&amp;gt;num&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;pp&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;primes&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;prime?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;*&lt;/span&gt;&lt;span class="o"&gt;'&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;tower/expt&lt;/span&gt; &lt;span class="nv"&gt;primes&lt;/span&gt; &lt;span class="nv"&gt;pp&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-&amp;gt;num&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c1"&gt;;; 168&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="negative-exponents-too"&gt;Negative exponents too?&lt;/h2&gt;
&lt;p&gt;While writing this, I learned that allowing negative exponents in prime factorizations enables you to represent not only all the integers (&lt;span class="math"&gt;\(n \in ℤ\)&lt;/span&gt;), but all the rationals (&lt;span class="math"&gt;\(n \in ℚ\)&lt;/span&gt;) as well.&lt;/p&gt;
&lt;p&gt;For some rational number &lt;span class="math"&gt;\(q = \frac{a}{b}\)&lt;/span&gt;, let &lt;span class="math"&gt;\(\vec a, \vec b\)&lt;/span&gt; be the exponent vectors for the integers &lt;span class="math"&gt;\(a, b\)&lt;/span&gt;. The exponent vector for &lt;span class="math"&gt;\(q\)&lt;/span&gt; is then &lt;span class="math"&gt;\(\vec q = \vec a - \vec b\)&lt;/span&gt;. For example,&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{gathered}
q = \frac{171}{98} \\[0.8em]
\begin{aligned}
a = 171 = 3^2 \times 19^1 \\
b = 98 = 2^1 \times 7^2 \\[0.8em]
\end{aligned} \\
\begin{aligned}
\vec a &amp;amp;= \begin{bmatrix}\; \; \; 0 &amp;amp; 2 &amp;amp; 0 &amp;amp; \; \; \; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\end{bmatrix} \\
\vec b &amp;amp;= \begin{bmatrix}\; \; \; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \; \; \; 2 &amp;amp; \textcolor{#bbb}{0} &amp;amp; \textcolor{#bbb}{0} &amp;amp; \textcolor{#bbb}{0} &amp;amp; \textcolor{#bbb}{0}\end{bmatrix} \\
\vec q &amp;amp;= \begin{bmatrix}-1 &amp;amp; 2 &amp;amp; 0 &amp;amp; -2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\end{bmatrix} \\[0.8em]
\end{aligned} \\
\begin{aligned}
q &amp;amp;= 2^{-1} \times 3^2 \times 7^{-2} \times 19^1 \\
  &amp;amp;= \frac{1}{2} \times 9 \times \frac{1}{49} \times 19
\end{aligned}
\end{gathered}$$&lt;/div&gt;
&lt;p&gt;The following code is something I scratched up quickly as Clojure does not have a simple way to implement something like &lt;code class="highlight"&gt;map-longest&lt;/code&gt;. That is, a way to map over multiple collections, filling in dummy values to make each collection the same size as the largest—in this case, the light gray &lt;span class="math"&gt;\(\textcolor{#bbb}{0}\)&lt;/span&gt; in &lt;span class="math"&gt;\(\vec b\)&lt;/span&gt; above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;prime-powers-rational&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;r&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;rationalize&lt;/span&gt; &lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="c1"&gt;;; Sanity check to accept decimal representations of rational numbers&lt;/span&gt;
    &lt;span class="c1"&gt;;; while still rejecting irrational numbers&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;when &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;or &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;ratio?&lt;/span&gt; &lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;= &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;double &lt;/span&gt;&lt;span class="nv"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;double &lt;/span&gt;&lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-integer&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;numerator&lt;/span&gt; &lt;span class="nv"&gt;r&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nv"&gt;d&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-integer&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;denominator&lt;/span&gt; &lt;span class="nv"&gt;r&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nv"&gt;pn&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;concat &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;repeat &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count &lt;/span&gt;&lt;span class="nv"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nv"&gt;pd&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;concat &lt;/span&gt;&lt;span class="nv"&gt;d&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;repeat &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;-&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map - &lt;/span&gt;&lt;span class="nv"&gt;pn&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
             &lt;span class="nv"&gt;reverse&lt;/span&gt;
             &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;drop-while &lt;/span&gt;&lt;span class="nv"&gt;zero?&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;;; truncate zeros from the end&lt;/span&gt;
             &lt;span class="nv"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;)))))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-rational&lt;/span&gt; &lt;span class="mi"&gt;171&lt;/span&gt;&lt;span class="nv"&gt;/98&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;;; (-1 2 0 -2 0 0 0 1)&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-&amp;gt;num&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;-1&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;-2&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c1"&gt;;; 171/98&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pretty cool!&lt;/p&gt;
&lt;p&gt;While the rational version doesn’t reside strictly within the confines of number theory, as it goes beyond the integers, it is still mathematically interesting and can be wrapped up neatly in a bulletproof &lt;code class="highlight"&gt;prime-powers&lt;/code&gt; function that can handle both integer and rational inputs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;prime-powers&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;when &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;pos? &lt;/span&gt;&lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;integer?&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-integer&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;prime-powers-rational&lt;/span&gt; &lt;span class="nv"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Apparently, this can be &lt;a href="https://math.stackexchange.com/questions/873455/factorization-of-rational-powers-of-rational-numbers"&gt;extended to rational powers of rational numbers&lt;/a&gt; as well, which would seem to allow irrational numbers to be represented as well, though that is starting to get a little deep for me 😅&lt;/p&gt;
&lt;p&gt;That’s all for now. Stay tuned for more posts in this series!&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic"&gt;Fundamental theorem of arithmetic&lt;/a&gt;, Wikipedia&lt;/li&gt;
&lt;li&gt;&lt;a href="https://oeis.org/wiki/Prime_factorization"&gt;Prime factorization&lt;/a&gt;, &lt;span class="caps"&gt;OEIS&lt;/span&gt; Wiki&lt;/li&gt;
&lt;/ul&gt;</content><category term="number theory"></category><category term="math"></category><category term="clojure"></category></entry><entry><title>The mod(ular arithmetic) squad</title><link href="http://tabidots.github.io/2019/02/mod-squad" rel="alternate"></link><published>2019-02-01T07:35:36+07:00</published><updated>2019-02-01T07:35:36+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-02-01:/2019/02/mod-squad</id><summary type="html">&lt;p&gt;Modular arithmetic is weird, cool, and generates some trippy&amp;nbsp;polynomials!&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Finally, I’m on the last of the exercises from Chapter 2 of &lt;em&gt;A Programmer’s Introduction to Mathematics&lt;/em&gt;. The exercise prompts are deceptively terse, and their connection to the material presented in the chapter is not always immediately obvious. Working through them can take entire days (for me, anyway).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write a web app that implements the distribution and reconstruction of the secret sharing protocol using the polynomial interpolation algorithm presented in this chapter, using modular arithmetic modulo and a 32-bit modulus &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To be honest, it took many hours before I could even understand what I was actually being asked to do. I understood&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;part of the &lt;em&gt;what&lt;/em&gt;: the concept of modulo, and&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;why&lt;/em&gt;: modular arithmetic avoids floating-point rounding errors that emerge in the &lt;span class="math"&gt;\(\frac{x - x_j}{x_i - x_j}\)&lt;/span&gt; operation of interpolating polynomials&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But I was clueless about the rest—namely&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how modular arithmetic relates to polynomials; do you perform &lt;span class="math"&gt;\(\mod n\)&lt;/span&gt; on all the coefficients?&lt;/li&gt;
&lt;li&gt;how does division (fractional quantities) even work in modular arithmetic?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At first, I spent way too much time going down fruitless dead-ends. I tried adding in &lt;code class="highlight"&gt;mod&lt;/code&gt; in random places, but I still got floating-point rounding errors.&lt;/p&gt;
&lt;h1 id="multiplicative-inverse"&gt;Multiplicative inverse&lt;/h1&gt;
&lt;p&gt;Eventually, I found my way to &lt;a href="https://math.stackexchange.com/a/2924485"&gt;the key insight that got things moving&lt;/a&gt;: There is no “division” as such in modular arithmetic. Instead, there is the &lt;em&gt;multiplicative inverse&lt;/em&gt;, which is analogous in a somewhat non-obvious way to regular division.&lt;/p&gt;
&lt;p&gt;In normal arithmetic, the inverse of a number (let’s say &lt;span class="math"&gt;\(b\)&lt;/span&gt;) is the entity that, when multiplied by &lt;span class="math"&gt;\(b\)&lt;/span&gt;, gives &lt;span class="math"&gt;\(1\)&lt;/span&gt;. That will be the reciprocal, and if &lt;span class="math"&gt;\(b\)&lt;/span&gt; is a whole number, then the inverse of &lt;span class="math"&gt;\(b\)&lt;/span&gt; can be written &lt;span class="math"&gt;\(b^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
b \cdot \textrm{inv} &amp;amp;= 1 \\
\textrm{inv} &amp;amp;= \frac{1}{b} \\
&amp;amp;= b^{-1}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;In modular arithmetic, the multiplicative inverse of a number &lt;span class="math"&gt;\(b\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mod n\)&lt;/span&gt; is also written &lt;span class="math"&gt;\(b^{-1}\)&lt;/span&gt;, and it is the integer that, when multiplied by &lt;span class="math"&gt;\(b\)&lt;/span&gt;, gives &lt;span class="math"&gt;\(1 \mod n\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ b \cdot b^{-1} \equiv 1 \mod n $$&lt;/div&gt;
&lt;p&gt;Now, finding the inverse is not exactly straightforward. There is an algorithm, but let’s first explore some examples. The StackExchange answer linked above enumerates the multiplicative inverses of numbers in &lt;span class="math"&gt;\(\mod 11\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
1^{-1} &amp;amp;= 1 &amp;amp; 1 \cdot \textcolor{red}{1} &amp;amp;= 1 &amp;amp;\equiv 1 \mod 11 \\
2^{-1} &amp;amp;= 6 &amp;amp; 2 \cdot \textcolor{red}{6} &amp;amp;= 12 &amp;amp;\equiv 1 \mod 11 \\
3^{-1} &amp;amp;= 4 &amp;amp; 3 \cdot \textcolor{red}{4} &amp;amp;= 12 &amp;amp;\equiv 1 \mod 11 \\
4^{-1} &amp;amp;= 3 &amp;amp; 4 \cdot \textcolor{red}{3} &amp;amp;= 12 &amp;amp;\equiv 1 \mod 11 \\
5^{-1} &amp;amp;= 9 &amp;amp; 5 \cdot \textcolor{red}{9} &amp;amp;= 45 &amp;amp;\equiv 1 \mod 11 \\
6^{-1} &amp;amp;= 2 &amp;amp; 6 \cdot \textcolor{red}{2} &amp;amp;= 12 &amp;amp;\equiv 1 \mod 11 \\
7^{-1} &amp;amp;= 8 &amp;amp; 7 \cdot \textcolor{red}{8} &amp;amp;= 56 &amp;amp;\equiv 1 \mod 11 \\
8^{-1} &amp;amp;= 7 &amp;amp; 8 \cdot \textcolor{red}{7} &amp;amp;= 56 &amp;amp;\equiv 1 \mod 11 \\
9^{-1} &amp;amp;= 5 &amp;amp; 9 \cdot \textcolor{red}{5} &amp;amp;= 45  &amp;amp;\equiv 1 \mod 11 \\
10^{-1} &amp;amp;= 10 &amp;amp; 10 \cdot \textcolor{red}{10} &amp;amp;= 100 &amp;amp;\equiv 1 \mod 11
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Back to division. In normal arithmetic, we can use the property of the inverse to rewrite fractions in a slightly awkward way: &lt;span class="math"&gt;\(\frac{a}{b} = a \cdot \frac{1}{b} = a \cdot b^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By this logic, “division” can be performed in modular arithmetic by multiplying &lt;span class="math"&gt;\(a \cdot b^{-1} \mod n\)&lt;/span&gt; and reducing the answer you get &lt;span class="math"&gt;\(\mod n\)&lt;/span&gt;. For example, &lt;span class="math"&gt;\(\frac{7}{6}\)&lt;/span&gt; would be obtained in &lt;span class="math"&gt;\(\mod 11\)&lt;/span&gt; by:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
7 \cdot 6^{-1} &amp;amp;= 7 \cdot 2 \\ &amp;amp;= 14 \\ &amp;amp;\equiv 3 \mod 11
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Interestingly, for prime moduli, there is &lt;a href="https://stackoverflow.com/a/4798776/4210855"&gt;a very quick and easy way to do it natively in Python&lt;/a&gt;, which is much faster and simpler than the full algorithm for arbitrary moduli:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mod_inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Same as x ** mod-2 % mod&lt;/span&gt;

&lt;span class="n"&gt;mod_inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I don’t really understand the &lt;code class="highlight"&gt;-2&lt;/code&gt; part, but it seems that in &lt;span class="math"&gt;\(\mod 11\)&lt;/span&gt;, any number raised to the &lt;em&gt;ninth power&lt;/em&gt; will produce a number that is one more than a multiple of 11.&lt;/p&gt;
&lt;h1 id="polynomial-interpolation-modularly"&gt;Polynomial interpolation, modularly&lt;/h1&gt;
&lt;p&gt;Now, to apply this finding to polynomial interpolation. This was the original, non-modular, non-cool version:&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^n y_i \Bigg(\prod_{j \not= i} \frac{x - x_j}{x_i - x_j}\Bigg) $$&lt;/div&gt;
&lt;p&gt;To make it modular and cool, we do this:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
f(x) &amp;amp;= \sum_{i=0}^n y_i \Bigg(\prod_{j \not= i} \textcolor{teal}{\frac{1}{x_i - x_j}}x + \textcolor{orange}{\frac{-x_j}{x_i - x_j}}\Bigg) \\
&amp;amp;= \sum_{i=0}^n y_i \Bigg(\prod_{j \not= i} \textcolor{lightgray}{1 \cdot }\textcolor{teal}{(x_i - x_j)^{-1}} \cdot x + \textcolor{orange}{-x_j \cdot (x_i - x_j)^{-1}} \Bigg) \mod n
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Keeping in mind that the equivlent of modular &lt;code class="highlight"&gt;a / b&lt;/code&gt; in Python is &lt;code class="highlight"&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mod_inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;mod&lt;/span&gt;&lt;/code&gt;, this is surprisingly easy to implement in the author’s &lt;code class="highlight"&gt;single_term&lt;/code&gt; function (original code is &lt;a href="https://github.com/pim-book/programmers-introduction-to-mathematics/blob/master/secret-sharing/interpolate.py"&gt;here&lt;/a&gt;). Let’s assume a global variable &lt;code class="highlight"&gt;MOD&lt;/code&gt; that we’ll create later.&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_1" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mod_inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;mod_inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The coefficients of the polynomial generated by the &lt;code class="highlight"&gt;interpolate&lt;/code&gt; function must also be reduced. Code-wise, that’s pretty easy:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;terms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ZERO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_1" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;terms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ZERO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Except for one thing—the original class made no provision for the &lt;code class="highlight"&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;/code&gt; operator. So we have to add that to the class (original code is &lt;a href="https://github.com/pim-book/programmers-introduction-to-mathematics/blob/master/secret-sharing/polynomial.py"&gt;here&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__mod__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, the value returned by &lt;code class="highlight"&gt;evaluateAt&lt;/code&gt; must also be reduced &lt;span class="math"&gt;\(\mod n\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;theSum&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_3_1" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;theSum&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;MOD&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For some reason, the &lt;code class="highlight"&gt;ZERO&lt;/code&gt; polynomial has to be changed slightly:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_4_0" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ZERO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_4_1" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ZERO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="making-floating-points-sink"&gt;Making floating points sink&lt;/h1&gt;
&lt;p&gt;Modular arithmetic only deals with integers, or &lt;code class="highlight"&gt;int&lt;/code&gt;, so care must be taken to avoid any accidental coercion to &lt;code class="highlight"&gt;float&lt;/code&gt;s in the course of the code. Otherwise, that defeats the purpose of going through all this trouble to avoid floating-point rounding errors.&lt;/p&gt;
&lt;p&gt;The initial value of each single term in the &lt;code class="highlight"&gt;single_term&lt;/code&gt; function is a &lt;code class="highlight"&gt;float&lt;/code&gt;, so let’s change that:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_5_0" name="__tabs_5" type="radio"/&gt;
&lt;label for="__tab_5_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_5_1" name="__tabs_5" type="radio"/&gt;
&lt;label for="__tab_5_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Summing a list of &lt;code class="highlight"&gt;Polynomial&lt;/code&gt;s (or even just adding two together) produced floating-point values even when the original coefficients were integers. It turns out that the author explicitly coerced the value in the function &lt;code class="highlight"&gt;add&lt;/code&gt; of the &lt;code class="highlight"&gt;Polynomial&lt;/code&gt; class (which was loaded into the operator via &lt;code class="highlight"&gt;__add__&lt;/code&gt;). Another easy fix:&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_6_0" name="__tabs_6" type="radio"/&gt;
&lt;label for="__tab_6_0"&gt;Broke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;zip_longest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fillvalue&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_6_1" name="__tabs_6" type="radio"/&gt;
&lt;label for="__tab_6_1"&gt;Woke&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;zip_longest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fillvalue&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="modulus-prime"&gt;Modulus Prime&lt;/h1&gt;
&lt;p&gt;Man, the word &lt;em&gt;modulus&lt;/em&gt; is such a cool word. Anyway, we need to create one.&lt;/p&gt;
&lt;p&gt;The prompt in the book specifies the use of a 32-bit number. It just so happens that Python has a handy random number generator, &lt;code class="highlight"&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getrandbits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/code&gt;, that takes &lt;code class="highlight"&gt;x&lt;/code&gt; bits as an argument.&lt;/p&gt;
&lt;p&gt;Since our simplified algorithm for modular exponentiation assumes that the modulus is prime, we also have to find some way to check for primality. &lt;code class="highlight"&gt;sympy&lt;/code&gt; has one such function; no need to reinvent the wheel. (While I am finding number theory very fascinating, I also don’t have 5 years to spend on one chapter of this book!)&lt;/p&gt;
&lt;p&gt;So, with that information, it should be pretty easy to come up with a random 32-bit prime modulus:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sympy.ntheory.primetest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;isprime&lt;/span&gt;

&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;big_prime&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getrandbits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# a random 32-bit number&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;isprime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;

&lt;span class="n"&gt;MOD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;big_prime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This instantly gives us gigantic numbers like &lt;code class="highlight"&gt;3898342621&lt;/code&gt; that automatically satisfy two conditions: (1) Occupy 32 bits and (2) Be prime. Perfect!&lt;/p&gt;
&lt;h1 id="the-proof-is-in-the-polynomials"&gt;The proof is in the polynomials&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MOD&lt;/span&gt; &lt;span class="c1"&gt;# 2606193617&lt;/span&gt;
&lt;span class="n"&gt;points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;interpolate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 347492486 + 2084954895 x^1 + 173746241 x^2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# [5, 6, 7]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It works! Our interpolated polynomial is&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = 173746241 x^2 + 2084954895 x + 347492486 \mod 2606193617 $$&lt;/div&gt;
&lt;p&gt;and despite the enormous coefficients, it does actually pass through the points I specified.&lt;/p&gt;
&lt;p&gt;It should be noted that this code &lt;em&gt;does not&lt;/em&gt; work for negative numbers, as someone commented in response to the quick-and-dirty modular exponentation function I found on Stack Exchange. You can make modular arithmetic work with negative numbers, but it takes a little more fiddling.&lt;/p&gt;
&lt;p&gt;You could also just take the lazy route and not use negative &lt;span class="math"&gt;\(y\)&lt;/span&gt; values when implementing this for secret-sharing.&lt;/p&gt;
&lt;p&gt;Just out of curiosity, what does this modular polynomial look like, compared to the non-modular one?&lt;/p&gt;
&lt;p&gt;&lt;img alt="polynomial interpolation" src="../../images/polynomial_interp.png"/&gt;&lt;/p&gt;
&lt;p&gt;The modular polynomial has an interesting shape! Note that the scale on the vertical axis is &lt;code class="highlight"&gt;1e9&lt;/code&gt;, or &lt;em&gt;one billion&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I find it so bizarre and fascinating that the floating-point coefficients are off by such a small amount, yet result in an inaccuracy that would make the result completely useless for cryptography, while the modular coefficients are so gigantic (along with the fluctuations in the graph) yet do create a polynomial that passes through the given points.&lt;/p&gt;
&lt;p&gt;To illustrate just how small the floating-point errors are, here is a comparison between the floating-point and decimal version of the hand-interpolated polynomial:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{gathered}
-0.1\overline{3} x^2 + 1.4 x + 3.7\overline{3}  \\
-0.13333333333333358 x^2 + 1.3999999999999995 x + 3.733333333333336
\end{gathered}$$&lt;/div&gt;
&lt;p&gt;That’s enough for this post. I’ll get on implementing this into an actual toy-cryptography web app soon!&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://math.stackexchange.com/questions/2922433/how-to-compute-prod-i-1ny-i-left-prod-genfrac01j-not-ij-1"&gt;How to compute &lt;span class="math"&gt;\(\prod_{i=1}^n y'{_i}^{\big(\prod_{j \not=i, j=1}^n \frac{x_j}{x_j-x_i}\big)}\)&lt;/span&gt; with modular arithmetic for Lagrange&lt;/a&gt;, Stack Exchange Mathematics&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/4798776/4210855"&gt;Modular multiplicative inverse function in Python&lt;/a&gt;, Stack Overflow&lt;/li&gt;
&lt;/ul&gt;</content><category term="modular arithmetic"></category></entry><entry><title>PIM notes, Chapter 2: Exercises</title><link href="http://tabidots.github.io/2019/01/pim-notes-chapter-2-exercises" rel="alternate"></link><published>2019-01-25T16:55:04+07:00</published><updated>2019-01-25T16:55:04+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-25:/2019/01/pim-notes-chapter-2-exercises</id><summary type="html">&lt;p&gt;Exercises from Chapter 2 of &lt;span class="caps"&gt;PIM&lt;/span&gt;, minus the coding&amp;nbsp;projects.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;These exercises exclude the coding projects, which I will write about later and post on GitHub.&lt;/p&gt;
&lt;p&gt;Answering these has been very tedious. The first few were easy, but they quickly ballooned in difficulty / time required and so they got put on the back burner for a bit. I hope this book becomes more enjoyable.&lt;/p&gt;
&lt;h1 id="21"&gt;2.1&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The highest-degree term in &lt;span class="math"&gt;\(f\)&lt;/span&gt; is &lt;span class="math"&gt;\(x^2\)&lt;/span&gt;; the highest-degree term in &lt;span class="math"&gt;\(g\)&lt;/span&gt; is &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The highest possible power of &lt;span class="math"&gt;\(x\)&lt;/span&gt; that can occur in &lt;span class="math"&gt;\(f \cdot g\)&lt;/span&gt; is &lt;span class="math"&gt;\(x^2 \cdot x\)&lt;/span&gt;, or &lt;span class="math"&gt;\(x^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The highest-degree term in &lt;span class="math"&gt;\(f\)&lt;/span&gt; is &lt;span class="math"&gt;\(x^n\)&lt;/span&gt;; the highest-degree term in &lt;span class="math"&gt;\(g\)&lt;/span&gt; is &lt;span class="math"&gt;\(m\)&lt;/span&gt;. The highest possible power of &lt;span class="math"&gt;\(x\)&lt;/span&gt; that can occur in &lt;span class="math"&gt;\(f \cdot g\)&lt;/span&gt; is &lt;span class="math"&gt;\(x^n \cdot x^m = x^{n + m}\)&lt;/span&gt;, resulting in a polynomial of degree &lt;span class="math"&gt;\(n+m\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This does not hold when &lt;span class="math"&gt;\(f\)&lt;/span&gt; or &lt;span class="math"&gt;\(g\)&lt;/span&gt; are the zero polynomial. The generalization could be changed to say that &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(g\)&lt;/span&gt; are polynomials with degrees &lt;span class="math"&gt;\(n\)&lt;/span&gt; and &lt;span class="math"&gt;\(m\)&lt;/span&gt; where &lt;span class="math"&gt;\(n \geq 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(m \geq 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="22"&gt;2.2&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(n = 24\)&lt;/span&gt;, then the relative prime numbers of &lt;span class="math"&gt;\(n\)&lt;/span&gt; are &lt;span class="math"&gt;\(5, 7, 11, 13, 17, 19, 23\)&lt;/span&gt;, which means &lt;span class="math"&gt;\(\phi(n) = 8\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One example of a monic polynomial is &lt;span class="math"&gt;\(f(x) = x^5 + 3x^4 + 12x^3 + x + 7\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(f(x) = 6x^3 + 18x^2 - 3x\)&lt;/span&gt;, a &lt;span class="math"&gt;\(3x\)&lt;/span&gt; can be factored out of each term, leaving the factors &lt;span class="math"&gt;\(g(x) = 2x^3 + 6x^2 - 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(h(x) = 3x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(x) = 17x^3 - 2\)&lt;/span&gt; and &lt;span class="math"&gt;\(g(x) = 6x^2\)&lt;/span&gt; are relatively prime polynomials. &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; is also irreducible. For the functions &lt;span class="math"&gt;\(f(x) = 2x^2 + 4\)&lt;/span&gt; and &lt;span class="math"&gt;\(g(x) = 4x^2 + 8\)&lt;/span&gt;, their greatest common divisor, since it must be monic, is &lt;span class="math"&gt;\((j)x = x^2 + 4\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="23"&gt;2.3&lt;/h1&gt;
&lt;p&gt;According to Euler’s theorem, &lt;span class="math"&gt;\(a^{\varphi(n)}\over{n}\)&lt;/span&gt; has remainder &lt;span class="math"&gt;\(1\)&lt;/span&gt; (or &lt;span class="math"&gt;\(a^{\varphi(n)}\)&lt;/span&gt; &lt;code class="highlight"&gt;mod&lt;/code&gt; &lt;span class="math"&gt;\(n = 1\)&lt;/span&gt;). This means that
    &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
    a^{\varphi(n)} \bmod n &amp;amp;= 1 \\
    \frac{a^{\varphi(n)}}{n} &amp;amp;= c + \frac{1}{n} \\
    a^{\varphi(n)} &amp;amp;= cn + 1 \\
    \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(c\)&lt;/span&gt; must be a nonnegative integer, which means that &lt;span class="math"&gt;\(\frac{a^{\varphi(n)} - 1}{n}\)&lt;/span&gt; must be a nonnegative integer. Using the numbers from the previous example (&lt;span class="math"&gt;\(n = 24\)&lt;/span&gt;, &lt;span class="math"&gt;\(\varphi(n) = 8\)&lt;/span&gt;), we can let &lt;span class="math"&gt;\(a = 5\)&lt;/span&gt;.
    &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
    \frac{5^{8}}{24} &amp;amp;= c + \frac{1}{24} \\
    5^{8} &amp;amp;= 24c + 1 \\
    390625 - 1 &amp;amp;= 24c \\
    c &amp;amp;= 16276
    \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
  Indeed, &lt;span class="math"&gt;\(\frac{390625 - 1}{24}\)&lt;/span&gt; is a nonnegative integer.&lt;/p&gt;
&lt;h1 id="24"&gt;2.4&lt;/h1&gt;
&lt;p&gt;Based on the definition, a number &lt;span class="math"&gt;\(z\)&lt;/span&gt; is algebraic if there is some polynomial function &lt;span class="math"&gt;\(f(x) = a_0 + a_1x + \cdots + a_nx^n\)&lt;/span&gt;, where all &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; are rational, such that &lt;span class="math"&gt;\(f(z) = 0\)&lt;/span&gt;. &lt;span class="math"&gt;\(\sqrt 2\)&lt;/span&gt; is algebraic because it is the root of &lt;span class="math"&gt;\(x^2 - 2\)&lt;/span&gt;, which has the rational coefficients &lt;span class="math"&gt;\(a_0 = -2\)&lt;/span&gt;, &lt;span class="math"&gt;\(a_1 = 0\)&lt;/span&gt;, and &lt;span class="math"&gt;\(a_2 = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You can find this by trying to get to &lt;span class="math"&gt;\(z\)&lt;/span&gt; from 0, then going backwards through that order of operations starting with &lt;span class="math"&gt;\(x\)&lt;/span&gt;. I find this easier to visualize by writing in Clojure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sqrt&lt;/span&gt;         &lt;span class="c1"&gt;; x^2&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;    &lt;span class="c1"&gt;; - 2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What about &lt;span class="math"&gt;\(\phi = \frac{1 + \sqrt 5}{2}\)&lt;/span&gt;?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;/ &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;              &lt;span class="c1"&gt;; 2x&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;            &lt;span class="c1"&gt;; - 1&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sqrt&lt;/span&gt;         &lt;span class="c1"&gt;; all of that ^2&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;  &lt;span class="c1"&gt;; - 5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
f(x) &amp;amp;= (2x - 1)^2 - 5 \\
&amp;amp;= 2x^2 - 4x + 1 - 5 \\
&amp;amp;= 2x^2 - 4x - 4
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;So, we have a polynomial. Since we started from 0 and worked in reverse to come up with this polynomial, that should mean &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is algebraic. Let’s check:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
f(\phi) &amp;amp;= \Big(\cancel{2}(\frac{1 + \sqrt 5}{\cancel{2}}) - 1\Big)^2 - 5 \\
&amp;amp;= (\cancel{1} + \sqrt 5 - \cancel{1})^2 - 5 \\
&amp;amp;= (\sqrt 5)^2 - 5 \\
&amp;amp;= 0
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;And &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is its root, so yes, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is an algebraic number. Now, how about &lt;span class="math"&gt;\(\sqrt 2 + \sqrt 3\)&lt;/span&gt;? At first I tried to work it out with Clojure and I got the wrong answer. But there is definitely a way to make &lt;span class="math"&gt;\(\sqrt 2 + \sqrt 3 = 0\)&lt;/span&gt;. I did it the old-fashioned way, filling up an entire sheet of paper with algebra.&lt;/p&gt;
&lt;p&gt;You have to square the term to get rid of the square roots, but because &lt;span class="math"&gt;\((a + b)^2 = a^2 + 2ab + b^2\)&lt;/span&gt;, you’ll also have to get rid of the &lt;span class="math"&gt;\(2\sqrt 2\sqrt 3\)&lt;/span&gt; that remains when you do &lt;span class="math"&gt;\((\sqrt 2 + \sqrt 3)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
f(x) &amp;amp;= \Big(\frac{x^2 - 5}{2}\Big)^2 - 6 \\
&amp;amp;= \frac{1}{4}x^4 + \frac{5}{2}x^2 + \frac{25}{4} - 6 \\
&amp;amp;= \frac{1}{4}x^4 + \frac{5}{2}x^2 + \frac{1}{4}
\end{aligned} $$&lt;/div&gt;
&lt;h1 id="25"&gt;2.5&lt;/h1&gt;
&lt;h2 id="product-and-sum-of-algebraic-numbers"&gt;Product and sum of algebraic numbers&lt;/h2&gt;
&lt;p&gt;A polynomial encompasses the operations of addition, multiplication, and exponentiation. To find the root of a polynomial is to perform the ”opposite” of these operations.&lt;/p&gt;
&lt;p&gt;Addition and multiplication are commutative, so their opposite is themselves; the opposite of exponentiation is to take a root.&lt;/p&gt;
&lt;p&gt;It would seem from the above work that any number made from combinations of these “opposite” operations performed on rational numbers is the root of &lt;em&gt;some&lt;/em&gt; polynomial and is therefore algebraic.&lt;/p&gt;
&lt;p&gt;Therefore, for any two algebraic numbers, their sum and their product are both algebraic as well.&lt;/p&gt;
&lt;h2 id="proof-regarding-pie-and-pi-e"&gt;Proof regarding &lt;span class="math"&gt;\(\pi+e\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt; &lt;/h2&gt;
&lt;p&gt;For this part, I was stuck, so I got some help from the author himself:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the &lt;span class="math"&gt;\(\pi+e\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt; part, there are two steps: (1) prove that a number which is the root of a polynomial whose coefficients are algebraic is also algebraic, and (2) construct a polynomial whose roots are &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt;, and whose coefficients can be expressed in terms of &lt;span class="math"&gt;\(\pi+e\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The author said the second part was easy. It is, but it’s not obvious. Apparently you can create a polynomial from a series of roots by just &lt;a href="https://www.purplemath.com/modules/fromzero2.htm"&gt;subtracting them from &lt;span class="math"&gt;\(x\)&lt;/span&gt; and multiplying them together&lt;/a&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ (x-r_1)(x-r_2) $$&lt;/div&gt;
&lt;p&gt;Since we want a polynomial with roots &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
f(x) &amp;amp;= (x-\pi)(x-e) \\
&amp;amp;= x^2 - ex - \pi x + \pi e \\
&amp;amp;= x^2 - (\textcolor{teal}{\pi + e})x + \textcolor{orange}{\pi e}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;Now, back to the first part. In a more general form, the statement about the roots of a polynomial would look like&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \prod_{i=1}^n (x-r_i) $$&lt;/div&gt;
&lt;p&gt;for a polynomial with &lt;span class="math"&gt;\(n\)&lt;/span&gt; roots.&lt;/p&gt;
&lt;p&gt;That means all that can ever happen to any &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; is the accumulation of multiplication or addition operations.&lt;/p&gt;
&lt;p&gt;As discussed above, for any two algebraic numbers, their sum and their product are both algebraic as well. So for a set of &lt;span class="math"&gt;\(r_1, \cdots, r_n\)&lt;/span&gt; that are all algebraic, &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; will also be algebraic.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; are known to &lt;em&gt;not&lt;/em&gt; be algebraic, so accumulating multiplication or addition with algebraic numbers will never &lt;em&gt;make&lt;/em&gt; them algebraic. Therefore the resulting coefficients will not be algebraic.&lt;/p&gt;
&lt;p&gt;Let’s try making &lt;span class="math"&gt;\(\pi + e\)&lt;/span&gt; a root of a polynomial that has an &lt;em&gt;algebraic&lt;/em&gt; root as well.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
g(x) &amp;amp;= \Big(x - (\pi + e)\Big)(x + 2) \\
&amp;amp;= x^2 + 2x - (\pi + e)x - 2(\pi + e) \\
&amp;amp;= x^2 + (2 - \pi - e)x - 2\pi - e
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Nope! How about &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt;?&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
h(x) &amp;amp;= (x - \pi e)(x + 2) \\
&amp;amp;= x^2 + 2x - \pi e x - 2 \pi e \\
&amp;amp;= x^2 + (2 - \pi e)x - 2 \pi e
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Here, we cannot know for sure. If &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt; is algebraic, then &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; will also be algebraic. But we don’t know. So it is true that &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; are not algebraic, but &lt;span class="math"&gt;\(\pi + e\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi e\)&lt;/span&gt; cannot &lt;em&gt;both&lt;/em&gt; be algebraic.&lt;/p&gt;
&lt;h1 id="26"&gt;2.6&lt;/h1&gt;
&lt;p&gt;I don’t know how to prove Vieta’s formulas other than working through them. The product is easier than the sum and can be done without substituting actual numbers. Let’s try a polynomial of degree 3 (&lt;span class="math"&gt;\(n=3\)&lt;/span&gt;):&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered}
(x - r_3)(x - r_2)(x - r_1) \\
(x^2 - r_2x - r_3x + \textcolor{red}{r_2r_3})(x - r_1) \\
x^3 - r_2x^2 - r_3x^2 + r_2r_3x - (r_1x^2 + r_1r_2x + r_1r_3x - \textcolor{red}{r_1r_2r_3}) \\
x^3 \textcolor{teal}{- r_2x^2 - r_3x^2} + \textcolor{orange}{r_2r_3x} - \textcolor{teal}{r_1x^2} \textcolor{orange}{- r_1r_2x - r_1r_3x} + \textcolor{red}{r_1r_2r_3} \\
{\underbrace{\textcolor{lightgray}{1}}_{a_3}} x^3 + {\underbrace{(\textcolor{teal}{-r_1 - r_2- r_3}}_{a_2})} x^2 + {\underbrace{(\textcolor{orange}{-r_1r_2 - r_1r_3 + r_2r_3})}_{a_1}} x + \underbrace{\textcolor{red}{r_1r_2r_3}}_{a_0}
\end{gathered} $$&lt;/div&gt;
&lt;p&gt;According to Vieta’s formula, the following should be true:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\prod_{i=1}^n r_i &amp;amp;= (-1)^n \frac{a_0}{a_n} \\
r_1r_2r_3 &amp;amp;\overset{?}{=} (-1)^3 \frac{r_1r_2r_3}{1} \end{aligned} $$&lt;/div&gt;
&lt;p&gt;Indeed it is! And if you look at the term I went back and highlighted in red above, you can see why:&lt;/p&gt;
&lt;p&gt;As you keep multiplying binomials, the final coefficient (the one without a variable) is always going to be the cumulative product of the roots (the term in the binomials without a variable), with the sign switching for each binomial you multiply.&lt;/p&gt;
&lt;p&gt;But what about the sum of the roots?&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\sum_{i=1}^n r_i &amp;amp;= -\frac{a_{n-1}}{a_n} \\
r_1 + r_2 + r_3 &amp;amp;\overset{?}{=} -\frac{a_2}{a_3} \\
&amp;amp;\overset{?}{=} -\frac{-r_1 - r_2 - r_3}{1} \\
&amp;amp;= r_1 + r_2 + r_3
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;I can’t explain why this is the case, but I can see that it does work.&lt;/p&gt;
&lt;p&gt;So Vieta’s formulas are basically saying that for any degree &lt;span class="math"&gt;\(n\)&lt;/span&gt; polynomial &lt;span class="math"&gt;\(a_n \prod_{i=1}^n (x - r_i)\)&lt;/span&gt; with roots &lt;span class="math"&gt;\(r_1, ..., r_n\)&lt;/span&gt;, where &lt;span class="math"&gt;\(a_n\)&lt;/span&gt; acts to “scale” the polynomial, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a_0\)&lt;/span&gt; (the coefficient without a variable) is the de-scaled product of all the roots (flipping its sign for each iteration), and&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a_{n-1}\)&lt;/span&gt;, the coefficient of the second highest term, is the flipped-sign de-scaled sum of all the roots.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="29"&gt;2.9&lt;/h1&gt;
&lt;p&gt;Wilkinson’s polynomial has a very precise shape because it was created from multiplying very simple binomials. It is “infinitely” steep near its roots (almost a straight line) and because the terms are of such a high order, altering the coefficients slightly turns what was a very “simple” function (the product of simple binomials) into an extremely complicated one.&lt;/p&gt;
&lt;h1 id="212"&gt;2.12&lt;/h1&gt;
&lt;p&gt;As far as I found, fields of math involved in different proofs of the Fundamental Theorem of Algebra include: Complex analysis, real analysis, topology, and Riemannian differential geometry. Pretty scary stuff.&lt;/p&gt;</content></entry><entry><title>PIM notes, Chapter 2 (Polynomials)</title><link href="http://tabidots.github.io/2019/01/pim-notes-chapter-2" rel="alternate"></link><published>2019-01-17T12:54:19+07:00</published><updated>2019-01-17T12:54:19+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-17:/2019/01/pim-notes-chapter-2</id><summary type="html">&lt;p&gt;My first experience with something that resembles a math textbook in many years, but this time with “big kid math.” It’s&amp;nbsp;hard!&lt;/p&gt;</summary><content type="html">
&lt;p&gt;I started reading Jeremy Kun’s &lt;em&gt;&lt;a href="https://pimbook.org/"&gt;A Programmer’s Introduction to Mathematics&lt;/a&gt;&lt;/em&gt;. This is just a collection of my notes from Chapter 2, or code/math that I felt like writing/typesetting as an exercise while working through the chapter.&lt;/p&gt;
&lt;p&gt;I activated the SuperFences Markdown plugin in the blog’s settings, so it’s really cool to write code for the same thing in different languages side-by-side. (The Java lexer is a little off, though.)&lt;/p&gt;
&lt;p&gt;Note: The chapter is divided into the “main material,” an implementation of something that uses the relevant math, and exercises. I had actually gotten through the material and code part of the chapter last week, before I wrote about Markov matrices, and thought I’d be able to publish the complete notes in one go.&lt;/p&gt;
&lt;p&gt;However, the exercises are time-consuming and quickly get very difficult, so my impatience compels me to split this post in two, since it’d be strange to be writing about &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; notation after covering way more advanced material! I’ll publish my answers to the exercises another week.&lt;/p&gt;
&lt;h1 id="sum-summation"&gt;&lt;span class="math"&gt;\(\sum\)&lt;/span&gt; (Summation)&lt;/h1&gt;
&lt;p&gt;Summation notation wasn’t new to me (I learned it the hard way trying to make sense of the linear regression stuff), nor was the Python equivalent. However, since it was presented in Java, I thought it was good opportunity to see the correspondence between Python and Java. When I first started learning to code (beyond web development), I ran away from Java with my tail between my legs, but now it makes a lot more sense. It’s just terribly verbose and inefficient.&lt;/p&gt;
&lt;p&gt;Also, I wanted to use an easy example to get reacquainted with Clojure, my other favorite language, and pick up some R and Julia along the way too.&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^x i $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;list_comp_sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_1" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;sum-to&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sum-to&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_2" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;sumTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;mySum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;mySum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mySum&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;sumTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$9&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_3" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_3"&gt;Julia&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_1_4" name="__tabs_1" type="radio"/&gt;
&lt;label for="__tab_1_4"&gt;R&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sum_to&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
  &lt;span class="nf"&gt;for &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;sum_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;[1]&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="prod-pi-product"&gt;&lt;span class="math"&gt;\(\prod\)&lt;/span&gt; (Pi-product)&lt;/h1&gt;
&lt;p&gt;This was new to me, and so was the existence of the &lt;code class="highlight"&gt;*=&lt;/code&gt; operator, which I guess I had never had a need for.&lt;/p&gt;
&lt;div class="math"&gt;$$ g(x) = \prod_{j=1}^x i $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_1" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;mult-all&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="nv"&gt;*&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;mult-all&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_2" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;multAll&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;myProd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;myProd&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;myProd&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;multAll&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$10&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_3" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_3"&gt;Julia&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_2_4" name="__tabs_2" type="radio"/&gt;
&lt;label for="__tab_2_4"&gt;R&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mult_all&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
  &lt;span class="nf"&gt;for &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;mult_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;[1]&lt;/span&gt; &lt;span class="m"&gt;120&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="sumprod-nested-product-in-sum"&gt;&lt;span class="math"&gt;\(\sum\prod\)&lt;/span&gt; (Nested product in sum)&lt;/h1&gt;
&lt;p&gt;Pretty wild. I tried really hard to translate this into Clojure, but I couldn’t as-is.&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^n \textrm{bar}(i) \Bigg(\prod_{j \not = i} \textrm{foo}(i, j)\Bigg) $$&lt;/div&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;inner&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;
        &lt;span class="n"&gt;inner&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_3_1" name="__tabs_3" type="radio"/&gt;
&lt;label for="__tab_3_1"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;innerProd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;++)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;innerProd&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="horners-method"&gt;Horner’s Method&lt;/h1&gt;
&lt;p&gt;I had never heard of this until I encountered it in the code for the author’s &lt;code class="highlight"&gt;Polynomial&lt;/code&gt; class. It is definitely easier to understand how it works in Python than it is to understand why it works mathematically!&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \sum_{i=0}^{n-1} a_i x^i &amp;amp;= a_0 + a_1 x + a_2 x^2 + \cdots + a_{n-1} x^{n-1} \\
&amp;amp;= a_0 + x(a_1 + x(a_2 + \cdots + x\,a_{n-1})) \end{aligned} $$&lt;/div&gt;
&lt;p&gt;Cooler still, the recursion evident in the Python code means that it can be implemented as a &lt;code class="highlight"&gt;reduce&lt;/code&gt; in functional programming, making it extremely concise and loopless.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(f(x) = 2x^3 + 4x + 3\)&lt;/span&gt;, let’s find &lt;span class="math"&gt;\(f(2)\)&lt;/span&gt; with Horner’s method.&lt;/p&gt;
&lt;div class="superfences-tabs"&gt;
&lt;input checked="checked" id="__tab_4_0" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_0"&gt;Python&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;horners_method&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;horners_method&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_4_1" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_1"&gt;Clojure&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;horners-method&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;coefs&lt;/span&gt; &lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;+ &lt;/span&gt;&lt;span class="nv"&gt;%2&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;* &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt; &lt;span class="nv"&gt;%1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reverse &lt;/span&gt;&lt;span class="nv"&gt;coefs&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="nv"&gt;user=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;horners-method&lt;/span&gt; &lt;span class="nv"&gt;coefs&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;27&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;input id="__tab_4_2" name="__tabs_4" type="radio"/&gt;
&lt;label for="__tab_4_2"&gt;Java&lt;/label&gt;
&lt;div class="superfences-content"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;main&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="nf"&gt;hornersMethod&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;];&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;jshell&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;hornersMethod&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;},&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;$32&lt;/span&gt; &lt;span class="o"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;27.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="nested-polynomials"&gt;Nested polynomials&lt;/h1&gt;
&lt;p&gt;Speaking of nested polynomials, in the section on interpolating polynomials (normal ones), I was stuck on this line in the function &lt;code class="highlight"&gt;single_term()&lt;/code&gt; for a bit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;single_term&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;xi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;

        &lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="hll"&gt;        &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;theTerm&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;yi&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function is supposed to get us the term of the polynomial for point &lt;span class="math"&gt;\(i\)&lt;/span&gt; of the points we feed it. It’s this, without the summation:&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = \sum_{i=0}^n y_i \Bigg(\prod_{j \not= i} \frac{x - x_j}{x_i - x_j}\Bigg) $$&lt;/div&gt;
&lt;p&gt;How does the fraction &lt;span class="math"&gt;\(\frac{x - x_j}{x_i - x_j}\)&lt;/span&gt; get broken down into &lt;code class="highlight"&gt;Polynomial(&lt;/code&gt;&lt;span class="math"&gt;\(\frac{-x_j}{x_i - x_j}, \frac{1}{x_i - x_j}\)&lt;/span&gt;&lt;code class="highlight"&gt;)&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;code class="highlight"&gt;&lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;/code&gt; produces a polynomial &lt;span class="math"&gt;\(a\textcolor{lightgray}{x^0} + b\textcolor{orange}x\textcolor{lightgray}{^1} + c\textcolor{orange}{x^2}\)&lt;/span&gt;, so &lt;code class="highlight"&gt;&lt;span class="n"&gt;Polynomial&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xj&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xi&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xj&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;&lt;/code&gt; yields&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{-\textcolor{maroon}{x_j}}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}} + \frac{1}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}}\textcolor{orange}x = \frac{\textcolor{orange}x - \textcolor{maroon}{x_j}}{\textcolor{teal}{x_i} - \textcolor{maroon}{x_j}}$$&lt;/div&gt;
&lt;p&gt;Ah, makes sense. It‘s easy to miss (for me, anyway), but the function &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; isn’t the only polynomial here—the term within the &lt;span class="math"&gt;\(\prod\)&lt;/span&gt; is itself also a polynomial.&lt;/p&gt;
&lt;p&gt;At first, I thought it was just a clever trick, but the reason for factoring out the &lt;span class="math"&gt;\(x\)&lt;/span&gt; without a subscript is basically that unlike everything else in the entire function, that &lt;span class="math"&gt;\(x\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt; being iterated over by either the &lt;span class="math"&gt;\(\prod\)&lt;/span&gt; (iterator &lt;span class="math"&gt;\(j\)&lt;/span&gt;) or &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; (iterator &lt;span class="math"&gt;\(i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;It’s the general indeterminate quantity &lt;span class="math"&gt;\(\textcolor{orange}x\)&lt;/span&gt;, and not &lt;span class="math"&gt;\(\textcolor{teal}{x_i}\)&lt;/span&gt; or &lt;span class="math"&gt;\(\textcolor{maroon}{x_j}\)&lt;/span&gt; (i.e., the x-coordinate of one of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; points that we provided to the function), which are actually part of the coefficients here. Incidentally, separating the static &lt;code class="highlight"&gt;x&lt;/code&gt; from the dynamic &lt;code class="highlight"&gt;x&lt;/code&gt;s was a stumbling block for me as I imagined how to tackle this.&lt;/p&gt;</content><category term="pimbook"></category></entry><entry><title>The Loopless Loop (or How I made my code run 7,000 times faster)</title><link href="http://tabidots.github.io/2019/01/loopless-loop" rel="alternate"></link><published>2019-01-10T19:00:46+07:00</published><updated>2019-01-10T19:00:46+07:00</updated><author><name>Justin Douglas</name></author><id>tag:tabidots.github.io,2019-01-10:/2019/01/loopless-loop</id><summary type="html">&lt;p&gt;In which I (1) discover that the purpose of linear algebra is not to just manipulate spreadsheets and move vectors around but to make your code faster and cleaner—in other words, to give it a Zen uppercut; and (2) learn LaTeX and start a blog just to see syntax-highlighted code and properly typeset math on the same&amp;nbsp;page.&lt;/p&gt;</summary><content type="html">
&lt;h1 id="part-1-ugly-math-ugly-code"&gt;Part 1: Ugly math, ugly code&lt;/h1&gt;
&lt;p&gt;As I embarked on my journey to learn the math side of machine learning, all of the blog posts seemed to point to linear algebra as the starting point. The problem was, nothing I read made it immediately clear &lt;em&gt;how&lt;/em&gt; linear algebra played a role in machine learning.&lt;/p&gt;
&lt;p&gt;Worse yet, the whole discussion of vectors, matrices, linear combinations, and linear transformations seemed completely disconnected from my layman’s understanding of machine learning.&lt;/p&gt;
&lt;p&gt;The Khan Academy videos on linear algebra are quite tedious and I didn’t feel I was getting anywhere. &lt;a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;3blue1brown’s Linear Algebra series&lt;/a&gt; is much more engaging and lucid, but I was still getting bogged down in the theory without seeing the application.&lt;/p&gt;
&lt;p&gt;Needless to say, it was pretty slow going.&lt;/p&gt;
&lt;h2 id="my-linear-algebra-a-ha-moment"&gt;My linear algebra &lt;em&gt;a-ha&lt;/em&gt; moment&lt;/h2&gt;
&lt;p&gt;It wasn’t until I switched gears and decided, on a whim, to tackle &lt;em&gt;linear regression&lt;/em&gt; that linear algebra really started to click for me. On a practical level, code that uses linear-algebraic methods simplifies work for the computer by orders of magnitude, making it possible to process massive datasets—and process them rapidly. This is obviously a critical requirement in the age of Big Data.&lt;/p&gt;
&lt;p&gt;And on a conceptual level, it gave me my first mathematical &lt;em&gt;a-ha&lt;/em&gt; moment:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;By manipulating matrices and vectors, you can achieve the same outcome as a loop without explicitly looping—a loopless loop&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;That&lt;/em&gt; is why linear algebra is the cornerstone of machine learning.&lt;/p&gt;
&lt;p&gt;The only thing is, no single resource I found on the internet seemed to really clarify the mathematical and programmatic aspects &lt;em&gt;at the same time&lt;/em&gt; without getting too abstract on the math side of things.&lt;/p&gt;
&lt;p&gt;As a self-taught and formerly math-phobic coder, I needed a guide that progressed from from inelegant code (which I could understand) and inelegant math to (mind-blowingly) elegant math, which would then lay the groundwork for writing extremely elegant—and performant—code (which is incomprehensible without understanding the math).&lt;/p&gt;
&lt;p&gt;This is that guide, created from my notes from Week 1 of my machine learning journey. I’ve split it up into multiple posts, since it’s quite long.&lt;/p&gt;
&lt;h2 id="how-to-make-a-computer-explode"&gt;How to make a computer explode&lt;/h2&gt;
&lt;p&gt;I always thought that for-loops were simply a fact of life.&lt;/p&gt;
&lt;p&gt;While I understood the basic principle behind stochastic gradient descent, I had never implemented it myself before. If I had tried my hand at it before learning the math involved, I probably would have come up with this monster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# DON'T TRY THIS&lt;/span&gt;

&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;lists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;actual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# 0 to start&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sum_of_sq_errs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
                &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;sum_of_sq_errs&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;partial_deriv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;summation_part_of_cost&lt;/span&gt;
            &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;partial_deriv&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="c1"&gt;# I REALLY HOPE YOU DIDN'T TRY THIS&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay, maybe not. (It would be pretty hard to write this without understanding the math involved.)&lt;/p&gt;
&lt;p&gt;Count those loops—&lt;em&gt;three&lt;/em&gt;, to be exact! Terrifying. Now, being handy with Python, I could probably have calculated &lt;code class="highlight"&gt;partial_deriv&lt;/code&gt; in one line with an even more terrifying list comprehension, just to show off:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;partial_deriv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;…which shaves off four lines, at the expense of all readability. But bigger problems remain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The time complexity of this program is off the charts.&lt;/strong&gt; In &lt;a href="https://en.wikipedia.org/wiki/Time_complexity"&gt;Big-O time complexity&lt;/a&gt; terms, it is &lt;em&gt;at least&lt;/em&gt; &lt;span class="math"&gt;\(O(n^3)\)&lt;/span&gt;, if not more, which is cubic time, or (literally) &lt;em&gt;exponentially &lt;a href="http://bigocheatsheet.com/"&gt;horrible&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It doesn’t even work.&lt;/strong&gt; Even with a dataset of unremarkable size, you’re bound to get a &lt;code class="highlight"&gt;&lt;span class="n"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;overflow&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;/code&gt; that can’t be avoided even if you set the &lt;code class="highlight"&gt;learning_rate&lt;/code&gt; to an impractically small value like &lt;code class="highlight"&gt;0.00001&lt;/code&gt;. Trust me, I’ve tried.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there any way beyond this impasse?&lt;/p&gt;
&lt;h2 id="meeting-a-zen-master-on-the-road"&gt;Meeting a Zen master on the road&lt;/h2&gt;
&lt;p&gt;In Zen Buddhism, there is a famous book called &lt;em&gt;The Gateless Gate&lt;/em&gt;, which is a collection of &lt;em&gt;koans&lt;/em&gt;. A Zen &lt;em&gt;koan&lt;/em&gt; is a riddle that cannot be approached with the rational mind. For example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Goso said: “When you meet a Zen master on the road, you cannot talk to him, but neither can you face him with silence. What are you going to do?”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To solve it, you have to transcend the duality of &lt;em&gt;this&lt;/em&gt; and &lt;em&gt;not-this&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Give him an uppercut&lt;br/&gt;
And you will be called one who understands Zen.&lt;/em&gt;&lt;br/&gt;
—The Gateless Gate, Koan #36&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you might imagine, linear algebra—&lt;strong&gt;the loopless loop&lt;/strong&gt;—is the Zen uppercut of coding. What, then is the target of our uppercut?&lt;/p&gt;
&lt;h2 id="linear-regression-a-basic-overview"&gt;Linear regression: A basic overview&lt;/h2&gt;
&lt;p&gt;Basically, linear regression is used to make predictions about a &lt;em&gt;thing&lt;/em&gt; based on its characteristics, assuming that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that there is some correlation among those characteristics and&lt;/li&gt;
&lt;li&gt;that you have plenty of data about other &lt;em&gt;things&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The intuition here can be explained with middle school algebra. Imagine you know the square footage and price of 200 houses and you want to estimate the price of a house with a given square footage.&lt;/p&gt;
&lt;p&gt;Obviously, this is an oversimplified correlation for the sake of example. (And for some reason, everyone seems to explain this concept using houses, so why reinvent the wheel?)&lt;/p&gt;
&lt;p&gt;If you were to make a scatter plot of that data, with the area along the x-axis and the price along the y-axis, the pattern might roughly look like it follows a line—not perfectly linear, but linear &lt;em&gt;enough&lt;/em&gt; to predict the price of a house with &lt;span class="math"&gt;\(x\)&lt;/span&gt; square footage. You can use linear regression to work backward from the data to determine this line, &lt;strong&gt;the line of best fit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In middle school algebra, lines are written in the form&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{orange}{b} $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt; is the input, &lt;span class="math"&gt;\(\textcolor{magenta}{m}\)&lt;/span&gt; is the slope of the line, &lt;span class="math"&gt;\(\textcolor{orange}{b}\)&lt;/span&gt; moves the line up and down on the graph, and &lt;span class="math"&gt;\(y\)&lt;/span&gt; is the height of the line at point &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our house problem can also be framed as a line, where &lt;span class="math"&gt;\(\textcolor{teal}{x}\)&lt;/span&gt; is the square footage, which influences the price by some value &lt;span class="math"&gt;\(\textcolor{magenta}{m}\)&lt;/span&gt;, to which we add some kind of base price to bring us to the final price, &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Well, that was easy enough, right?&lt;/p&gt;
&lt;h2 id="multiple-linear-regression-because-more-is-better-or-something"&gt;Multiple linear regression: Because more is better (or something)&lt;/h2&gt;
&lt;p&gt;In the real world, of course, area is not the only factor that decides the price of a house. There are many others. Can we still adapt our middle school equation to this problem if each house has 3 features—say, area, nearby property values, and age of the building?&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{magenta}{n}\textcolor{teal}{z} + \textcolor{magenta}{o}\textcolor{teal}{a} + \textcolor{orange}{b} $$&lt;/div&gt;
&lt;p&gt;We can, but it’s messy. (It’s also no longer a line, but let’s ignore that for now.) First, let’s rewrite that “base price” as &lt;span class="math"&gt;\(b \cdot 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ y = \textcolor{magenta}{m}\textcolor{teal}{x} + \textcolor{magenta}{n}\textcolor{teal}{z} + \textcolor{magenta}{o}\textcolor{teal}{a} + \textcolor{magenta}{b}\cdot\textcolor{teal}{1} $$&lt;/div&gt;
&lt;p&gt;This gives us a nice symmetry: Notice that all of the teal variables are features, which are multiplied by their degree of influence (called a &lt;em&gt;coefficient&lt;/em&gt; in statistics, or a &lt;em&gt;weight&lt;/em&gt; in machine learning). When you add all these together, you get the price of the house.&lt;/p&gt;
&lt;p&gt;This is called &lt;strong&gt;multiple linear regression&lt;/strong&gt;. Most people wouldn’t skip directly to multiple &lt;span class="caps"&gt;LR&lt;/span&gt; after introducing single &lt;span class="caps"&gt;LR&lt;/span&gt;, but single &lt;span class="caps"&gt;LR&lt;/span&gt; is pretty easy to digest if you can understand high school calculus (derivatives), so it didn’t level up my math knowledge.&lt;/p&gt;
&lt;p&gt;Now, let’s code our equation, putting all the feature values into a &lt;em&gt;list&lt;/em&gt; &lt;code class="highlight"&gt;features&lt;/code&gt; and the weights into another &lt;em&gt;list&lt;/em&gt; &lt;code class="highlight"&gt;weights&lt;/code&gt;. In Python, in increasing order of elegance, we can write the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Using completely random numbers just to show the code&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 3 features; first value is the “dummy feature”&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Brute-force addition&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we use &lt;span class="math"&gt;\(x\)&lt;/span&gt;s to denote our features, &lt;span class="math"&gt;\(θ\)&lt;/span&gt;s to denote our weights, and subscript numbers to denote the position of each item in a list (series), then we can rewrite our equation in a slightly more organized way:&lt;/p&gt;
&lt;div class="math"&gt;$$ h_θ(x) = \textcolor{magenta}{θ_0}\textcolor{lightgray}{x_0} + \textcolor{magenta}{θ_1}\textcolor{teal}{x_1} + \cdots + \textcolor{magenta}{θ_n}\textcolor{teal}{x_n}  $$&lt;/div&gt;
&lt;p&gt;which just happens to be the &lt;strong&gt;generalized form of linear regression&lt;/strong&gt;—&lt;em&gt;general&lt;/em&gt; in the sense that it can accommodate any number of features, whether that’s 1 or 1,000.&lt;/p&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is the collection of all feature values &lt;span class="math"&gt;\(\textcolor{teal}{x_1}\)&lt;/span&gt; through &lt;span class="math"&gt;\(\textcolor{teal}{x_n}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of features. (And &lt;span class="math"&gt;\(x_{1000}\)&lt;/span&gt; actually isn’t too crazy in terms of real-world datasets!) It also includes the dummy feature &lt;span class="math"&gt;\(\textcolor{lightgray}{x_0} = 1\)&lt;/span&gt;. Likewise, &lt;span class="math"&gt;\(θ\)&lt;/span&gt; is the collection of all weights, including &lt;span class="math"&gt;\(\textcolor{magenta}{θ_0}\)&lt;/span&gt;, the “base price” in our example. In machine learning, this is called the &lt;em&gt;bias&lt;/em&gt; value.&lt;/p&gt;
&lt;p&gt;Finally, the function notation &lt;span class="math"&gt;\(h_θ(x)\)&lt;/span&gt; indicates that this is the &lt;strong&gt;hypothesis&lt;/strong&gt; for item (house) &lt;span class="math"&gt;\(x\)&lt;/span&gt; given the collection of weights &lt;span class="math"&gt;\(θ\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="how-to-python-in-math-lesson-1"&gt;How to Python in math (Lesson 1)&lt;/h2&gt;
&lt;p&gt;If you know anything about programming, you know that the last line of code above is no way to write a program. Accommodating 100 features would be a chore, and accommodating a variable number of features would be impossible. Naturally, we would use the magic of iteration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Super-basic iteration over the lists&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;No one actually writes the linear regression formula like this, but if you wanted to, you could express the above code in math using a &lt;em&gt;summation&lt;/em&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ h_θ(x) = \sum_{j=0}^n \textcolor{magenta}{θ_j}\textcolor{teal}{x_j} $$&lt;/div&gt;
&lt;p&gt;That Greek letter &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; (sigma) means &lt;em&gt;summation&lt;/em&gt;. Basically, run a for-loop that adds the result of the following expression for each sample (house) starting at &lt;span class="math"&gt;\(j=0\)&lt;/span&gt; until &lt;span class="math"&gt;\(j=n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you know Python better than you know math (as I did), then you might try further refactoring the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Functional programming version&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;

&lt;span class="c1"&gt;# More Pythonic version of the above&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I didn’t realize this when I wrote the first draft of this post, but even expressing the simple Python function &lt;code class="highlight"&gt;zip(x, y)&lt;/code&gt; in math requires linear algebra. I’ll get back to the non-fancy version of linear regression in just a minute, but for sake of thoroughness, if &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(θ\)&lt;/span&gt; are both vectors, then&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\vec x = \begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{bmatrix} &amp;amp; \qquad{}
\vec θ = \begin{bmatrix} θ_0 \\ θ_1 \\ \vdots \\ θ_n \end{bmatrix} &amp;amp; \quad{}
z(\vec x, \vec θ) = \vec θ\vec x \end{aligned} $$&lt;/div&gt;
&lt;p&gt;Similarly, we can rewrite our equation as a function in Python, too. Let’s leave vectors aside for now, but keep the &lt;code class="highlight"&gt;zip&lt;/code&gt; and encapsulate it into a function, because it’s clean and easy to understand.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="einstein-notation-making-math-less-confusing-by-making-it-more-confusing"&gt;Einstein notation: Making math less confusing by making it more confusing&lt;/h2&gt;
&lt;p&gt;This is all great if there’s only one &lt;span class="math"&gt;\(x\)&lt;/span&gt; (house). But we will need tons of houses to make a decent prediction. Our list &lt;code class="highlight"&gt;houses&lt;/code&gt; needs to be changed into a &lt;em&gt;list of lists&lt;/em&gt;. For the sake of example, if we had three houses and three features, &lt;code class="highlight"&gt;houses&lt;/code&gt; would look like this (remember the dummy feature):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# random values&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This also allows us to refer to specific features of specific houses using two indexes, &lt;code class="highlight"&gt;houses[i][j]&lt;/code&gt;. How do we do this in math, though? Enter &lt;strong&gt;Einstein notation&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ h_θ(x^i) = θ_0x_0^i + θ_1x_1^i + \cdots + θ_nx_n^i  $$&lt;/div&gt;
&lt;p&gt;The superscript numbers here &lt;em&gt;aren’t&lt;/em&gt; exponents. You would think Einstein, of all people, could come up with something less confusing, but that is the convention, so it’s important to become familiar with it.&lt;/p&gt;
&lt;p&gt;Just remember that in linear regression, there are conventional choices for the variable names. &lt;span class="math"&gt;\(i\)&lt;/span&gt; denotes the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th house, and &lt;span class="math"&gt;\(j\)&lt;/span&gt; the &lt;span class="math"&gt;\(j\)&lt;/span&gt;th feature.&lt;/p&gt;
&lt;div class="math"&gt;$$ x^{\textcolor{orange}{i \textrm{th sample}}}_{\textcolor{blue}{j \textrm{th feature}}} $$&lt;/div&gt;
&lt;p&gt;So if we were to start describing each hypothesis for our dataset individually,&lt;/p&gt;
&lt;div class="math"&gt;$$ h_θ(x^i) = \left\{\begin{array}{ll}
h_θ(x^0) = θ_0x_0^0 + θ_1x_1^0 + \cdots + θ_nx_n^0 \\[0.5em]
h_θ(x^1) = θ_0x_0^1 + θ_1x_1^1 + \cdots + θ_nx_n^1 \\[0.5em]
h_θ(x^2) = θ_0x_0^2 + θ_1x_1^2 + \cdots + θ_nx_n^2 \\[0.5em]
h_θ(x^3) = θ_0x_0^2 + θ_1x_1^2 + \cdots + θ_nx_n^2
\end{array}\right. $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;hyps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="cost-function-how-accurate-are-our-predictions"&gt;Cost function: How accurate are our predictions?&lt;/h2&gt;
&lt;p&gt;Seeing as the goal of linear regression is to come up with a line that best fits the data, we need some way to evaluate a line’s &lt;strong&gt;goodness of fit&lt;/strong&gt; to the data. One measure of that is the &lt;strong&gt;mean squared error&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="error"&gt;Error&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Error&lt;/em&gt; is how far the prediction for one sample (house) is from its actual value.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{error}_i &amp;amp;= \textrm{prediction}_i - \textrm{actual}_i\\
&amp;amp;= \hat{Y}_i - Y_i
\end{aligned} $$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The subscript &lt;span class="math"&gt;\(i\)&lt;/span&gt; here is &lt;strong&gt;not&lt;/strong&gt; Einstein notation, because these are just lists of values, not a “spreadsheet” of rows and columns. The Einstein notation in this discussion of linear regression only applies to &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat Y\)&lt;/span&gt; is read “Y-hat,” which is just a statistical convention. It can be substituted with our function &lt;span class="math"&gt;\(h_θ(x^i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \textrm{error}_i = h_θ(x^i) - Y_i $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;actual_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 3 houses; random values&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;hypothesis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="squared-error"&gt;Squared error&lt;/h3&gt;
&lt;p&gt;We &lt;em&gt;square&lt;/em&gt; it so that (1) all values are positive, preventing underestimates and overestimates from canceling each other out; and (2) larger errors are considered proportionally “more erroneous” than smaller errors.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{SE}_i &amp;amp;= \textrm{error}^2\\
&amp;amp;= (\hat{Y}_i - Y_i)^2\\
&amp;amp;= (h_θ(x^i) - Y_i)^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="mean-squared-error"&gt;Mean squared error&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;mean&lt;/em&gt; value of the squared error for all samples can give us an idea about our line’s goodness of fit.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{MSE} &amp;amp;= \frac{\textrm{SE}_\textcolor{red}{1} \textcolor{blue}{+ \cdots + } \textrm{ SE}_\textcolor{red}{\textrm{number of samples}}}{\textrm{number of samples}}\\
&amp;amp;= \frac{\textcolor{blue}{\sum}_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} \textrm{SE}}{m}\\
&amp;amp;= \frac{\sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (\hat{Y}_{\textcolor{red}i} - Y_{\textcolor{red}i})^2}{m}\\
&amp;amp;= \frac{1}{m} \sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (\hat{Y}_{\textcolor{red}i} - Y_{\textcolor{red}i})^2\\
&amp;amp;= \frac{1}{m} \sum_{\textcolor{red}{i=1}}^{\textcolor{red}{m}} (h_θ(x^{\textcolor{red}i}) - Y_{\textcolor{red}i})^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_se&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;

&lt;span class="c1"&gt;# More Pythonic and more similar to the actual math notation&lt;/span&gt;
&lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;span class="caps"&gt;MSE&lt;/span&gt; gives us an indication of goodness of fit, but it’s difficult to tie that value directly to the data. You can use the &lt;span class="caps"&gt;RMSE&lt;/span&gt; (root mean squared error), which is just the square root of the &lt;span class="caps"&gt;MSE&lt;/span&gt;, to reframe the average error in terms of the data. In this case, the &lt;span class="caps"&gt;RMSE&lt;/span&gt; would tell us how much (in dollars) that our prediction line was off by.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s use the mean squared error to rewrite this equation as a function of the collection of weights. Every time we change the weights, we will obtain a different line with a different goodness of fit (&lt;span class="caps"&gt;MSE&lt;/span&gt;), and this relationship can be illustrated by a function called the &lt;em&gt;cost function&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Well, almost. This function is conventionally named &lt;span class="math"&gt;\(J\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ J(θ) = \frac{1}{\textcolor{magenta}{2}m} \sum_{i=1}^m (h_θ(x^i) - Y_i)^2 $$&lt;/div&gt;
&lt;p&gt;Where did that &lt;span class="math"&gt;\(\textcolor{magenta}{2}\)&lt;/span&gt; come from? Again, this is just a matter of convention. The &lt;span class="math"&gt;\(\textcolor{magenta}{2}\)&lt;/span&gt; will cancel out in the the next step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Keep in mind that &lt;span class="math"&gt;\(θ\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; are lists and &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a list of lists. What this means is that in a situation with two features (plus the dummy feature),&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} h_{θ_\textcolor{teal}{0}, θ_\textcolor{teal}{1}, θ_\textcolor{teal}{2}}(x) &amp;amp;= h_{θ_\textcolor{teal}{0}}(x_\textcolor{teal}{0}) + h_{θ_\textcolor{teal}{1}}(x_\textcolor{teal}{1}) + h_{θ_2}(x_\textcolor{teal}{2}) \\
h_{θ_\textcolor{teal}{0}, θ_\textcolor{teal}{1}, θ_\textcolor{teal}{2}}(x^\textcolor{red}{i}) &amp;amp;= h_{θ_\textcolor{teal}{0}}(x_\textcolor{teal}{0}^\textcolor{red}{i}) + h_{θ_\textcolor{teal}{1}}(x_\textcolor{teal}{1}^\textcolor{red}{i}) + h_{θ_\textcolor{teal}{2}}(x_\textcolor{teal}{2}^\textcolor{red}{i}) \\
J(θ_\textcolor{teal}{0}, θ_\textcolor{teal}{1}, θ_\textcolor{teal}{2}) &amp;amp;= \frac{1}{2m} \sum_{\textcolor{red}{i=1}}^\textcolor{red}{m} \Big[h_{θ_\textcolor{teal}{0}}(x_\textcolor{teal}{0}^\textcolor{red}{i}) + h_{θ_\textcolor{teal}{1}}(x_\textcolor{teal}{1}^\textcolor{red}{i}) + h_{θ_\textcolor{teal}{2}}(x_\textcolor{teal}{2}^\textcolor{red}{i}) - Y_{\textcolor{red}i}\Big]^2
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Numbers in teal represent feature numbers; numbers in red represent sample numbers.&lt;/p&gt;
&lt;h2 id="multivariable-calculus-how-much-effect-does-each-weight-have"&gt;Multivariable calculus: How much effect does each weight have?&lt;/h2&gt;
&lt;p&gt;Now, imagine each feature as knobs on a radio. Increasing or decreasing the weight of each feature is like turning up or down the knob for that feature. We want to “tune” our line to be as close to the data as possible by “dialing” the features up and down. In order to do this, we need to determine the effect that a given combination of knob settings has on the final output.&lt;/p&gt;
&lt;p&gt;In math terms, this is akin to asking “How much does &lt;span class="math"&gt;\(J\)&lt;/span&gt; change when &lt;span class="math"&gt;\(θ\)&lt;/span&gt; changes?” Sounds like derivatives from high school calculus.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} f(x) &amp;amp;= 5x^2 \\
\frac{df}{dx} &amp;amp;= 5\cdot2x^{2-1} \\
&amp;amp;= 10x
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Our function &lt;span class="math"&gt;\(J\)&lt;/span&gt; is actually one function inside of another, so the chain rule applies. Bonus points if you remember that from high school—I didn’t.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
J(θ) &amp;amp;= \textcolor{purple}{\frac{1}{2m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{\Big[} \textcolor{orange}{h_θ(x^i)} \textcolor{purple}{- Y_i)\Big]^2} \\
\frac{dJ(θ)}{dθ} &amp;amp;= d_\textcolor{purple}{\textrm{outer}} \cdot d_\textcolor{orange}{\textrm{inner}}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\textcolor{purple}{\textrm{outer}} &amp;amp;= \textcolor{purple}{\frac{1}{2m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{(} \textcolor{orange}{\textrm{inner}} \textcolor{purple}{- Y_i)^2} &amp;amp;
\textcolor{orange}{\textrm{inner}} &amp;amp;= \textcolor{magenta}{θ_0}\textcolor{teal}{x_0} + \textcolor{magenta}{θ_1}\textcolor{teal}{x_1} + \cdots + \textcolor{magenta}{θ_n}\textcolor{teal}{x_n} \\
d_\textcolor{purple}{\textrm{outer}} &amp;amp;= \textcolor{purple}{\frac{\cancel{2}}{\cancel{2}m}} \textcolor{purple}{\sum_{i=1}^m} \textcolor{purple}{\Big[} \textcolor{orange}{h_θ(x^i)} \textcolor{purple}{{- Y_i\Big]}^2} &amp;amp; d_\textcolor{orange}{\textrm{inner}} &amp;amp;= ???
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;This is where I got stuck. &lt;span class="math"&gt;\(θ\)&lt;/span&gt; is a collection of values, not just a single value. Each knob on our radio affects the output individually, and we have to determine the individual effect of each knob.&lt;/p&gt;
&lt;p&gt;It helps to start by breaking down what the chain rule is actually saying.&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{d[\textcolor{purple}{\textrm{outer}}(\textcolor{orange}{\textrm{inner}}(x))]}{dx} = \frac{d_\textcolor{purple}{\textrm{outer}}}{d_\textcolor{orange}{\textrm{inner}}} \cdot \frac{d_\textcolor{orange}{inner}}{dx} $$&lt;/div&gt;
&lt;p&gt;This means our “outer derivative” &lt;span class="math"&gt;\(d_\textcolor{purple}{\textrm{outer}}\)&lt;/span&gt; tells us how much our cost function &lt;span class="math"&gt;\(J(θ)\)&lt;/span&gt; changes in response to a given change in our hypothesis &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;. We now need to find the “inner derivative” &lt;span class="math"&gt;\(d_{\textcolor{orange}{\textrm{inner}}}\)&lt;/span&gt;, which tells us how much our hypothesis &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; changes in response to a given change in our weights &lt;span class="math"&gt;\(θ\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But since &lt;span class="math"&gt;\(θ\)&lt;/span&gt; is a collection of values, there isn’t a single derivative, but rather several &lt;em&gt;partial derivatives&lt;/em&gt;, which indicate how much our hypothesis &lt;span class="math"&gt;\(h(x^i)\)&lt;/span&gt; for a specific sample (house) &lt;span class="math"&gt;\(x^i\)&lt;/span&gt; changes in response to a given change in &lt;em&gt;each&lt;/em&gt; of the weights &lt;span class="math"&gt;\(θ_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Another labeling convention—just as &lt;span class="math"&gt;\(i\)&lt;/span&gt; is used to refer to, or “index,” samples from &lt;span class="math"&gt;\(1\)&lt;/span&gt; through &lt;span class="math"&gt;\(m\)&lt;/span&gt;, the total number of samples, lowercase &lt;span class="math"&gt;\(j\)&lt;/span&gt; is used to index features from &lt;span class="math"&gt;\(0\)&lt;/span&gt; through &lt;span class="math"&gt;\(n\)&lt;/span&gt;, the total number of features. To return to our Einstein notation,
&lt;div class="math"&gt;$$ x^{1 \leq \space i^\textrm{th} \textrm{ sample} \space \leq \space m \textrm{ samples}}_{0 \space \leq \space j^\textrm{th} \textrm{ feature} \space \leq \space n \textrm{ features}} $$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In math notation, this is written with a funny “d” called a “del,” &lt;span class="math"&gt;\(\partial\)&lt;/span&gt;, like this:&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_θ(x^i)}{\partial θ_j} $$&lt;/div&gt;
&lt;p&gt;This looks crazy, but the process of finding these partial derivatives is really the same as finding a normal derivative, except you &lt;em&gt;treat all the other variables as constant&lt;/em&gt;, effectively ignoring them. So, for we now only have to concern ourselves with&lt;/p&gt;
&lt;div class="math"&gt;$$ h_{θ_j}(x^i) = \begin{cases}
\textcolor{magenta}{θ_0}\textcolor{teal}{x_0^i} \textcolor{lightgray}{+θ_1x_1^i + θ_2x_2^i} &amp;amp; \text{when } j=0 \\
\textcolor{lightgray}{θ_0x_0^i} \textcolor{lightgray}{+} \textcolor{magenta}{θ_1}\textcolor{teal}{x_1^i} \textcolor{lightgray}{+θ_2x_2^i} &amp;amp; \text{when } j=1 \\
\textcolor{lightgray}{θ_0x_0^i + θ_1x_1^i} \textcolor{lightgray}{+} \textcolor{magenta}{θ_2}\textcolor{teal}{x_2^i} &amp;amp; \text{when } j=2 \\
\end{cases} $$&lt;/div&gt;
&lt;p&gt;The derivative of a variable times something else is just the &lt;em&gt;something else&lt;/em&gt;. (For a line &lt;span class="math"&gt;\(y = 2x\)&lt;/span&gt;, &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt; is just &lt;span class="math"&gt;\(2\)&lt;/span&gt;, since its slope will be 2 at every point along the line.) Thus,&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_θ(x)}{\partial θ_j} = \begin{cases}
\frac{\partial h_θ(x)}{\partial θ_0} &amp;amp;= \textcolor{lightgray}{θ_0} \textcolor{teal}{x_0^i} &amp;amp; \text{when } j=0 \\[0.5em]
\frac{\partial h_θ(x)}{\partial θ_1} &amp;amp;= \textcolor{lightgray}{θ_1} \textcolor{teal}{x_1^i} &amp;amp; \text{when } j=1 \\[0.5em]
\frac{\partial h_θ(x)}{\partial θ_2} &amp;amp;= \textcolor{lightgray}{θ_2} \textcolor{teal}{x_2^i} &amp;amp; \text{when } j=2
\end{cases} $$&lt;/div&gt;
&lt;p&gt;Or just&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\partial h_θ(x)}{\partial θ_j} = \left\{\begin{array}{lr}
x_0^i &amp;amp; \text{when } j=0 \\[0.5em]
x_1^i &amp;amp; \text{when } j=1 \\[0.5em]
x_2^i &amp;amp; \text{when } j=2
\end{array}\right\}
= \textcolor{red}{x_j^i} $$&lt;/div&gt;
&lt;p&gt;That’s &lt;span class="math"&gt;\(d_{\textcolor{orange}{\textrm{inner}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\frac{\partial{J(θ)}}{\partial{θ_j}} &amp;amp;= d_\textcolor{purple}{\textrm{outer}} \cdot d_\textcolor{orange}{\textrm{inner}} \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m (h_θ(x^i) - Y_i) \cdot \textcolor{red}{x_j^i}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Phew! That was a lot of abstract math. Finally, we have something that can be translated into code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;which_weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;house&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;which_weight&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;house&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;houses&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;effect_of_all_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="stochastic-gradient-descent-making-regression-lines-fit-again"&gt;Stochastic gradient descent: Making regression lines fit again&lt;/h2&gt;
&lt;p&gt;Strictly speaking, &lt;span class="math"&gt;\(\frac{1}{m} \sum_{i=1}^m (h_θ(x^i) - Y_i) \cdot x_j^i\)&lt;/span&gt; is not a derivative, but a &lt;strong&gt;gradient&lt;/strong&gt;—a collection of partial derivatives. In high school calculus, the derivative at a given point is visualized as the line that is tangent to the graph’s curve at that point. In multivariable calculus, the gradient at a given point is visualized as the &lt;em&gt;plane&lt;/em&gt; that is tangent to the graph’s surface at the point.&lt;/p&gt;
&lt;p&gt;In more concrete terms, imagine running a small piece of cardboard around the sides of a coffee mug so that the cardboard follows the curvature of the mug. Every point on the surface of the mug corresponds to some combination of weights, and the closer we are to the top of the mug, the greater the value of our cost function is, and so the more inaccurate our prediction is. We want to find the bottom of the mug, where the piece of cardboard is parallel to the ground, because that is where the value of the cost function is as low as possible.&lt;/p&gt;
&lt;p&gt;When that value is zero, the line would fit our data perfectly. However, that’s not possible for real-world data, so we will settle for the lowest value—that is, we want to &lt;em&gt;minimize&lt;/em&gt; the cost function.&lt;/p&gt;
&lt;div class="math"&gt;$$ \underset{θ}{\arg\min} \, J(θ) $$&lt;/div&gt;
&lt;p&gt;In the language of math (and neural networks), this is called &lt;strong&gt;stochastic gradient descent&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;gradient&lt;/em&gt; is the thing we’re trying to minimize.&lt;/li&gt;
&lt;li&gt;This process is &lt;em&gt;stochastic&lt;/em&gt; (random) because we start with random weights (all zeros), which puts us at a random point on the mug.&lt;/li&gt;
&lt;li&gt;It is a &lt;em&gt;descent&lt;/em&gt; because we want to move down to a progressively flatter region of the mug with each attempt (combination of weights).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The descent occurs in “steps.” Imagine, for a moment, a basic parabola &lt;span class="math"&gt;\(f(x) = x^2\)&lt;/span&gt; instead of a mug. The derivative at any point &lt;span class="math"&gt;\(x\)&lt;/span&gt; is &lt;span class="math"&gt;\(2x\)&lt;/span&gt;. Positive &lt;span class="math"&gt;\(x\)&lt;/span&gt; values give us positive derivatives and negative &lt;span class="math"&gt;\(x\)&lt;/span&gt; values give us negative derivatives. If we started at some point to the right of 0 and wanted to follow the parabola to its trough, we could do that by subtracting something from &lt;span class="math"&gt;\(x\)&lt;/span&gt;. Likewise, if we started at some point to the left of 0, we’d want to add something to &lt;span class="math"&gt;\(x\)&lt;/span&gt;—or rather, subtract a negative value.&lt;/p&gt;
&lt;p&gt;This means that if we start at any point &lt;span class="math"&gt;\(x\)&lt;/span&gt; and subtract &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt;, we will tend toward the trough. We don’t necessarily know exactly what our new &lt;span class="math"&gt;\(x\)&lt;/span&gt; value will be, but we can assume that subtracting &lt;span class="math"&gt;\(\frac{dy}{dx}\)&lt;/span&gt; again will take us closer to the trough, although slightly less closer. Each step brings us increasingly closer but in progressively smaller steps. At some point, we will reach &lt;strong&gt;convergence&lt;/strong&gt;, or a point that is close enough to minimum.&lt;/p&gt;
&lt;p&gt;The same applies to gradients. The gradient for any set of weights &lt;span class="math"&gt;\(θ\)&lt;/span&gt; tells us the &lt;em&gt;opposite&lt;/em&gt; direction we should go in to find the bottom of the mug. That means that if we start with some initial collection of weights &lt;span class="math"&gt;\(θ\)&lt;/span&gt; and keep subtracting the gradient, which is notated &lt;span class="math"&gt;\(\nabla J\)&lt;/span&gt;, we should eventually arrive at the bottom.&lt;/p&gt;
&lt;div class="math"&gt;$$ \textrm{repeat } θ := θ - \nabla J \textrm{ until convergence} $$&lt;/div&gt;
&lt;p&gt;But try translating this into Python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gradient_of_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;effect_of_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;last_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;gradient_of_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
    &lt;span class="n"&gt;last_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_weights&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convergence&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;

&lt;span class="n"&gt;minimum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_weights&lt;/span&gt; &lt;span class="c1"&gt;# ???&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In doing so, a couple of questions arise (besides the suspicion that there are way too many loops and functions):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do you compare weights so that you can know which of two collections is “lesser” and which is “greater”?&lt;/li&gt;
&lt;li&gt;How do you know when you’ve reached convergence?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It‘s clear that we have more or less come as far as we can with the level of math and coding that we have used so far.&lt;/p&gt;
&lt;h1 id="part-2-crazy-math-beautiful-code"&gt;Part 2: Crazy math, beautiful code&lt;/h1&gt;
&lt;p&gt;When you think about it, it almost seems a little backwards to call linear algebra the starting point of machine learning. After all, we’ve come this far without it. Multivariable calculus strikes me as more fundamental, although I suppose that it might be hard to imagine the output of a two-variable function as a bowl- or mug-shaped object without the concept of vectors.&lt;/p&gt;
&lt;p&gt;In any case, it’s time for that Zen uppercut.&lt;/p&gt;
&lt;p&gt;I won’t go into the mechanics of vector and matrix operations here; they are too tedious to write about, and I’m certainly not the best person to explain them. What I’m more interested in is the concept of &lt;strong&gt;vectorization&lt;/strong&gt;: the “translation” (pun intended) of the algebra and calculus above into linear algebra and multivariable calculus, as well as what that looks like in Python (using NumPy).&lt;/p&gt;
&lt;h2 id="vectorize-all-the-things"&gt;Vectorize all the things!!!&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ninja mode activated! (That was easy, eh?) First of all, let’s convert all lists to &lt;em&gt;vectors&lt;/em&gt; and all lists of lists to &lt;em&gt;matrices&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textbf X =
\begin{bmatrix}
  x_1^1 &amp;amp; x_2^1 &amp;amp; \dots  &amp;amp; x_n^1 \\[0.5em]
  x_1^2 &amp;amp; x_2^2 &amp;amp; \dots  &amp;amp; x_n^2 \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  x_1^m &amp;amp; x_2^m &amp;amp; \dots  &amp;amp; x_n^m
\end{bmatrix} \qquad{}
&amp;amp; \vec θ = \begin{bmatrix} θ_0 \\ θ_1 \\ θ_2 \\ \vdots \\ θ_n \end{bmatrix}
&amp;amp; \vec y = \begin{bmatrix} y_0 \\ y_1 \\ \vdots \\ y_m \end{bmatrix}
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;Let’s use &lt;code class="highlight"&gt;scikit-learn&lt;/code&gt; to generate a suitable dataset for us. Since we won’t have to do any of the computations by hand, let’s go wild with the number of features and samples. We also need a vector with our initial weights (all zeros).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# as many rows as X has columns, and 1 column&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One important thing to note is that the dummy feature &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; needs to be added to the data. Also, &lt;code class="highlight"&gt;scikit-learn&lt;/code&gt; generates a &lt;code class="highlight"&gt;y&lt;/code&gt; array that doesn’t have the proper dimensions of a vector for some reason.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# padding for bias column&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# this just fixes a quirk of sklearn's output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \textbf X =
\begin{bmatrix}
  \textcolor{red}1 &amp;amp; x_1^1 &amp;amp; x_2^1 &amp;amp; \dots &amp;amp; x_n^1 \\[0.5em]
  \textcolor{red}1 &amp;amp; x_1^2 &amp;amp; x_2^2 &amp;amp; \dots &amp;amp; x_n^2 \\[0.5em]
  \textcolor{red}\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{red}1 &amp;amp; x_1^m &amp;amp; x_2^m &amp;amp; \dots &amp;amp; x_n^m
\end{bmatrix} \qquad{}
&amp;amp; \vec θ = \begin{bmatrix} θ_0 \\ θ_1 \\ θ_2 \\ \vdots \\ θ_n \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;The hypothesis function &lt;span class="math"&gt;\(h_θ(x^i)\)&lt;/span&gt; can now be written succinctly as the product of the “houses” matrix &lt;span class="math"&gt;\(\textbf X\)&lt;/span&gt; and the weights vector &lt;span class="math"&gt;\(\vec θ\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned} \textbf X\vec θ &amp;amp;= \begin{bmatrix}
  \textcolor{teal}{x_0^1} &amp;amp; \textcolor{teal}{x_1^1} &amp;amp; \textcolor{teal}{x_2^1} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^1} \\[0.5em]
  \textcolor{teal}{x_0^2} &amp;amp; \textcolor{teal}{x_1^2} &amp;amp; \textcolor{teal}{x_2^2} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^2} \\[0.5em]
  \textcolor{teal}\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{teal}{x_0^m} &amp;amp; \textcolor{teal}{x_1^m} &amp;amp; \textcolor{teal}{x_2^m} &amp;amp; \dots &amp;amp; \textcolor{teal}{x_n^m}
\end{bmatrix} \begin{bmatrix} \textcolor{magenta}{θ_0} \\ \textcolor{magenta}{θ_1} \\ \textcolor{magenta}{θ_2} \\ \vdots \\ \textcolor{magenta}{θ_n} \end{bmatrix} \\
&amp;amp;= \begin{bmatrix}
  \textcolor{magenta}{θ_0}\textcolor{teal}{x_0^1} &amp;amp; \textcolor{magenta}{θ_1}\textcolor{teal}{x_1^1} &amp;amp; \textcolor{magenta}{θ_2}\textcolor{teal}{x_2^1} &amp;amp; \dots &amp;amp; \textcolor{magenta}{θ_n}\textcolor{teal}{x_n^1} \\[0.5em]
  \textcolor{magenta}{θ_0}\textcolor{teal}{x_0^2} &amp;amp; \textcolor{magenta}{θ_1}\textcolor{teal}{x_1^2} &amp;amp; \textcolor{magenta}{θ_2}\textcolor{teal}{x_2^2} &amp;amp; \dots &amp;amp; \textcolor{magenta}{n_2}\textcolor{teal}{x_n^2} \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  \textcolor{magenta}{θ_0}\textcolor{teal}{x_0^m} &amp;amp; \textcolor{magenta}{θ_1}\textcolor{teal}{x_1^m} &amp;amp; \textcolor{magenta}{θ_2}\textcolor{teal}{x_2^m} &amp;amp; \dots &amp;amp; \textcolor{magenta}{θ_n}\textcolor{teal}{x_n^m}
\end{bmatrix} \end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Here is the generalized form of linear regression before and after vectorization, followed by the vectorized NumPy version.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Notice how the non-vectorized version inherently refers to only one sample at a time, with the superscript &lt;span class="math"&gt;\(i\)&lt;/span&gt;. This implies the equation is true for each &lt;span class="math"&gt;\(i\)&lt;/span&gt;, but the vectorized version automatically includes every sample at the same time without us even having to know how many there are. So let’s use the plural &lt;code class="highlight"&gt;hypotheses&lt;/code&gt; to name the result.&lt;/li&gt;
&lt;li&gt;Look at how simple the math expression and the code become! (Of course, it does rely on a sufficient understanding of matrix multiplication.)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
h_θ(x^i) &amp;amp;= \textcolor{magenta}{θ_0}\textcolor{teal}{x_0^i} + \textcolor{magenta}{θ_1}\textcolor{teal}{x_1^i} + \cdots + \textcolor{magenta}{θ_n}\textcolor{teal}{x_n^i} \\
h_θ(\textbf X) &amp;amp;= \textbf X\vec θ
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hypotheses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="c1"&gt;# @ is short for matrix multiplication&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="vectorizing-the-cost-function-so-fresh-so-clean"&gt;Vectorizing the cost function: So fresh, so clean&lt;/h2&gt;
&lt;p&gt;This makes it easy to express the error as the difference between the hypothesis and the actual value:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textrm{error} &amp;amp;= \hat{Y}_i - Y_i \\
&amp;amp;= h_θ(x^i) - y_i \\
\vec e &amp;amp;= \textbf X\vec θ - \vec y
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hypotheses&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Recall that the cost function involves the sum of squared errors. In linear algebra, summation can be expressed as the product of a transposed vector of ones and a vector with the values to be summed, which struck me as a very clever manipulation.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\vec o = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
&amp;amp; \quad{} \vec e = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_m \end{bmatrix}
= \begin{bmatrix} h_θ(x^1) - y_1 \\ h_θ(x^2) - y_2 \\ \vdots \\ h_θ(x^m) - y_m \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\vec o^T\vec e &amp;amp;= \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_m \end{bmatrix} \\
&amp;amp;= \textcolor{lightgray}1 \cdot e_1 + \textcolor{lightgray}1 \cdot e_2 + \cdots + \textcolor{lightgray}1 \cdot e_m = \sum_{i=1}^m e_i
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;The cost function then becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
J(θ) &amp;amp;= \frac{1}{2m}\sum_{i=1}^m \Big[h_θ(x^i) - Y_i\Big]^2 \\
&amp;amp;= \frac{1}{2m}\sum_{i=1}^m {(e_i)}^2 \\
&amp;amp;= \frac{1}{2m} \vec o^T \vec e^2
\end{aligned} $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the NumPy function &lt;code class="highlight"&gt;np.square(z)&lt;/code&gt; is faster than &lt;code class="highlight"&gt;z ** 2&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="c1"&gt;# just a random big dataset for testing&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mf"&gt;7.99&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mf"&gt;1.19&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mf"&gt;7.01&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and &lt;code class="highlight"&gt;o.T @ np.square(z))&lt;/code&gt; (that is, &lt;span class="math"&gt;\(\vec o^T\textbf Z^2\)&lt;/span&gt;)  blows &lt;code class="highlight"&gt;sum(z ** 2)&lt;/code&gt; (that is, &lt;span class="math"&gt;\(\sum_{i=1}^m (z_i)^2\)&lt;/span&gt;) out of the water:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1.36&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mf"&gt;16.3&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.37&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mf"&gt;12.1&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;rando&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# because rando has more than 1 column&lt;/span&gt;
&lt;span class="mf"&gt;25.1&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mi"&gt;603&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, it’s helpful to turn our cost function into a Python function that takes &lt;span class="math"&gt;\(\textbf X, \vec y, \vec θ\)&lt;/span&gt; as its inputs. This will make it easier to evaluate our model later.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;cost_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 1x1 matrix&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# plain number&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="vectorizing-the-gradient-ninja-mode-on-overdrive"&gt;Vectorizing the gradient: Ninja mode on overdrive&lt;/h2&gt;
&lt;p&gt;On to the gradient. This is where linear algebra really kicks this thing into high gear.&lt;/p&gt;
&lt;p&gt;(This is also where I get to show off my LaTeX chops.)&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\nabla J = \frac{\partial J(θ)}{\partial θ_j} &amp;amp;= \Bigg\{\frac{\partial J(θ)}{\partial θ_0}, \frac{\partial J(θ)}{\partial θ_1}, \cdots, \frac{\partial J(θ)}{\partial θ_n}\Bigg\} \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m \Big[h_θ(x^i) - Y_i\Big]^2 \cdot x_j^i \qquad{} \textrm{for } 0 \leq j \leq n \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m {(e_i)}^2 x_j^i \qquad{}\qquad{}\qquad{}\qquad{}\qquad{}  " \\
&amp;amp;= \frac{1}{m} \sum_{i=1}^m x_j^i {(e_i)}^2 \qquad{}\qquad{}\qquad{}\qquad{}\qquad{}  " \\
&amp;amp;= \Bigg\{ \textcolor{teal}{
  \frac{1}{m} \sum_{i=1}^m x_0^i {(e_i)}^2, \frac{1}{m} \sum_{i=1}^m x_1^i {(e_i)}^2, \cdots, \frac{1}{m} \sum_{i=1}^m x_n^i {(e_i)}^2
  }\Bigg\}
\end{aligned} $$&lt;/div&gt;
&lt;p&gt;Transposing &lt;span class="math"&gt;\(\textbf X\)&lt;/span&gt; and squaring &lt;span class="math"&gt;\(\vec e\)&lt;/span&gt; gives us:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
\textbf X^T = \begin{bmatrix}
  x_0^1 &amp;amp; x_0^2 &amp;amp; \dots &amp;amp; x_0^m \\[0.5em]
  x_1^1 &amp;amp; x_1^2 &amp;amp; \dots &amp;amp; x_1^m \\[0.5em]
  \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\[0.5em]
  x_n^1 &amp;amp; x_n^2 &amp;amp; \dots  &amp;amp; x_n^m
\end{bmatrix} &amp;amp; \quad{} \vec e^2 = \begin{bmatrix} {(e_1)}^2 \\[0.5em] {(e_2)}^2 \\[0.5em] \vdots \\[0.5em] {(e_m)}^2 \end{bmatrix}
\end{aligned} $$&lt;/div&gt;
&lt;div class="math"&gt;$$ \textbf X^T \vec e^2 = \begin{bmatrix}
  x_0^1 {(e_1)}^2 + x_0^2 {(e_2)}^2 + x_0^3 {(e_3)}^2 + \cdots + x_0^m {(e_m)}^2 \\[0.5em]
  x_1^1 {(e_1)}^2 + x_1^2 {(e_2)}^2 + x_1^3 {(e_3)}^2 + \cdots + x_1^m {(e_m)}^2 \\[0.5em]
  \vdots \\[0.5em]
  x_n^1 {(e_1)}^2 + x_n^2 {(e_2)}^2 + x_n^3 {(e_3)}^2 + \cdots + x_n^m {(e_m)}^2
\end{bmatrix} = \begin{bmatrix}
  \sum_{i=1}^m x_0^i{(e_1)}^2 \\[0.5em]
  \sum_{i=1}^m x_1^i{(e_1)}^2 \\[0.5em]
  \vdots \\[0.5em]
  \sum_{i=1}^m x_n^i{(e_1)}^2 \\
\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Notice how multiplying this result by &lt;span class="math"&gt;\(\frac{1}{m}\)&lt;/span&gt; gives us a vector containing the same values highlighted above in teal.&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{1}{m} \textbf X^T \vec e^2 = \frac{1}{m} \begin{bmatrix}
  \sum_{i=1}^m x_0^i{(e_1)}^2 \\[0.5em]
  \sum_{i=1}^m x_1^i{(e_1)}^2 \\[0.5em]
  \vdots \\[0.5em]
  \sum_{i=1}^m x_n^i{(e_1)}^2 \\
\end{bmatrix} = \begin{bmatrix}
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_0^i{(e_1)}^2 } \\[0.5em]
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_1^i{(e_1)}^2 } \\[0.5em]
  \textcolor{teal}{ \vdots } \\[0.5em]
  \textcolor{teal}{ \frac{1}{m} \sum_{i=1}^m x_n^i{(e_1)}^2 } \\
\end{bmatrix} = \begin{bmatrix}
  \frac{\partial J(θ)}{\partial θ_0} \\[0.5em]
  \frac{\partial J(θ)}{\partial θ_1} \\[0.5em]
  \vdots \\[0.5em]
  \frac{\partial J(θ)}{\partial θ_n}
\end{bmatrix} $$&lt;/div&gt;
&lt;p&gt;Astonishingly, that gigantic mess can be expressed as&lt;/p&gt;
&lt;div class="math"&gt;$$ \nabla J = \frac{1}{m} \textbf X^T \vec e^2 $$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Finally&lt;/em&gt;, we can work out the last function, the gradient descent function:&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{gathered} \vec θ := \vec θ - α\frac{1}{m} \textbf X^T \vec e^2 \\
\textrm{repeat until convergence} \end{gathered} $$&lt;/div&gt;
&lt;p&gt;Hey, where’d that &lt;span class="math"&gt;\(α\)&lt;/span&gt; come from? That’s the &lt;strong&gt;learning rate&lt;/strong&gt;, a small number that adjusts the size of each training step. Too large and you jump right over the minimum; too small and you never reach the minimum.&lt;/p&gt;
&lt;p&gt;For now, let’s choose an arbitrary value for &lt;span class="math"&gt;\(α\)&lt;/span&gt; and disregard the whole bit about convergence. If we wanted to perform stochastic gradient descent with 100 steps, this is how we’d do it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let’s refactor this as a function &lt;code class="highlight"&gt;train_lr_model&lt;/code&gt; that takes &lt;span class="math"&gt;\(\textbf X, \vec y\)&lt;/span&gt;, and the number of steps (training epochs) as its inputs, and outputs the weights &lt;span class="math"&gt;\(\vec θ\)&lt;/span&gt;. Along the way, let’s have it tell us the cost. If all goes well, we should see that number approach zero as training progresses.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# dot products are single values, but NumPy returns them as 1x1 matrices&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;weights_300_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_lr_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we can predict the output for a random set of feature values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="n"&gt;mystery_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mystery_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;mystery_input&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# dummy feature&lt;/span&gt;
&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mystery_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_300_epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I like to geek out on math notation and wrote this function to generate LaTeX for the equation of the model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;expand_model_latex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{round(w[0], 2)}x_{i}"&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"$$ h_θ(x) &lt;/span&gt;&lt;span class="se"&gt;\a&lt;/span&gt;&lt;span class="s2"&gt;pprox {' + '.join(terms)} $$"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$ h_θ(x) \approx -2.36x_0 + 63.26x_1 + 61.15x_2 + 88.99x_3 + 0.82x_4 + 58.95x_5 $$&lt;/div&gt;
&lt;p&gt;We’re not done with linear regression, but let’s recap so that this post can end.&lt;/p&gt;
&lt;h2 id="summary-so-far"&gt;Summary so far&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;Broke&lt;/th&gt;
&lt;th align="center"&gt;Woke&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear regression model&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$h_θ(x) = θ_0x_0 + θ_1x_1 + \cdots + θ_nx_n$$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$h_θ(\textbf X) = \textbf X\vec θ$$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cost function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$J(θ) = \frac{1}{2m}\sum_{i=1}^m{\Big[h_θ(x^i) - y_i\Big]}^2$$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \begin{gathered} \vec e = h_θ{\textbf X} - \vec y \\ J(θ) = \frac{1}{2m}\vec o^T\vec e^2 \end{gathered} $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient of cost function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \frac{\partial J(θ)}{\partial θ_j} = \frac{1}{m}\sum_{i=1}^m\Big[h_θ(x^i) - y_i\Big]x_j^i $$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \nabla J = \frac{1}{m}\textbf X^T\vec e $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient descent function&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ θ_j := θ_j - α\frac{\partial J(θ)}{\partial θ_j} $$&lt;/div&gt;
&lt;/td&gt;
&lt;td align="center"&gt;
&lt;div class="math"&gt;$$ \vec θ := \vec θ - α\nabla J $$&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Such complex math can be applied to a linear regression model trained in some 20 lines of Python!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# padding for bias column&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# this just fixes a quirk of sklearn's output&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;cost_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="n"&gt;step_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_distance&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Completed {epochs} epochs of training."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Final cost: {cost(X, y, weights)}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="err"&gt;@&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="comparison-of-performance"&gt;Comparison of performance&lt;/h2&gt;
&lt;p&gt;This is a contrived comparison, but it helps to illustrate very clearly the point of jumping through all of these mathematical hoops.&lt;/p&gt;
&lt;p&gt;On the 500-sample, 5-feature dataset we’ve been using, the vectorized gradient descent function runs over &lt;strong&gt;7,000 times faster&lt;/strong&gt; than the terrible, monstrous, don’t-say-I-didn’t-warn-you procedural version from the beginning of this post.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;train_lr_model_procedurally&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;14.2&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mi"&gt;609&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;train_lr_model_vectorizedly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.00001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;2.28&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="mf"&gt;64.9&lt;/span&gt; &lt;span class="err"&gt;µ&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="err"&gt;±&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;runs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="remaining-questions-for-future-posts-links-will-be-included-upon-publication"&gt;Remaining questions for future posts (links will be included upon publication)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Are there any other simple techniques to speed this up? (Mean normalization)&lt;/li&gt;
&lt;li&gt;&lt;a href="squarest-root-in-babylon"&gt;Are there any complex techniques to speed this up? (Automatic differentiation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="from-zero-to-ero"&gt;How do we know when we’ve reached convergence? (Epsilon)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How can &lt;em&gt;linear&lt;/em&gt; (as opposed to &lt;em&gt;logistic&lt;/em&gt;) regression be applied to language tasks? (General backpropagation of neural networks)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;The Essence of Linear Algebra&lt;/a&gt; (video series), Grant Sanderson&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2"&gt;Linear Regression using Python&lt;/a&gt;, Animesh Agarwal&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ritchieng.com/multi-variable-linear-regression/"&gt;Linear Regression with Multiple Variables&lt;/a&gt;, Ritchie Ng&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb"&gt;Understanding and Calculating the Cost Function for Linear Regression&lt;/a&gt;, Lachlan Miller&lt;/li&gt;
&lt;li&gt;&lt;a href="http://anwarruff.com/the-linear-regression-cost-function-in-matrix-form/"&gt;The Linear Regression Cost Function in Matrix Form&lt;/a&gt;, Anwar Ruff&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version"&gt;Multivariable chain rule, simple version&lt;/a&gt;, Khan Academy&lt;/li&gt;
&lt;/ul&gt;</content><category term="koan"></category><category term="multivariable calculus"></category><category term="stochastic gradient descent"></category><category term="linear algebra"></category></entry></feed>