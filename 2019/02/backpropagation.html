<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, Iâ€™ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesnâ€™t follow suit quite soÂ easily!)" />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@tabidots" />
    <meta name="twitter:creator" content="@tabidots" />
    <meta name="twitter:title" content="djdw on the 0s andÂ 1s" />
    <meta name="twitter:description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, Iâ€™ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesnâ€™t follow suit quite soÂ easily!)" />
    <meta name="twitter:image:src" content="../../images/backprop/backprop_math.png" />
    <meta name="twitter:domain" content="judosaltgenius.com" />

    <meta property="og:title" content="djdw on the 0s andÂ 1s" />
    <meta property="og:type" content="article" />
    <meta property="og:description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, Iâ€™ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesnâ€™t follow suit quite soÂ easily!)" />
    <meta property="og:image" content="../../images/backprop/backprop_math.png" />
    <meta property="og:site_name" content="judosaltgenius.com" />
    <meta property="og:url" content="../../2019/02/backpropagation" />

        <link rel="alternate"  href="http://tabidots.github.io/feeds/all.atom.xml" type="application/atom+xml" title="Judo Salt Genius Full Atom Feed"/>

        <title>djdw on the 0s andÂ 1s - Judo Salt Genius</title>


    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css" integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js" integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css" />
    <link rel="stylesheet" href="../../theme/css/pure.css?v=0.1.0" />
      <!-- CSS specified by the user -->


      <link href="../../assets/mystyle.css" type="text/css" rel="stylesheet" />
    <link rel="stylesheet" href="../../theme/css/pygments.css" />

    <!-- for pelican_dynamic plugin -->
    <!-- end pelican_dynamic -->

</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
                    <a href="../../author/justin-douglas" title="See posts by Justin Douglas">
                        <img class="avatar" alt="Justin Douglas" src="https://www.gravatar.com/avatar/74f13134596b2ed04a497936e3fdfd33?s=140">
                    </a>
                <h2 class="article-info">Justin Douglas</h2>



                <p class="article-date">Tue 05 February 2019</p>

                <a class="header-article-home" href="/">&larr;Home</a>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>djdw on the 0s and&nbsp;1s</h1>
                </header>
            </section>

                <nav class="toc">
                <div class="toc">
<ul>
<li><a href="#forward-propagation">Forward propagation</a><ul>
<li><a href="#linear-regression-on-steroids">Linear regression onÂ steroids</a></li>
<li><a href="#sigmoid-sounds-like-steroid-but-isnt">Sigmoid (sounds like steroid, butÂ isnâ€™t)</a></li>
<li><a href="#mathing-it-out">Mathing itÂ out</a></li>
<li><a href="#implementing-it-recursively">Implementing itÂ recursively</a></li>
<li><a href="#testing-it-out">Testing itÂ out</a></li>
</ul>
</li>
<li><a href="#backpropagation">Backpropagation</a><ul>
<li><a href="#sigmoid-prime-brother-of-modulus-prime">Sigmoid Prime (brother of ModulusÂ Prime)</a></li>
<li><a href="#fracpartial-textbf-jpartial-textbf-w-on-the-turntable-or-something">\(\frac{\partial \textbf J}{\partial \textbf w}\) on the turntableâ€¦ orÂ something</a></li>
<li><a href="#chain-rule-forever-and-ever-and-ever-and">Chain rule forever and ever and everÂ andâ€¦</a></li>
<li><a href="#recursion-to-the-rescue">Recursion to theÂ rescue</a></li>
<li><a href="#testing-it-out_1">Testing itÂ out</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
                </nav>

            
<p>I got frustrated with not being able to go farther with using <code class="highlight">autodiff</code> for my simple toy linear regression model, so I decided it was time to actually try training a toy neural network for the firstÂ time.</p>
<p>For this, I followed Stephen Welchâ€™s excellent <a href="https://www.youtube.com/watch?v=bxe2T-V8XRs">Neural Networks Demystified</a> series. I had actually encountered the series a few years ago, which I recall gave me my initial intuitive understanding of neural networks, but I was not at all into math at that time and did not think I could grok the relevantÂ code.</p>
<p>The tutorial is pretty short and the end result is not <em>that</em> excitingâ€”at least for me, anyway. So I knew I wouldnâ€™t get much out of writing a post solely about theÂ tutorial.</p>
<p>However, in <a href="https://www.youtube.com/watch?v=GlcnxUlrtek">Video #4</a>, he breezes through backpropagationâ€”the heart of neural networksâ€”and while the tutorial gives enough of an understanding of the math to build similar neural networks to the ones in the video, with one hidden layer, I wanted to see if there was a tidy way to generalize his code to an arbitrary number of hiddenÂ layers.</p>
<p>So thatâ€™s what this post will be about, although it does skip a few intermediate steps from the last linear regression post,Â math-wise.</p>
<h1 id="forward-propagation">ForwardÂ propagation</h1>
<h2 id="linear-regression-on-steroids">Linear regression onÂ steroids</h2>
<p>Before you can do backpropagation, you need to do forward propagation. That is, you feed your inputs forward through each successive layer of the network until you get to the last layer, which is yourÂ output.</p>
<p>In the toy linear regression model from before, this was the process of getting an estimate <span class="math">\(\hat y\)</span> by initially setting the weights vector <span class="math">\(\vec \theta\)</span> to all zeros (or random values), and multiplying it by the input. The weights vector would then be updated and the input would be fed forward again on every iteration of gradientÂ descent.</p>
<p>In a neural network, itâ€™s basically the same, just more complex. The linear regression model consisted of <span class="math">\(j\)</span> features contributing to one output; a neural network, meanwhile could have arbitrary layers of an arbitrary number ofÂ features.</p>
<p>What was a single weights vector <span class="math">\(\vec \theta\)</span> is now one of several weights matrices <span class="math">\(\textbf w\)</span>. <a href="https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d">Erik HallstrÃ¶mâ€™s post</a> has some pretty clear graphics toÂ illustrate:</p>
<p><img src="../../images/backprop/weights_matrix.png" width="300"/></p>
<p>The <span class="math">\(j\)</span>th row of the matrix is the weights (influence) of the six neurons (features) of the preceding layer on the <span class="math">\(j\)</span>th neuron (feature) of the nextÂ layer.</p>
<p>Forward propagation is simply multiplying your input matrix by the first weight matrix, applying an activation function to the result. This result is the next input, which you multiply by the next weight matrix, and soÂ on.</p>
<h2 id="sigmoid-sounds-like-steroid-but-isnt">Sigmoid (sounds like steroid, butÂ isnâ€™t)</h2>
<p>Here, we are going to use the <strong>sigmoid function</strong>, <span class="math">\(\sigma(z)\)</span>, as our activation function. This is pretty standard in basic neural networkÂ implementations.</p>
<div class="math">$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$</div>
<p>Or, inÂ Python:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
<p><img src="../../images/backprop/sigmoid.png" width="300"/></p>
<p>Neat.</p>
<h2 id="mathing-it-out">Mathing itÂ out</h2>
<p>So then, given an input matrix <span class="math">\(\textbf X\)</span>, this is what a basic forward propagation sequence looksÂ like:</p>
<div class="math">$$ \begin{aligned}
\textbf z_1 &amp;:= \textbf X \textbf w_0 &amp;\to \textbf a_1 &amp;:= \sigma(\textbf z_1) \\
&amp; &amp; \textbf z_2 &amp;:= \textbf a_1 \textbf w_1 &amp;\to \textbf a_2 &amp;:= \sigma(\textbf z_2) \\
&amp; &amp; &amp; &amp; \hat y &amp;:= \textbf a_2
\end{aligned} $$</div>
<p>where <span class="math">\(\textbf z_n\)</span> are the â€œuncookedâ€ products of inputs and weights at layer <span class="math">\(n\)</span> and <span class="math">\(\textbf a_n\)</span> are the â€œcookedâ€ versions (activated values) of thoseÂ products.</p>
<p>You could write this as a single nested function, but like a programming one-liner, itâ€™s unreadable. I have chosen to write it this way to highlight its recursive nature (<em>ahem</em> <code class="highlight">reduce</code> <em>ahem</em>).</p>
<h2 id="implementing-it-recursively">Implementing itÂ recursively</h2>
<p>If we conceive of our networkâ€™s weights matrices as a list of matrices <code class="highlight">w</code> <span class="math">\(= [\textbf w_0, \textbf w_1, \cdots, \textbf w_{k-2}]\)</span> where <span class="math">\(k\)</span> is the number of layers in our network, we can then use <code class="highlight"><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> to refer to the weights between layers 0 and 1, and soÂ on.</p>
<p>(Notation such as <span class="math">\(W_{jk}\)</span> is common, but Iâ€™m using a single index to reduceÂ clutter.)</p>
<p>With that in mind, letâ€™s rewrite <code class="highlight"><span class="k">class</span> <span class="nc">NeuralNetwork</span></code> to take a list of layer dimensions, such that
we can recreate the Welch Labs 2-3-1 network with the list <code class="highlight">[2,3,1]</code> and <a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;index=4&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;t=0s">3blue1brownâ€™s number-recognition network</a> with the list <code class="highlight">[784,16,16,10]</code>.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/>
<label for="__tab_1_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_dims</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span> <span class="o">=</span> <span class="n">layer_dims</span>
        <span class="c1"># Initialize n-1 matrices containing random weights</span>
        <span class="c1"># Weight matrices must have rows like the prev layer</span>
        <span class="c1"># and columns like the next layer (otherwise you must transpose)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div></div>
<input id="__tab_1_1" name="__tabs_1" type="radio"/>
<label for="__tab_1_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span><span class="p">)</span>
</pre></div></div>
</div>
<p>With this flexible setup, forward propagation is just a matter of <code class="highlight">reduce</code>, as I alluded toÂ above:</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/>
<label for="__tab_2_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">prv</span><span class="p">,</span> <span class="n">nex</span><span class="p">:</span> <span class="n">prv</span> <span class="err">@</span> <span class="n">nex</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_2_1" name="__tabs_2" type="radio"/>
<label for="__tab_2_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">X</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">z3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">y_hat</span>
</pre></div></div>
</div>
<p>Look how concise thatÂ is!</p>
<p>However, thinking ahead, we will need to keep the intermediate values for when we do backpropagation, so letâ€™s flesh this out aÂ bit.</p>
<p>Similar to our <code class="highlight">w</code> list, we should instantiate an <code class="highlight">a</code> list <span class="math">\(= [\textbf a_0, \textbf a_1, \cdots, \textbf a_k]\)</span> and a <code class="highlight">z</code> list <span class="math">\(= [\textbf z_0, \textbf z_1, \cdots, \textbf z_k]\)</span> when we run theÂ function.</p>
<p>To make bookkeeping a little easier, letâ€™s add the necessary padding so that <code class="highlight"><span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> and <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> refer to <span class="math">\(\textbf z\)</span> and <span class="math">\(\textbf a\)</span> at layer <span class="math">\(n\)</span> (starting from zero). There is no â€œuncookedâ€ input, so <code class="highlight"><span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span></code>.</p>
<p>Meanwhile <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> is technically the initial input matrix and <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> is the output <span class="math">\(\hat y\)</span>.</p>
<h2 id="testing-it-out">Testing itÂ out</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span> <span class="err">@</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_input</span><span class="p">]</span> <span class="c1"># consider X as a_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>          <span class="c1"># keep the indexes of z and a aligned</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>
</pre></div>
<p>You can test this out with the Welch Labs toyÂ data:</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/>
<label for="__tab_3_0">Input</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="mi">75</span><span class="p">],[</span><span class="mi">82</span><span class="p">],[</span><span class="mi">93</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="c1"># normalize data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="mi">100</span>

<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">NN</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_3_1" name="__tabs_3" type="radio"/>
<label for="__tab_3_1">Output</label>
<div class="superfences-content"><div class="highlight"><pre><span></span>array([[0.58313228],
       [0.5781811 ],
       [0.59692924]])
</pre></div></div>
</div>
<p>The codeÂ works!</p>
<h1 id="backpropagation">Backpropagation</h1>
<p>Now that weâ€™ve fed our input <em>forward</em> through the network, we now need to take its output and propagate the error (discrepancy between the output and truth) <em>backward</em> through the network, to figure out how much to adjust the weightsÂ by.</p>
<p>This is where the calculus becomes quite messy compared to the toy linear regression model. Each successive layer in the network introduces a new layer of functions that must be chain-ruled through in order toÂ differentiate.</p>
<p>I am not really the best candidate to explain the nitty-gritty details of backpropagation. What follows will be  mainly my notes, which were enough to help me understand how to generalize the code to any number ofÂ layers.</p>
<h2 id="sigmoid-prime-brother-of-modulus-prime">Sigmoid Prime (brother of ModulusÂ Prime)</h2>
<p>If, as a high school student, I had known advanced math would turn into a cast of Transformers characters, I might have stuck with itÂ ğŸ˜‚</p>
<p>Anyway, before we go further, we have to define <span class="math">\(\sigma'(z)\)</span>, the derivative of our sigmoidÂ function.</p>
<div class="math">$$ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} $$</div>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
<p><img src="../../images/backprop/sigmoid_prime.png" width="300"/></p>
<p>Nice.</p>
<h2 id="fracpartial-textbf-jpartial-textbf-w-on-the-turntable-or-something"><span class="math">\(\frac{\partial \textbf J}{\partial \textbf w}\)</span> on the turntableâ€¦ orÂ something</h2>
<p>In the toy linear regression model, gradient descent required finding <span class="math">\(\frac{\partial J}{\partial \theta}\)</span>. In our neural network, gradient descent is going to require finding <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w}\)</span> for all <span class="math">\(\textbf w\)</span> in our list <code class="highlight">w</code>.</p>
<p>The tricky thing is that because the neural network is essentially functions layered on top of each other, the farther you go back toward the beginning of the network, the more complicated the derivativesÂ become.</p>
<p>You could think of this in the following way: Values farther toward the beginning of the network have a subtler effect on the final output, while values farther toward the output have a more direct effect on the final output, despite the fact that we cannot manipulate themÂ directly.</p>
<p>Here are the equations from the Welch Labs example, with the indices adjusted to reflect our zero-basedÂ setup.</p>
<div class="math">$$ \begin{aligned}
\textrm{error} &amp;= (\hat y - y) \\
\delta_2 &amp;= (\hat y - y) \sigma'(\textbf z_2) &amp;\to
\frac{\partial \textbf J}{\partial \textbf w_1} &amp;= \textbf a_2^T \delta_2 \\[0.8em]
\delta_1 &amp;= \delta_2 \textbf w_1^T \sigma'(\textbf z_1) &amp;\to
\frac{\partial \textbf J}{\partial \textbf w_0} &amp;= \textbf X^T \delta_1
\end{aligned} $$</div>
<p>Small delta <span class="math">\(\delta\)</span> (also a matrix), is referred to as the <strong>error signal</strong>. <span class="math">\(\delta_n\)</span> indicates how much the output changes when <span class="math">\(\textbf z_n\)</span>, the â€œuncookedâ€ values of layer <span class="math">\(n\)</span>,Â change.</p>
<p>My main question here was: Given that these two gradients are not exactly the same, what would they look like in a neural network with moreÂ layers?</p>
<h2 id="chain-rule-forever-and-ever-and-ever-and">Chain rule forever and ever and everÂ andâ€¦</h2>
<p>This looks really scary, but it was the only way I could figure out theÂ pattern.</p>
<p>Suppose we have a neural network with four layers (like the 3blue1brownÂ network).</p>
<div class="math">$$ \begin{aligned}
\frac{\partial \textbf J}{\partial \textbf w_2} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \hat y (=\textbf a_3)}{\partial \textbf z_3}}
\frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textcolor{#1f77b4}{\overbrace{(\hat y - y) \sigma'(\textbf z_3)}^{\delta_3}}
\frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textcolor{#1f77b4}{\delta_3} \frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textbf a_2^T \color{#1f77b4}{\delta_3} \\[1.2em]
\frac{\partial \textbf J}{\partial \textbf w_1} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \textbf a_3}{\partial \textbf z_3}}
\textcolor{#e377c2}{\frac{\partial \textbf z_3}{\partial \textbf a_2}
\frac{\partial \textbf a_2}{\partial \textbf z_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\overbrace{\textcolor{#1f77b4}{\overbrace{(\hat y - y) \sigma'(\textbf z_3)}^{\delta_3}} \frac{\partial \textbf z_3}{\partial \textbf a_2}
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\overbrace{\delta_3 \textbf w_2^T
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\delta_2} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textbf a_1^T \textcolor{#e377c2}{\delta_2} \\[1.2em]
\frac{\partial \textbf J}{\partial \textbf w_0} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \textbf a_3}{\partial \textbf z_3}} \textcolor{#e377c2}{\frac{\partial \textbf z_3}{\partial \textbf a_2}
\frac{\partial \textbf a_2}{\partial \textbf z_2}} \textcolor{mediumpurple}{\frac{\partial \textbf z_2}{\partial \textbf a_1}
\frac{\partial \textbf a_1}{\partial \textbf z_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\overbrace{\textcolor{#e377c2}{\overbrace{\textcolor{#1f77b4}{\delta_3} \textbf w_2^T
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf a_1}
\sigma'(\textbf z_1)}^{\delta_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\overbrace{\delta_2 \textbf w_1^T
\sigma'(\textbf z_1)}^{\delta_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\delta_1} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textbf a_0^T \textcolor{mediumpurple}{\delta_1} = \textbf X^T \textcolor{mediumpurple}{\delta_1}
\end{aligned} $$</div>
<p>This reveals a fewÂ generalities.</p>
<ol>
<li>
<p><span class="math">\(\frac{\partial \textbf a_n}{\partial \textbf z_n}\)</span>, or how much the â€œcookedâ€ values of a layer change in relation to the â€œuncookedâ€ values, is simply the derivative of the sigmoid function, since going from â€œuncookedâ€ to â€œcookedâ€ only involved the sigmoid function.
<div class="math">$$ \frac{\partial \textbf a_n}{\partial \textbf z_n} = \sigma'(\textbf z_n) $$</div>
</p>
</li>
<li>
<p><span class="math">\(\frac{\partial \textbf z_{n+1}}{\partial \textbf a_n}\)</span>, or how much a layerâ€™s â€œcookedâ€ values change the â€œuncookedâ€ values of the next layer, is simply the weights between the two layers. Makes sense, right? The weights matrix does have to be transposed for the multiplication to work, though.
<div class="math">$$ \frac{\partial \textbf z_{n+1}}{\partial \textbf a_n} = \textbf w_n^T $$</div>
</p>
</li>
<li>
<p>The first error signal, <span class="math">\(\textcolor{#1f77b4}{\delta_3}\)</span>, is different, but after that, you can find the error signal of any layer, <span class="math">\(\delta_n\)</span>, by piling on more of the above two partial derivatives:
<div class="math">$$ \delta_n = \delta_{n+1}
\frac{\partial \textbf z_{n+1}}{\partial \textbf a_n}
\frac{\partial \textbf a_n}{\partial \textbf z_n}
= \delta_{n+1} \textbf w_n^T \sigma'(\textbf z_n) $$</div>
</p>
</li>
<li>
<p>Finally, if we save all the error signals in a list <code class="highlight">deltas</code> <span class="math">\(= [\delta_k-1, \delta_2, \cdots, \delta_1]\)</span> (note that this will be the same length as <code class="highlight">w</code>, one shorter than <code class="highlight">a</code>, and <code class="highlight">z</code>), then obtaining <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w_n}\)</span> is just a matter of reversing <code class="highlight">deltas</code> and <code class="highlight">zip</code>ping it with <code class="highlight">w</code>.</p>
</li>
</ol>
<h2 id="recursion-to-the-rescue">Recursion to theÂ rescue</h2>
<p>In the four-layer example, <code class="highlight">deltas[0]</code> <span class="math">\(= \textcolor{#1f77b4}{\delta_3}\)</span>, so we can start offÂ defining</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
</pre></div>
<p>From there, we need to walk backwards through the lists <code class="highlight">w</code> and <code class="highlight">z</code>, starting from the second-to-last value of each, to get the values weÂ need.</p>
<p>Since we are reusing the newest <span class="math">\(\delta_n\)</span> result (i.e., <code class="highlight"><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code>) in each iteration, the loop goes roughly likeÂ this:</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="err">?</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="err">?</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="err">?</span><span class="p">])</span>
    <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
<p>Finally, once we have populated <code class="highlight">deltas</code>, then populating a list <code class="highlight">djdw</code> of all <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w_n}\)</span> can be done in oneÂ line:</p>
<div class="highlight"><pre><span></span><span class="n">djdw</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>
</pre></div>
<p>This works out perfectly,Â because</p>
<ol>
<li><code class="highlight"><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> is the input <span class="math">\(\textbf X\)</span>,Â and</li>
<li>even though <code class="highlight">a</code> is one longer than <code class="highlight">deltas</code>, <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code> is not used in these calculations and so <code class="highlight"><span class="nb">zip</span><span class="p">()</span></code> will disregardÂ it.</li>
</ol>
<p><code class="highlight">djdw</code> thus contains <span class="math">\([\textbf X^T \delta_1, \textbf a_1^T \delta_2, \textbf a_2^T \delta_3] = [\frac{\partial \textbf J}{\partial \textbf w_0}, \frac{\partial \textbf J}{\partial \textbf w_1}, \frac{\partial \textbf J}{\partial \textbf w_2}]\)</span>.</p>
<p>That gives us our final backpropagation function. Iâ€™ve condensed it a little here just to highlight the comparison with the hard-coded version, but the full version with comments is posted at theÂ bottom.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_4_0" name="__tabs_4" type="radio"/>
<label for="__tab_4_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ll</span><span class="p">):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>        

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">djdw</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">djdw</span> <span class="ow">in</span> <span class="n">NN</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)])</span>
</pre></div></div>
<input id="__tab_4_1" name="__tabs_4" type="radio"/>
<label for="__tab_4_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">cost_prime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">delta3</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta3</span> <span class="o">=</span> <span class="n">delta3</span>
        <span class="n">dJdW2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">delta3</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta3</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
        <span class="n">dJdW1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">delta2</span>
        <span class="k">return</span> <span class="n">dJdW1</span><span class="p">,</span> <span class="n">dJdW2</span>

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">dJdW1</span><span class="p">,</span> <span class="n">dJdW2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_prime</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dJdW1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">dJdW2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
</pre></div></div>
</div>
<p>Nice andÂ compact!</p>
<h2 id="testing-it-out_1">Testing itÂ out</h2>
<p>Letâ€™s generate a random dataset and network similar to the 3blue1brown one. That is, there should be input layer with 784 neurons, followed by 2 hidden layers and 1 output layer. His network is <code class="highlight"><span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span></code> but why not spice things up a bit and change the numbers around aÂ little?</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">784</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="nb">len</span><span class="p">(</span><span class="n">NN</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c1"># 26048</span>
</pre></div>
<p>Awesome! It works. And there are a whopping <strong>26,048 weights</strong> in this behemoth neural network.Â Wow.</p>
<p>With some minor adjustments to the <code class="highlight">get_params</code> and <code class="highlight">set_params</code> methods in the original class, we should be able to drop this right into the Welch Labs <code class="highlight">Trainer</code> as-is.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_5_0" name="__tabs_5" type="radio"/>
<label for="__tab_5_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">weight</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">num_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">num_weights</span><span class="p">],</span>
                                   <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">num_weights</span><span class="p">:]</span>
</pre></div></div>
<input id="__tab_5_1" name="__tabs_5" type="radio"/>
<label for="__tab_5_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#Get W1 and W2 unrolled into vector:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1">#Set W1 and W2 using single paramater vector.</span>
        <span class="n">W1_start</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">W1_end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">W1_start</span><span class="p">:</span><span class="n">W1_end</span><span class="p">],</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">))</span>
        <span class="n">W2_end</span> <span class="o">=</span> <span class="n">W1_end</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">W1_end</span><span class="p">:</span><span class="n">W2_end</span><span class="p">],</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span><span class="p">))</span>
</pre></div></div>
</div>
<p>Letâ€™s train it (See <code class="highlight">Trainer</code> tab in â€œSummaryâ€Â below.)!</p>
<p>Well, being a newbie at this, it wasnâ€™t quite so obvious to me, but it wasnâ€™t feasible to train such a gigantic neural network with non-industrial-strength tools. Even with smaller networks, unnormalized data easily caused overflowÂ errors.</p>
<p>This post is already quite long and complicated, and I donâ€™t want to stray from the topic of backpropagation by getting into numerical stability, so I decided to pare things down a bitÂ instead.</p>
<p>Still, the following network has five layers, which is reasonably complex and a further test of the mathÂ involved.</p>
<div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">NN</span><span class="p">)</span>
<span class="n">T</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p><img src="../../images/backprop/nn_train.png"/></p>
<p>Hooray! ItÂ works.</p>
<h1 id="summary">Summary</h1>
<div class="superfences-tabs">
<input checked="checked" id="__tab_6_0" name="__tabs_6" type="radio"/>
<label for="__tab_6_0">setup</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">NN</span><span class="p">)</span>
<span class="n">T</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_6_1" name="__tabs_6" type="radio"/>
<label for="__tab_6_1">NeuralNetwork</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_dims</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span> <span class="o">=</span> <span class="n">layer_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span> <span class="err">@</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_input</span><span class="p">]</span> <span class="c1"># consider X as a_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>          <span class="c1"># keep the indexes of z and a aligned</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="c1"># Accumulate Î´'s in reverse order</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ll</span><span class="p">):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>

        <span class="c1"># get djdws by multiplying the transpose of each activation by each delta</span>
        <span class="c1"># X.T @ Î´_1, a_1.T @ Î´_2, a_2.T @ Î´_3... (note that deltas[0] is Î´_1)</span>
        <span class="n">djdw</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">djdw</span>

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">djdw</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">djdw</span> <span class="ow">in</span> <span class="n">NN</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">J</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">J</span>

    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">weight</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">num_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">num_weights</span><span class="p">],</span>
                                   <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">num_weights</span><span class="p">:]</span>
</pre></div></div>
<input id="__tab_6_2" name="__tabs_6" type="radio"/>
<label for="__tab_6_2">Trainer</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>

<span class="k">class</span> <span class="nc">Trainer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span> <span class="c1"># unedited from Welch labs version</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>

    <span class="k">def</span> <span class="nf">cost_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">callback_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">J</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">J</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">params0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'maxiter'</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">'disp'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_wrapper</span><span class="p">,</span> <span class="n">params0</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">method</span><span class="o">=</span><span class="s1">'BFGS'</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callback_f</span><span class="p">)</span>
</pre></div></div>
</div>
<h1 id="references">References</h1>
<ul>
<li><a href="https://www.youtube.com/watch?v=bxe2T-V8XRs">Neural Networks Demystified</a>, WelchÂ Labs</li>
<li><a href="https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f">Multi-Layer Neural Networks with Sigmoid Functionâ€” Deep Learning for Rookies (2)</a>, NahuaÂ Kang</li>
</ul>

            <p class="post-footer">
                // filed
under                    <a class="post-category" href="../../tag/machine-learning">machine learning</a>
                    <a class="post-category" href="../../tag/neural-networks">neural networks</a>
                    <a class="post-category" href="../../tag/backpropagation">backpropagation</a>
                in <a class="post-category" href="../../category/math-programming">Math, Programming</a>&nbsp;&nbsp;&nbsp;

                <span style="display:inline-block;">
                // share on <a href="https://twitter.com/share?text=%22djdw%20on%20the%200s%20and%C2%A01s%3A%20After%20my%20previous%20fail%20trying%20to%20use%20autograd%20with%20pandas%2C%20I%20decided%20to%20set%20simple%20linear%20regression%20aside%20and%20attempt%20training%20a%20neural%20network.%20Here%2C%20I%E2%80%99ve%20adapted%20the...%22&amp;hashtags=machinelearning%2Cneuralnetworks%2Cbackpropagation" target="_blank">
                    <i class="fa fa-twitter fa-lg"></i> Twitter
                </a>
                </span>
            </p>
            <div class="hr"></div>


            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Justin Douglas  2019. Published with <a href="https://github.com/getpelican/pelican">Pelican</a>.<br />This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>
</footer>        </div>
    </div>
</div>
    <script>
        renderMathInElement(document.body);
    </script>

			<!-- Script specified by the user -->
			<script type="text/javascript"  src="../../assets/tweaks.js"></script>

    <!-- for pelican_dynamic plugin -->



</body>
</html>