<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, I’ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesn’t follow suit quite so easily!)" />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@tabidots" />
    <meta name="twitter:creator" content="@tabidots" />
    <meta name="twitter:title" content="djdw on the 0s and 1s" />
    <meta name="twitter:description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, I’ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesn’t follow suit quite so easily!)" />
    <meta name="twitter:image:src" content="../../images/backprop/backprop_math.png" />
    <meta name="twitter:domain" content="judosaltgenius.com" />

    <meta property="og:title" content="djdw on the 0s and 1s" />
    <meta property="og:type" content="article" />
    <meta property="og:description" content="After my previous fail trying to use autograd with pandas, I decided to set simple linear regression aside and attempt training a neural network. Here, I’ve adapted the network in the Welch Labs tutorial to accommodate an arbitrary number of layers. (However, floating-point arithmetic doesn’t follow suit quite so easily!)" />
    <meta property="og:image" content="../../images/backprop/backprop_math.png" />
    <meta property="og:site_name" content="judosaltgenius.com" />
    <meta property="og:url" content="../../2019/02/backpropagation" />

        <link rel="alternate"  href="http://tabidots.github.io/feeds/all.atom.xml" type="application/atom+xml" title="Judo Salt Genius Full Atom Feed"/>

        <title>djdw on the 0s and 1s - Judo Salt Genius</title>


    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css" integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js" integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css" />
    <link rel="stylesheet" href="../../theme/css/pure.css?v=0.1.0" />
      <!-- CSS specified by the user -->


      <link href="../../assets/mystyle.css" type="text/css" rel="stylesheet" />
    <link rel="stylesheet" href="../../theme/css/pygments.css" />

    <!-- for pelican_dynamic plugin -->
    <!-- end pelican_dynamic -->

</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
                    <a href="../../author/justin-douglas" title="See posts by Justin Douglas">
                        <img class="avatar" alt="Justin Douglas" src="https://www.gravatar.com/avatar/74f13134596b2ed04a497936e3fdfd33?s=140">
                    </a>
                <h2 class="article-info">Justin Douglas</h2>



                <p class="article-date">Tue 05 February 2019</p>

                <a class="header-article-home" href="/">&larr;Home</a>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>djdw on the 0s and&nbsp;1s</h1>
                </header>
            </section>

                <nav class="toc">
                <div class="toc">
<ul>
<li><a href="#forward-propagation">Forward propagation</a><ul>
<li><a href="#linear-regression-on-steroids">Linear regression on steroids</a></li>
<li><a href="#sigmoid-sounds-like-steroid-but-isnt">Sigmoid (sounds like steroid, but isn’t)</a></li>
<li><a href="#mathing-it-out">Mathing it out</a></li>
<li><a href="#implementing-it-recursively">Implementing it recursively</a></li>
<li><a href="#testing-it-out">Testing it out</a></li>
</ul>
</li>
<li><a href="#backpropagation">Backpropagation</a><ul>
<li><a href="#sigmoid-prime-brother-of-modulus-prime">Sigmoid Prime (brother of Modulus Prime)</a></li>
<li><a href="#fracpartial-textbf-jpartial-textbf-w-on-the-turntable-or-something">\(\frac{\partial \textbf J}{\partial \textbf w}\) on the turntable… or something</a></li>
<li><a href="#chain-rule-forever-and-ever-and-ever-and">Chain rule forever and ever and ever and…</a></li>
<li><a href="#recursion-to-the-rescue">Recursion to the rescue</a></li>
<li><a href="#testing-it-out_1">Testing it out</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
                </nav>

            
<p>I got frustrated with not being able to go farther with using <code class="highlight">autodiff</code> for my simple toy linear regression model, so I decided it was time to actually try training a toy neural network for the first time.</p>
<p>For this, I followed Stephen Welch’s excellent <a href="https://www.youtube.com/watch?v=bxe2T-V8XRs">Neural Networks Demystified</a> series. I had actually encountered the series a few years ago, which I recall gave me my initial intuitive understanding of neural networks, but I was not at all into math at that time and did not think I could grok the relevant code.</p>
<p>The tutorial is pretty short and the end result is not <em>that</em> exciting—at least for me, anyway. So I knew I wouldn’t get much out of writing a post solely about the tutorial.</p>
<p>However, in <a href="https://www.youtube.com/watch?v=GlcnxUlrtek">Video #4</a>, he breezes through backpropagation—the heart of neural networks—and while the tutorial gives enough of an understanding of the math to build similar neural networks to the ones in the video, with one hidden layer, I wanted to see if there was a tidy way to generalize his code to an arbitrary number of hidden layers.</p>
<p>So that’s what this post will be about, although it does skip a few intermediate steps from the last linear regression post, math-wise.</p>
<h1 id="forward-propagation">Forward propagation</h1>
<h2 id="linear-regression-on-steroids">Linear regression on steroids</h2>
<p>Before you can do backpropagation, you need to do forward propagation. That is, you feed your inputs forward through each successive layer of the network until you get to the last layer, which is your output.</p>
<p>In the toy linear regression model from before, this was the process of getting an estimate <span class="math">\(\hat y\)</span> by initially setting the weights vector <span class="math">\(\vec \theta\)</span> to all zeros (or random values), and multiplying it by the input. The weights vector would then be updated and the input would be fed forward again on every iteration of gradient descent.</p>
<p>In a neural network, it’s basically the same, just more complex. The linear regression model consisted of <span class="math">\(j\)</span> features contributing to one output; a neural network, meanwhile could have arbitrary layers of an arbitrary number of features.</p>
<p>What was a single weights vector <span class="math">\(\vec \theta\)</span> is now one of several weights matrices <span class="math">\(\textbf w\)</span>. <a href="https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d">Erik Hallström’s post</a> has some pretty clear graphics to illustrate:</p>
<p><img src="../../images/backprop/weights_matrix.png" width="300"/></p>
<p>The <span class="math">\(j\)</span>th row of the matrix is the weights (influence) of the six neurons (features) of the preceding layer on the <span class="math">\(j\)</span>th neuron (feature) of the next layer.</p>
<p>Forward propagation is simply multiplying your input matrix by the first weight matrix, applying an activation function to the result. This result is the next input, which you multiply by the next weight matrix, and so on.</p>
<h2 id="sigmoid-sounds-like-steroid-but-isnt">Sigmoid (sounds like steroid, but isn’t)</h2>
<p>Here, we are going to use the <strong>sigmoid function</strong>, <span class="math">\(\sigma(z)\)</span>, as our activation function. This is pretty standard in basic neural network implementations.</p>
<div class="math">$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$</div>
<p>Or, in Python:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
<p><img src="../../images/backprop/sigmoid.png" width="300"/></p>
<p>Neat.</p>
<h2 id="mathing-it-out">Mathing it out</h2>
<p>So then, given an input matrix <span class="math">\(\textbf X\)</span>, this is what a basic forward propagation sequence looks like:</p>
<div class="math">$$ \begin{aligned}
\textbf z_1 &amp;:= \textbf X \textbf w_0 &amp;\to \textbf a_1 &amp;:= \sigma(\textbf z_1) \\
&amp; &amp; \textbf z_2 &amp;:= \textbf a_1 \textbf w_1 &amp;\to \textbf a_2 &amp;:= \sigma(\textbf z_2) \\
&amp; &amp; &amp; &amp; \hat y &amp;:= \textbf a_2
\end{aligned} $$</div>
<p>where <span class="math">\(\textbf z_n\)</span> are the “uncooked” products of inputs and weights at layer <span class="math">\(n\)</span> and <span class="math">\(\textbf a_n\)</span> are the “cooked” versions (activated values) of those products.</p>
<p>You could write this as a single nested function, but like a programming one-liner, it’s unreadable. I have chosen to write it this way to highlight its recursive nature (<em>ahem</em> <code class="highlight">reduce</code> <em>ahem</em>).</p>
<h2 id="implementing-it-recursively">Implementing it recursively</h2>
<p>If we conceive of our network’s weights matrices as a list of matrices <code class="highlight">w</code> <span class="math">\(= [\textbf w_0, \textbf w_1, \cdots, \textbf w_{k-2}]\)</span> where <span class="math">\(k\)</span> is the number of layers in our network, we can then use <code class="highlight"><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> to refer to the weights between layers 0 and 1, and so on.</p>
<p>(Notation such as <span class="math">\(W_{jk}\)</span> is common, but I’m using a single index to reduce clutter.)</p>
<p>With that in mind, let’s rewrite <code class="highlight"><span class="k">class</span> <span class="nc">NeuralNetwork</span></code> to take a list of layer dimensions, such that
we can recreate the Welch Labs 2-3-1 network with the list <code class="highlight">[2,3,1]</code> and <a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;index=4&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;t=0s">3blue1brown’s number-recognition network</a> with the list <code class="highlight">[784,16,16,10]</code>.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_1_0" name="__tabs_1" type="radio"/>
<label for="__tab_1_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_dims</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span> <span class="o">=</span> <span class="n">layer_dims</span>
        <span class="c1"># Initialize n-1 matrices containing random weights</span>
        <span class="c1"># Weight matrices must have rows like the prev layer</span>
        <span class="c1"># and columns like the next layer (otherwise you must transpose)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div></div>
<input id="__tab_1_1" name="__tabs_1" type="radio"/>
<label for="__tab_1_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span><span class="p">)</span>
</pre></div></div>
</div>
<p>With this flexible setup, forward propagation is just a matter of <code class="highlight">reduce</code>, as I alluded to above:</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_2_0" name="__tabs_2" type="radio"/>
<label for="__tab_2_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">prv</span><span class="p">,</span> <span class="n">nex</span><span class="p">:</span> <span class="n">prv</span> <span class="err">@</span> <span class="n">nex</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_2_1" name="__tabs_2" type="radio"/>
<label for="__tab_2_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">X</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">z3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">y_hat</span>
</pre></div></div>
</div>
<p>Look how concise that is!</p>
<p>However, thinking ahead, we will need to keep the intermediate values for when we do backpropagation, so let’s flesh this out a bit.</p>
<p>Similar to our <code class="highlight">w</code> list, we should instantiate an <code class="highlight">a</code> list <span class="math">\(= [\textbf a_0, \textbf a_1, \cdots, \textbf a_k]\)</span> and a <code class="highlight">z</code> list <span class="math">\(= [\textbf z_0, \textbf z_1, \cdots, \textbf z_k]\)</span> when we run the function.</p>
<p>To make bookkeeping a little easier, let’s add the necessary padding so that <code class="highlight"><span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> and <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> refer to <span class="math">\(\textbf z\)</span> and <span class="math">\(\textbf a\)</span> at layer <span class="math">\(n\)</span> (starting from zero). There is no “uncooked” input, so <code class="highlight"><span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span></code>.</p>
<p>Meanwhile <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> is technically the initial input matrix and <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span></code> is the output <span class="math">\(\hat y\)</span>.</p>
<h2 id="testing-it-out">Testing it out</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span> <span class="err">@</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_input</span><span class="p">]</span> <span class="c1"># consider X as a_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>          <span class="c1"># keep the indexes of z and a aligned</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>
</pre></div>
<p>You can test this out with the Welch Labs toy data:</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_3_0" name="__tabs_3" type="radio"/>
<label for="__tab_3_0">Input</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="mi">75</span><span class="p">],[</span><span class="mi">82</span><span class="p">],[</span><span class="mi">93</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="c1"># normalize data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="mi">100</span>

<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">NN</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_3_1" name="__tabs_3" type="radio"/>
<label for="__tab_3_1">Output</label>
<div class="superfences-content"><div class="highlight"><pre><span></span>array([[0.58313228],
       [0.5781811 ],
       [0.59692924]])
</pre></div></div>
</div>
<p>The code works!</p>
<h1 id="backpropagation">Backpropagation</h1>
<p>Now that we’ve fed our input <em>forward</em> through the network, we now need to take its output and propagate the error (discrepancy between the output and truth) <em>backward</em> through the network, to figure out how much to adjust the weights by.</p>
<p>This is where the calculus becomes quite messy compared to the toy linear regression model. Each successive layer in the network introduces a new layer of functions that must be chain-ruled through in order to differentiate.</p>
<p>I am not really the best candidate to explain the nitty-gritty details of backpropagation. What follows will be  mainly my notes, which were enough to help me understand how to generalize the code to any number of layers.</p>
<h2 id="sigmoid-prime-brother-of-modulus-prime">Sigmoid Prime (brother of Modulus Prime)</h2>
<p>If, as a high school student, I had known advanced math would turn into a cast of Transformers characters, I might have stuck with it 😂</p>
<p>Anyway, before we go further, we have to define <span class="math">\(\sigma'(z)\)</span>, the derivative of our sigmoid function.</p>
<div class="math">$$ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} $$</div>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
<p><img src="../../images/backprop/sigmoid_prime.png" width="300"/></p>
<p>Nice.</p>
<h2 id="fracpartial-textbf-jpartial-textbf-w-on-the-turntable-or-something"><span class="math">\(\frac{\partial \textbf J}{\partial \textbf w}\)</span> on the turntable… or something</h2>
<p>In the toy linear regression model, gradient descent required finding <span class="math">\(\frac{\partial J}{\partial \theta}\)</span>. In our neural network, gradient descent is going to require finding <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w}\)</span> for all <span class="math">\(\textbf w\)</span> in our list <code class="highlight">w</code>.</p>
<p>The tricky thing is that because the neural network is essentially functions layered on top of each other, the farther you go back toward the beginning of the network, the more complicated the derivatives become.</p>
<p>You could think of this in the following way: Values farther toward the beginning of the network have a subtler effect on the final output, while values farther toward the output have a more direct effect on the final output, despite the fact that we cannot manipulate them directly.</p>
<p>Here are the equations from the Welch Labs example, with the indices adjusted to reflect our zero-based setup.</p>
<div class="math">$$ \begin{aligned}
\textrm{error} &amp;= (\hat y - y) \\
\delta_2 &amp;= (\hat y - y) \sigma'(\textbf z_2) &amp;\to
\frac{\partial \textbf J}{\partial \textbf w_1} &amp;= \textbf a_2^T \delta_2 \\[0.8em]
\delta_1 &amp;= \delta_2 \textbf w_1^T \sigma'(\textbf z_1) &amp;\to
\frac{\partial \textbf J}{\partial \textbf w_0} &amp;= \textbf X^T \delta_1
\end{aligned} $$</div>
<p>Small delta <span class="math">\(\delta\)</span> (also a matrix), is referred to as the <strong>error signal</strong>. <span class="math">\(\delta_n\)</span> indicates how much the output changes when <span class="math">\(\textbf z_n\)</span>, the “uncooked” values of layer <span class="math">\(n\)</span>, change.</p>
<p>My main question here was: Given that these two gradients are not exactly the same, what would they look like in a neural network with more layers?</p>
<h2 id="chain-rule-forever-and-ever-and-ever-and">Chain rule forever and ever and ever and…</h2>
<p>This looks really scary, but it was the only way I could figure out the pattern.</p>
<p>Suppose we have a neural network with four layers (like the 3blue1brown network).</p>
<div class="math">$$ \begin{aligned}
\frac{\partial \textbf J}{\partial \textbf w_2} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \hat y (=\textbf a_3)}{\partial \textbf z_3}}
\frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textcolor{#1f77b4}{\overbrace{(\hat y - y) \sigma'(\textbf z_3)}^{\delta_3}}
\frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textcolor{#1f77b4}{\delta_3} \frac{\partial \textbf z_3}{\partial \textbf w_2} \\[0.8em]
&amp;= \textbf a_2^T \color{#1f77b4}{\delta_3} \\[1.2em]
\frac{\partial \textbf J}{\partial \textbf w_1} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \textbf a_3}{\partial \textbf z_3}}
\textcolor{#e377c2}{\frac{\partial \textbf z_3}{\partial \textbf a_2}
\frac{\partial \textbf a_2}{\partial \textbf z_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\overbrace{\textcolor{#1f77b4}{\overbrace{(\hat y - y) \sigma'(\textbf z_3)}^{\delta_3}} \frac{\partial \textbf z_3}{\partial \textbf a_2}
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\overbrace{\delta_3 \textbf w_2^T
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textcolor{#e377c2}{\delta_2} \frac{\partial \textbf z_2}{\partial \textbf w_1} \\[0.8em]
&amp;= \textbf a_1^T \textcolor{#e377c2}{\delta_2} \\[1.2em]
\frac{\partial \textbf J}{\partial \textbf w_0} &amp;=
\textcolor{#1f77b4}{(\hat y - y) \frac{\partial \textbf a_3}{\partial \textbf z_3}} \textcolor{#e377c2}{\frac{\partial \textbf z_3}{\partial \textbf a_2}
\frac{\partial \textbf a_2}{\partial \textbf z_2}} \textcolor{mediumpurple}{\frac{\partial \textbf z_2}{\partial \textbf a_1}
\frac{\partial \textbf a_1}{\partial \textbf z_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\overbrace{\textcolor{#e377c2}{\overbrace{\textcolor{#1f77b4}{\delta_3} \textbf w_2^T
\sigma'(\textbf z_2)}^{\delta_2}} \frac{\partial \textbf z_2}{\partial \textbf a_1}
\sigma'(\textbf z_1)}^{\delta_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\overbrace{\delta_2 \textbf w_1^T
\sigma'(\textbf z_1)}^{\delta_1}} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textcolor{mediumpurple}{\delta_1} \frac{\partial \textbf z_1}{\partial \textbf w_0} \\[0.8em]
&amp;= \textbf a_0^T \textcolor{mediumpurple}{\delta_1} = \textbf X^T \textcolor{mediumpurple}{\delta_1}
\end{aligned} $$</div>
<p>This reveals a few generalities.</p>
<ol>
<li>
<p><span class="math">\(\frac{\partial \textbf a_n}{\partial \textbf z_n}\)</span>, or how much the “cooked” values of a layer change in relation to the “uncooked” values, is simply the derivative of the sigmoid function, since going from “uncooked” to “cooked” only involved the sigmoid function.
<div class="math">$$ \frac{\partial \textbf a_n}{\partial \textbf z_n} = \sigma'(\textbf z_n) $$</div>
</p>
</li>
<li>
<p><span class="math">\(\frac{\partial \textbf z_{n+1}}{\partial \textbf a_n}\)</span>, or how much a layer’s “cooked” values change the “uncooked” values of the next layer, is simply the weights between the two layers. Makes sense, right? The weights matrix does have to be transposed for the multiplication to work, though.
<div class="math">$$ \frac{\partial \textbf z_{n+1}}{\partial \textbf a_n} = \textbf w_n^T $$</div>
</p>
</li>
<li>
<p>The first error signal, <span class="math">\(\textcolor{#1f77b4}{\delta_3}\)</span>, is different, but after that, you can find the error signal of any layer, <span class="math">\(\delta_n\)</span>, by piling on more of the above two partial derivatives:
<div class="math">$$ \delta_n = \delta_{n+1}
\frac{\partial \textbf z_{n+1}}{\partial \textbf a_n}
\frac{\partial \textbf a_n}{\partial \textbf z_n}
= \delta_{n+1} \textbf w_n^T \sigma'(\textbf z_n) $$</div>
</p>
</li>
<li>
<p>Finally, if we save all the error signals in a list <code class="highlight">deltas</code> <span class="math">\(= [\delta_k-1, \delta_2, \cdots, \delta_1]\)</span> (note that this will be the same length as <code class="highlight">w</code>, one shorter than <code class="highlight">a</code>, and <code class="highlight">z</code>), then obtaining <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w_n}\)</span> is just a matter of reversing <code class="highlight">deltas</code> and <code class="highlight">zip</code>ping it with <code class="highlight">w</code>.</p>
</li>
</ol>
<h2 id="recursion-to-the-rescue">Recursion to the rescue</h2>
<p>In the four-layer example, <code class="highlight">deltas[0]</code> <span class="math">\(= \textcolor{#1f77b4}{\delta_3}\)</span>, so we can start off defining</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
</pre></div>
<p>From there, we need to walk backwards through the lists <code class="highlight">w</code> and <code class="highlight">z</code>, starting from the second-to-last value of each, to get the values we need.</p>
<p>Since we are reusing the newest <span class="math">\(\delta_n\)</span> result (i.e., <code class="highlight"><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code>) in each iteration, the loop goes roughly like this:</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="err">?</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="err">?</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="err">?</span><span class="p">])</span>
    <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
<p>Finally, once we have populated <code class="highlight">deltas</code>, then populating a list <code class="highlight">djdw</code> of all <span class="math">\(\frac{\partial \textbf J}{\partial \textbf w_n}\)</span> can be done in one line:</p>
<div class="highlight"><pre><span></span><span class="n">djdw</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>
</pre></div>
<p>This works out perfectly, because</p>
<ol>
<li><code class="highlight"><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> is the input <span class="math">\(\textbf X\)</span>, and</li>
<li>even though <code class="highlight">a</code> is one longer than <code class="highlight">deltas</code>, <code class="highlight"><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code> is not used in these calculations and so <code class="highlight"><span class="nb">zip</span><span class="p">()</span></code> will disregard it.</li>
</ol>
<p><code class="highlight">djdw</code> thus contains <span class="math">\([\textbf X^T \delta_1, \textbf a_1^T \delta_2, \textbf a_2^T \delta_3] = [\frac{\partial \textbf J}{\partial \textbf w_0}, \frac{\partial \textbf J}{\partial \textbf w_1}, \frac{\partial \textbf J}{\partial \textbf w_2}]\)</span>.</p>
<p>That gives us our final backpropagation function. I’ve condensed it a little here just to highlight the comparison with the hard-coded version, but the full version with comments is posted at the bottom.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_4_0" name="__tabs_4" type="radio"/>
<label for="__tab_4_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ll</span><span class="p">):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>        

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">djdw</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">djdw</span> <span class="ow">in</span> <span class="n">NN</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)])</span>
</pre></div></div>
<input id="__tab_4_1" name="__tabs_4" type="radio"/>
<label for="__tab_4_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">cost_prime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">delta3</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta3</span> <span class="o">=</span> <span class="n">delta3</span>
        <span class="n">dJdW2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">delta3</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta3</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
        <span class="n">dJdW1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">delta2</span>
        <span class="k">return</span> <span class="n">dJdW1</span><span class="p">,</span> <span class="n">dJdW2</span>

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">dJdW1</span><span class="p">,</span> <span class="n">dJdW2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_prime</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dJdW1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">dJdW2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
</pre></div></div>
</div>
<p>Nice and compact!</p>
<h2 id="testing-it-out_1">Testing it out</h2>
<p>Let’s generate a random dataset and network similar to the 3blue1brown one. That is, there should be input layer with 784 neurons, followed by 2 hidden layers and 1 output layer. His network is <code class="highlight"><span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span></code> but why not spice things up a bit and change the numbers around a little?</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">784</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="nb">len</span><span class="p">(</span><span class="n">NN</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c1"># 26048</span>
</pre></div>
<p>Awesome! It works. And there are a whopping <strong>26,048 weights</strong> in this behemoth neural network. Wow.</p>
<p>With some minor adjustments to the <code class="highlight">get_params</code> and <code class="highlight">set_params</code> methods in the original class, we should be able to drop this right into the Welch Labs <code class="highlight">Trainer</code> as-is.</p>
<div class="superfences-tabs">
<input checked="checked" id="__tab_5_0" name="__tabs_5" type="radio"/>
<label for="__tab_5_0">Flexible</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">weight</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">num_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">num_weights</span><span class="p">],</span>
                                   <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">num_weights</span><span class="p">:]</span>
</pre></div></div>
<input id="__tab_5_1" name="__tabs_5" type="radio"/>
<label for="__tab_5_1">Hard-coded</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#Get W1 and W2 unrolled into vector:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1">#Set W1 and W2 using single paramater vector.</span>
        <span class="n">W1_start</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">W1_end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">W1_start</span><span class="p">:</span><span class="n">W1_end</span><span class="p">],</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer_size</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">))</span>
        <span class="n">W2_end</span> <span class="o">=</span> <span class="n">W1_end</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">W1_end</span><span class="p">:</span><span class="n">W2_end</span><span class="p">],</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer_size</span><span class="p">))</span>
</pre></div></div>
</div>
<p>Let’s train it (See <code class="highlight">Trainer</code> tab in “Summary” below.)!</p>
<p>Well, being a newbie at this, it wasn’t quite so obvious to me, but it wasn’t feasible to train such a gigantic neural network with non-industrial-strength tools. Even with smaller networks, unnormalized data easily caused overflow errors.</p>
<p>This post is already quite long and complicated, and I don’t want to stray from the topic of backpropagation by getting into numerical stability, so I decided to pare things down a bit instead.</p>
<p>Still, the following network has five layers, which is reasonably complex and a further test of the math involved.</p>
<div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">NN</span><span class="p">)</span>
<span class="n">T</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p><img src="../../images/backprop/nn_train.png"/></p>
<p>Hooray! It works.</p>
<h1 id="summary">Summary</h1>
<div class="superfences-tabs">
<input checked="checked" id="__tab_6_0" name="__tabs_6" type="radio"/>
<label for="__tab_6_0">setup</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
<span class="n">NN</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">NN</span><span class="p">)</span>
<span class="n">T</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div></div>
<input id="__tab_6_1" name="__tabs_6" type="radio"/>
<label for="__tab_6_1">NeuralNetwork</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_dims</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span> <span class="o">=</span> <span class="n">layer_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span> <span class="err">@</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_input</span><span class="p">]</span> <span class="c1"># consider X as a_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>          <span class="c1"># keep the indexes of z and a aligned</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">initial_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">initial_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="c1"># Accumulate δ's in reverse order</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ll</span><span class="p">):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">ll</span> <span class="o">-</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>

        <span class="c1"># get djdws by multiplying the transpose of each activation by each delta</span>
        <span class="c1"># X.T @ δ_1, a_1.T @ δ_2, a_2.T @ δ_3... (note that deltas[0] is δ_1)</span>
        <span class="n">djdw</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">deltas</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">djdw</span>

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">djdw</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">djdw</span> <span class="ow">in</span> <span class="n">NN</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">J</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">J</span>

    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">weight</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">num_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">num_weights</span><span class="p">],</span>
                                   <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">num_weights</span><span class="p">:]</span>
</pre></div></div>
<input id="__tab_6_2" name="__tabs_6" type="radio"/>
<label for="__tab_6_2">Trainer</label>
<div class="superfences-content"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>

<span class="k">class</span> <span class="nc">Trainer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span> <span class="c1"># unedited from Welch labs version</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>

    <span class="k">def</span> <span class="nf">cost_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">callback_f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">J</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">cost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">J</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">params0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'maxiter'</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">'disp'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_wrapper</span><span class="p">,</span> <span class="n">params0</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">method</span><span class="o">=</span><span class="s1">'BFGS'</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callback_f</span><span class="p">)</span>
</pre></div></div>
</div>
<h1 id="references">References</h1>
<ul>
<li><a href="https://www.youtube.com/watch?v=bxe2T-V8XRs">Neural Networks Demystified</a>, Welch Labs</li>
<li><a href="https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f">Multi-Layer Neural Networks with Sigmoid Function— Deep Learning for Rookies (2)</a>, Nahua Kang</li>
</ul>

            <p class="post-footer">
                // filed
under                    <a class="post-category" href="../../tag/machine-learning">machine learning</a>
                    <a class="post-category" href="../../tag/neural-networks">neural networks</a>
                    <a class="post-category" href="../../tag/backpropagation">backpropagation</a>
                in <a class="post-category" href="../../category/math-programming">Math, Programming</a>&nbsp;&nbsp;&nbsp;

                <span style="display:inline-block;">
                // share on <a href="https://twitter.com/share?text=%22djdw%20on%20the%200s%20and%C2%A01s%3A%20After%20my%20previous%20fail%20trying%20to%20use%20autograd%20with%20pandas%2C%20I%20decided%20to%20set%20simple%20linear%20regression%20aside%20and%20attempt%20training%20a%20neural%20network.%20Here%2C%20I%E2%80%99ve%20adapted%20the...%22&amp;hashtags=machinelearning%2Cneuralnetworks%2Cbackpropagation" target="_blank">
                    <i class="fa fa-twitter fa-lg"></i> Twitter
                </a>
                </span>
            </p>
            <div class="hr"></div>


            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Justin Douglas  2019. Published with <a href="https://github.com/getpelican/pelican">Pelican</a>.<br />This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>
</footer>        </div>
    </div>
</div>
    <script>
        renderMathInElement(document.body);
    </script>

			<!-- Script specified by the user -->
			<script type="text/javascript"  src="../../assets/tweaks.js"></script>

    <!-- for pelican_dynamic plugin -->



</body>
</html>