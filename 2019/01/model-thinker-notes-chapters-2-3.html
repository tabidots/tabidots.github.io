<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Getting reacquainted with statistics for the first time since my ill-fated stint in introductory statistics in college." />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@tabidots" />
    <meta name="twitter:creator" content="@tabidots" />
    <meta name="twitter:title" content="Model Thinker notes, Ch. 2 and 3" />
    <meta name="twitter:description" content="Getting reacquainted with statistics for the first time since my ill-fated stint in introductory statistics in college." />
    <meta name="twitter:image:src" content="../../images/model_thinker_cover.jpg" />
    <meta name="twitter:domain" content="judosaltgenius.com" />

    <meta property="og:title" content="Model Thinker notes, Ch. 2 and 3" />
    <meta property="og:type" content="article" />
    <meta property="og:description" content="Getting reacquainted with statistics for the first time since my ill-fated stint in introductory statistics in college." />
    <meta property="og:image" content="../../images/model_thinker_cover.jpg" />
    <meta property="og:site_name" content="judosaltgenius.com" />
    <meta property="og:url" content="../../2019/01/model-thinker-notes-chapters-2-3" />

        <link rel="alternate"  href="http://tabidots.github.io/feeds/all.atom.xml" type="application/atom+xml" title="Judo Salt Genius Full Atom Feed"/>

        <title>Model Thinker notes, Ch. 2 and 3 - Judo Salt Genius</title>


    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css" integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js" integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css" />
    <link rel="stylesheet" href="../../theme/css/pure.css?v=0.1.0" />
      <!-- CSS specified by the user -->


      <link href="../../assets/mystyle.css" type="text/css" rel="stylesheet" />
    <link rel="stylesheet" href="../../theme/css/pygments.css" />

    <!-- for pelican_dynamic plugin -->
    <!-- end pelican_dynamic -->

</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
                    <a href="../../author/justin-douglas" title="See posts by Justin Douglas">
                        <img class="avatar" alt="Justin Douglas" src="https://www.gravatar.com/avatar/74f13134596b2ed04a497936e3fdfd33?s=140">
                    </a>
                <h2 class="article-info">Justin Douglas</h2>



                <p class="article-date">Sun 20 January 2019</p>

                <a class="header-article-home" href="/">&larr;Home</a>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Model Thinker notes, Ch. 2 and&nbsp;3</h1>
                </header>
            </section>

                <nav class="toc">
                <div class="toc">
<ul>
<li><a href="#chapter-2">Chapter 2</a></li>
<li><a href="#chapter-3">Chapter 3</a><ul>
<li><a href="#diversity">Diversity</a></li>
<li><a href="#accuracy">Accuracy</a><ul>
<li><a href="#bias">Bias</a></li>
<li><a href="#variance">Variance</a></li>
</ul>
</li>
<li><a href="#we-all-want-what-we-cant-have">We all want what we can’t have</a></li>
<li><a href="#a-more-intuitive-explanation-of-r2">A more intuitive explanation of \(R^2\)</a></li>
<li><a href="#model-error-decomposition-theorem">Model error decomposition theorem</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
                </nav>

            
<p>Along with <a href="http://www.pimbook.org/"><em>A Programmer’s Introduction to Mathematics</em></a>, I started reading <a href="https://www.amazon.com/Model-Thinker-What-Need-Know/dp/0465094627/ref=zg_bs_13884_41?_encoding=UTF8&amp;psc=1&amp;refRID=ZK3QGA2250Q3CJSWR8Q2"><em>Model Thinker: What You Need to Know to Make Data Work for You</em></a> as a sort of analogous non-statistician’s introduction to statistics.</p>
<p>Or perhaps I should say <em>re</em>-introduction, since I had to take Intro to Stats in college. But I passed that class by the skin of my teeth and really developed an aversion to statistical analysis from that course. But it’s a necessary part of data science, so…</p>
<h1 id="chapter-2">Chapter 2</h1>
<p>Chapter 2 is about what models can do and their strength in numbers.</p>
<ul>
<li>
<p>Models can take one of three approaches:</p>
<ul>
<li><em>embodiment</em> approach (simplified but still realistic, like a geological model)</li>
<li><em>analogy</em> approach (abstracted from reality: <em>Think of the situation as a…</em>)</li>
<li><em>alternative reality</em> approach</li>
</ul>
</li>
<li>
<p>Models must be <em>communicable</em> and <em>tractable</em> (analyzable). This means that it should be possible to translate them precisely into math, code, or some other formal language. (Most models are mathematical in nature, but this is not a must.)</p>
</li>
<li>
<p>All models are wrong in their own way, so no single model can accurately represent complex phenomena, but many, together, are useful.</p>
</li>
<li>
<p>Models are a superhero <em><span class="caps">REDCAPE</span></em>: Reason, explain, design, communicate, action, predict, explore.</p>
</li>
<li>
<p>Many models can be applied to the same problem (many-to-one). And with <em>creativity</em>, one model can also be put to many uses. For example, how many applications are there for a random walk? (What a relief to know that there is a place at the table for creativity and a non-<span class="caps">STEM</span> background!)</p>
</li>
</ul>
<h1 id="chapter-3">Chapter 3</h1>
<p>Chapter 3 is about how there is a Goldilocks-esque balance to be struck with regard to the diversity of models and the accuracy of any given model.</p>
<p>More is better, until it’s not; more accurate is better, until it’s not.</p>
<p>The chapter uses some fancy-looking equations dressed up in terms that are specific to this context, but the equations are actually rooted in statistics and often have simpler, more intuitive explanations.</p>
<h2 id="diversity">Diversity</h2>
<ul>
<li>
<p><strong>Condorcet jury theorem</strong>: If all jury members in a case have an above-average probability of choosing the correct verdict, then each successive jury member after the first increases the probability of the whole jury choosing the correct verdict. The probability approaches 1 as you keep adding members.</p>
</li>
<li>
<p><strong>Wisdom of the crowd</strong>: The author’s <em>model diversity theorem</em> is an adaptation of <a href="https://www.archania.org/theorems/diversity/">the wisdom of the crowd theorem</a>, and it uses the <em>error</em> as we calculated in linear regression. (Actually, all of the equations in this chapter are based on a mean squared error of some sort.) The book presents it as follows:
  <br/>
<div class="math">$$ \underbrace{(\bar{M} - V)^2}_{\textrm{Many-Model Error}} =
  \underbrace{\sum_{i=1}^N \frac{(M_i - V)^2}{N}}_{\textrm{Average-Model Error}} -
  \underbrace{\textcolor{teal}{\sum_{i=1}^N} \frac{\textcolor{teal}{(M_i - \bar{M})^2}}{N}}_{\textrm{Diversity of Model Predictions}} $$</div>
<br/>
  where <span class="math">\(M_i\)</span> is the prediction of model <span class="math">\(i\)</span>, <span class="math">\(\bar{M}\)</span> is the average value of all models, and <span class="math">\(V\)</span> is the true value. In other words,</p>
<ol>
<li>if you have only one model (<span class="math">\(M_i = \bar{M}\)</span>), you can never decrease the discrepancy between its prediction and truth.</li>
<li>Even if you have many models that make identical predictions (still <span class="math">\(M_i = \bar{M}\)</span>), they will collectively misestimate truth by as much as each one misestimates truth.</li>
<li>However, as the models’ predictions diverge from each other (that is, as <span class="math">\(\color{teal}\sum_{i=1}^N (M_i - \bar{M})^2\)</span> increases), their <em>collective misestimation</em> goes on decreasing. This is similar to the Condorcet jury theorem, except in continuous rather than binary terms.</li>
</ol>
</li>
<li>
<p>However, there is a diminishing rate of returns as you <em>include more models</em>: Accuracy seems to converge on a limit of some sort, not unlike gradient descent. Conceived in that way, you should stop adding models once the error between <span class="math">\(n\)</span> models and <span class="math">\(n-1\)</span> models is lower than some <a href="/2019/01/from-zero-to-ero"><span class="math">\(\epsilon\)</span> threshold value</a>.</p>
</li>
<li>
<p>Why is this the case? Diversity is a factor of the data’s dimensionality. Models using the same (or similar) subsets of salient features are liable to predict similarly.</p>
</li>
<li>
<p>There is a limit to how independent (<em>accurate</em> and <em>diverse</em>) a given group of models can be. Accuracy may suffer as a result of artificially trying to increase diversity (categorizing a list of locations by alphabetical order, for example).</p>
</li>
</ul>
<h2 id="accuracy">Accuracy</h2>
<p>Increasing the fit (and thus predictive accuracy) of a model by increasing its granularity (adding categories, features, etc.) can backfire after a certain point.</p>
<p>The rest of the chapter is a long and slightly overly complicated explanation of the what statisticians call the <strong>bias-variance tradeoff</strong>.</p>
<blockquote>
<p>The relationship between bias and variance is similar to the relationship to <em>precision</em> and <em>recall</em> for binary classification. Ideal B-V are both as low as possible; ideal P-R are both as high as possible. However, they have a diametrically opposing relationship, so this is not actually feasible.</p>
</blockquote>
<h3 id="bias">Bias</h3>
<ul>
<li>
<p>High <strong>bias</strong> ⟶ underfitting (model is too coarse to capture the general trend of the data). <img alt="underfitting" src="../../images/underfitting.png"/></p>
<blockquote>
<p>Source for this illustration and the following ones: <a href="https://medium.freecodecamp.org/using-machine-learning-to-predict-the-quality-of-wines-9e2e13d7480d">FreeCodeCamp</a></p>
</blockquote>
</li>
<li>
<p>In the context of the chapter’s house price example, high bias is also called <strong>categorization error</strong>. This is the discrepancy between the samples in each category and the mean of that category.</p>
</li>
<li>
<p>Bias (categorization error) ↑ if your categories are bad and don’t accurately reflect any shared characteristics among the samples. This makes sense because in terms of a scatter plot, the data points in each category will be spread out randomly and have no trend. In terms of house prices, this could be like lumping real estate markets of states that begin with the same letter together.</p>
</li>
<li>
<p>Bias (categorization error) ↓ as category granularity ↑ (gets more precise)</p>
</li>
<li>
<p>Bias (categorization error) also ↓ as sample size ↑, because the law of large numbers dictates that a larger number of samples in a category will tend toward the mean of that category. This makes sense because more data points should more clearly indicate a trend for the model to follow.</p>
</li>
</ul>
<h3 id="variance">Variance</h3>
<ul>
<li>
<p>High <strong>variance</strong> ⟶ overfitting (model is so fine that it starts to capture noise in the data, or in other words, every little <em>variation</em>).
<img alt="overfitting" src="../../images/underfitting.png"/></p>
</li>
<li>
<p>In the context of the chapter’s house price example, high bias is also called <strong>valuation error</strong>. This is the total discrepancy between the estimated category means and the actual category means.</p>
</li>
<li>
<p>Variance (valuation error) ↓ as categories get larger, again because of the law of large numbers.</p>
</li>
</ul>
<h2 id="we-all-want-what-we-cant-have">We all want what we can’t have</h2>
<p>We want low variance and low bias:
<img alt="good fit" src="../../images/goodfit.png"/></p>
<p>But decreasing variance increases bias, and decreasing bias increases variance. So good luck with that.</p>
<h2 id="a-more-intuitive-explanation-of-r2">A more intuitive explanation of <span class="math">\(R^2\)</span></h2>
<p>There are many ways to measure a model’s accuracy, and there is a sidebar mentioning <span class="math">\(R^2\)</span>, which quantifies the predictive accuracy of a regression model. However, it doesn’t explain it anywhere near as intuitively as illustration on Wikipedia:</p>
<p><img alt="R² image" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Coefficient_of_Determination.svg/640px-Coefficient_of_Determination.svg.png" title="By Orzetto - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=11398293"/></p>
<p>The chapter’s definition of <span class="math">\(R^2\)</span> is a pretty common one: <em>The proportion of variance explained by the model</em>. That’s pretty abstract.</p>
<p>A more intuitive explanation, based on the above illustration, is: <em>How much better is a prediction made with your model than just taking the average of the data?</em></p>
<p>You want the total area of the blue squares (including overlaps) to be as small as possible compared to the total area of the red squares (including overlaps).</p>
<h2 id="model-error-decomposition-theorem">Model error decomposition theorem</h2>
<p>The problem with a score like <span class="math">\(R^2\)</span> is that it doesn’t tell you how much of the error is due to bias and how much is due to variance. I suppose that might be helpful if you know exactly how to fine-tune one or the other.</p>
<p>The author presents the <strong>model error decomposition theorem</strong> to solve this problem.</p>
<div class="math">$$ \begin{gathered}
\textrm{Model Error} = \textrm{Categorization Error} + \textrm{Valuation Error} \\
\underbrace{\sum_{x \in \textbf X} \Big(M(x) - V(x) \Big)^2}_\textrm{Model Error} =
\underbrace{\sum_{i=1}^n \sum_{x \in S_i} \Big(V(x) - V_i \Big)^2}_\textrm{Categorization Error} +
\underbrace{\sum_{i=1}^n \Big(M_i - V_i \Big)^2}_\textrm{Valuation Error}
\end{gathered} $$</div>
<p>Lotta variables here.</p>
<table>
<thead>
<tr>
<th align="left">Variable</th>
<th align="left">Meaning</th>
<th align="left">Variable</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><span class="math">\(x\)</span></td>
<td align="left">sample</td>
<td align="left"><span class="math">\(S_i\)</span></td>
<td align="left">category <span class="math">\(i\)</span></td>
</tr>
<tr>
<td align="left"><span class="math">\(\textbf X\)</span></td>
<td align="left">dataset</td>
<td align="left"><span class="math">\(M_i\)</span></td>
<td align="left">mean of model’s predictions for category <span class="math">\(i\)</span></td>
</tr>
<tr>
<td align="left"><span class="math">\(M(x)\)</span></td>
<td align="left">model’s predicition for sample <span class="math">\(x\)</span></td>
<td align="left"><span class="math">\(V_i\)</span></td>
<td align="left">mean of true values for category <span class="math">\(i\)</span></td>
</tr>
<tr>
<td align="left"><span class="math">\(V(x)\)</span></td>
<td align="left">true value for sample <span class="math">\(x\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>Thus,</p>
<ul>
<li><span class="math">\(\textrm{Model Error}\)</span>: sum discrepancy between model and truth for all samples</li>
<li><span class="math">\(\textrm{Categorization Error}\)</span>: sum discrepancy between true values in a category and true mean of that category for all categories</li>
<li><span class="math">\(\textrm{Valuation Error}\)</span>: sum discrepancy between predicted mean and true mean for all categories</li>
</ul>
<p>It seems like you could do <span class="math">\(\sqrt{\textrm{Model Error}}\)</span> to find a number the average amount that a prediction made with the model will be off by.</p>
<h1 id="references">References</h1>
<ul>
<li><a href="https://medium.freecodecamp.org/using-- machine-learning-to-predict-the-quality-of-wines-9e2e13d7480d">Using Machine Learning to Predict the Quality of Wines</a>, FreeCodeCamp</li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-variance tradeoff</a>, Wikipedia</li>
<li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of determination</a>, Wikipedia</li>
</ul>

            <p class="post-footer">
                // filed
under                    <a class="post-category" href="../../tag/bias">bias</a>
                    <a class="post-category" href="../../tag/variance">variance</a>
                    <a class="post-category" href="../../tag/model-error-decomposition">model error decomposition</a>
                    <a class="post-category" href="../../tag/predictive-accuracy">predictive accuracy</a>
                    <a class="post-category" href="../../tag/r2">R²</a>
                    <a class="post-category" href="../../tag/wisdom-of-the-crowd">wisdom of the crowd</a>
                in <a class="post-category" href="../../category/statistics">Statistics</a>&nbsp;&nbsp;&nbsp;

                <span style="display:inline-block;">
                // share on <a href="https://twitter.com/share?text=%22Model%20Thinker%20notes%2C%20Ch.%202%20and%C2%A03%3A%20Getting%20reacquainted%20with%20statistics%20for%20the%20first%20time%20since%20my%20ill-fated%20stint%20in%20introductory%20statistics%20in%C2%A0college.%22&amp;hashtags=bias%2Cvariance%2Cmodelerrordecomposition%2Cpredictiveaccuracy%2CR%C2%B2%2Cwisdomofthecrowd" target="_blank">
                    <i class="fa fa-twitter fa-lg"></i> Twitter
                </a>
                </span>
            </p>
            <div class="hr"></div>


            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Justin Douglas  2019. Published with <a href="https://github.com/getpelican/pelican">Pelican</a>.<br />This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>
</footer>        </div>
    </div>
</div>
    <script>
        renderMathInElement(document.body);
    </script>

			<!-- Script specified by the user -->
			<script type="text/javascript"  src="../../assets/tweaks.js"></script>

    <!-- for pelican_dynamic plugin -->



</body>
</html>